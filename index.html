<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhuohang Jiang</title>

  <meta name="author" content="Zhuohang Jiang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🌐</text></svg>">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css" />
  <link rel="stylesheet" href="magic/dist/magic.css">
  <link rel="stylesheet" href="hover.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/wow/1.1.2/wow.min.js"></script>
  <style>
    .aligned-table {
      width: 100% !important;
      table-layout: fixed !important;
      margin-bottom: 10px !important;
    }

    .aligned-table td:first-child {
      width: 60% !important;
      text-align: left !important;
    }

    .aligned-table td:last-child {
      width: 40% !important;
      text-align: right !important;
    }

    /* 确保Award和Professional Service章节的右侧内容完美对齐 */
    table[style*="table-layout: fixed"] td:last-child {
      text-align: right !important;
      padding-right: 0 !important;
    }

    table[style*="table-layout: fixed"] td:first-child {
      text-align: left !important;
      padding-left: 0 !important;
    }
  </style>
</head>



<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                  <div class="wow animate__animated animate__fadeInDown" style="font-size: 2.5em; font-weight: bold;">
                    <name>Zhuohang Jiang</name>
                  </div>

                  </p>
                  <div>
                    <p>I am currently pursuing a PhD degree at the <strong> <a href="https://www.polyu.edu.hk/">Hong
                          Kong Polytechnic University</a></strong>. My supervisors are <a
                        href="https://scholar.google.cz/citations?user=D1LEg-YAAAAJ&hl=zh-CN">Qing Li</a>
                      and <a href="https://scholar.google.cz/citations?user=SQ9UbHIAAAAJ&hl=zh-CN&oi=ao">Wenqi Fan</a>.
                      My current Cumulative GPA is <strong>3.60/4.00</strong>.
                    </p>
                    <p>I studied at <a href="https://www.scu.edu.cn">Sichuan University (SCU)</a> from 2020 to 2024,
                      where I majored in Computer Science & Technology. My
                      <strong> Major GPA (CS courses):
                        3.79/4, 89.39/100;</strong>
                      Overall GPA: 3.78/4, 89.25/100
                    </p>
                    <p>
                      During my time at Sichuan University, I worked as a research assistant at
                      <a href="http://www.machineilab.org/">MachineILab</a>
                      from 2022 to 2024, advised by
                      <a href="https://scholar.google.com/citations?user=-cNWmJMAAAAJ&hl=zh-CN">Prof. JiZhe Zhou</a>.
                      I participated in one National Natural Science Foundation of China project and one National Key
                      R&D
                      Program of China.
                    </p>
                  </div>

                  <p style="text-align:center">
                    <a href="mailto:zhuohangjiang2002@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="data/ZhuohangJiang-CV.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.cz/citations?hl=zh-CN&user=h3vEhrgAAAAJ">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/jzzzzh">Github</a>&nbsp
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/zhuohang.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/zhuohang.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research Topics</heading>
                  <p>
                    My research interests lie in <strong>large language models (LLMs), retrieval-augmented generation
                      (RAG),
                      and Recommender Systems (RecSys)</strong>. I focus on both <strong>theoretical
                      foundations</strong> and
                    practical applications of LLM-based systems.
                  </p>
                  <p>
                    My previous research was primarily focused on topics within computer vision, such as tampering
                    detection and object recognition tasks. I have contributed to the design of high-impact benchmarks,
                    such as <strong>HiBench</strong>, and comprehensive surveys like <strong>WebAgents</strong>.
                    My work has been published in <strong>top-tier conferences</strong>, including
                    <strong>NeurIPS 2024</strong> (spotlight) and <strong>AAAI 2025</strong>, and has accumulated
                    <strong>80+ citations</strong>, with an <strong>h-index of 4</strong>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>News</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <div>
                    <p> <strong>🎤 2025-08-07</strong> - Invited to give a talk "Understanding Hierarchical Data with
                      Large Language Models: RAG, Structural Reasoning, and Future Directions" at <strong>KDD 2025
                        Reasoning Day</strong> in Toronto, Canada! 🎙️</p>
                  </div>
                  <div>
                    <p> <strong>🏅 2025-06-18</strong> - Achieved <strong>12th place globally</strong> in KDD Cup 2025 -
                      Meta CRAG-MM Multimodal Retrieval Challenge among hundreds of international teams! 🌍</p>
                  </div>
                  <div>
                    <p> <strong>🏆 2025-05-16</strong> - Our benchmark paper <a
                        href="https://arxiv.org/abs/2503.00912">HiBench</a> was accepted to <strong>KDD Benchmark
                        Track</strong>! 🎉</p>
                  </div>
                  <div>
                    <p> <strong>🎉 2025-05-07</strong> - Our survey paper <a href="https://arxiv.org/abs/2503.23350">A
                        Survey
                        of WebAgents: Towards Next-Generation AI Agents for Web Automation with Large Foundation
                        Models</a> was accepted to <strong>KDD Tutorial Track</strong>! 🎊</p>
                  </div>
                  <div>
                    <p> <strong>📜 2025-03-30</strong> - Completed the survey paper <a
                        href="https://arxiv.org/abs/2503.23350">A Survey of WebAgents: Towards Next-Generation AI Agents
                        for Web Automation with Large Foundation Models</a>.</p>
                  </div>
                  <div>
                    <p> <strong>📘 2025-03-01</strong> - Completed the <a
                        href="https://arxiv.org/abs/2503.00912">HiBench</a> paper and released the code and dataset on
                      <a href="https://github.com/jzzzzh/HiBench">GitHub</a> and
                      <a href="https://huggingface.co/datasets/zhuohang/HiBench">Hugging Face</a>.
                    </p>
                  </div>



                  <!-- 剩余的新闻默认隐藏 -->

                  <div class="more-news">

                    <p> <strong>🌟 2025-01-15</strong> - <a
                        href="https://ojs.aaai.org/index.php/AAAI/article/view/33198">Mesoscopic Insights: Orchestrating
                        Multi-Scale & Hybrid Architecture for Image Manipulation Localization</a> was published in AAAI
                      2025.</p>

                    <p> <strong>🏆 2024-12-01</strong> - <a href="https://arxiv.org/abs/2406.10580">IMDL-BenCo </a>was
                      published in NeurIPS 2024 Benchmark Tracks and received a Spotlight award.</p>

                    <p> <strong>🎓 2024-09-01</strong> - Beginning my pursuit of a PhD degree in Hong Kong PolyU.</p>

                    <p> <strong>🎓 2024-06-26</strong> - Got <strong>Outstanding Graduate Award</strong> from Sichuan
                      University and Sichuan Province! 🎉</p>

                    <p> <strong>🎓 2024-06-26</strong> - Graduated from Sichuan University with a bachelor's degree.</p>

                    <p> <strong>🛠️ 2024-06-12</strong> - Completed the co-work project <a
                        href="https://github.com/scu-zjz/IMDLBenCo"> IMDLBenCo</a> and finished a paper
                      <a href="https://arxiv.org/abs/2406.10580"><strong>IMDL-BenCo: A Comprehensive Benchmark and
                          Codebase for Image Manipulation Detection & Localization</strong></a>
                    </p>

                    <p> <strong>🔍 2024-05-24</strong> - Finished a paper <a
                        href="https://arxiv.org/abs/2406.12736"><strong>Beyond
                          Visual Appearances: Privacy-sensitive Objects Identification via Hybrid Graph
                          Reasoning</strong></a> </p <p> <strong>📚 2023-08-01</strong> - Participated in <strong>NUS
                      Summer School</strong> research
                    program at National University of Singapore, completed face recognition project! 🇸🇬</p>
                  </div>

                  <div style="text-align: center; margin-top: 20px;">
                    <button id="expand-btn" onclick="showMoreNews()" class="hvr-grow"
                      style="padding: 10px 20px; font-size: 16px; background-color: #007BFF; color: white; border: none; border-radius: 5px; cursor: pointer;">
                      More News
                    </button>
                  </div>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>
          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Selected Publications</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>[KDD'25] A Survey of WebAgents: Towards Next-Generation AI Agents for Web Automation with
                    Large
                    Foundation Models</papertitle>
                  [<a href="https://arxiv.org/abs/2503.23350">arxiv</a>]
                  <br>
                  Liangbo Ning, Ziran Liang, <b>Zhuohang Jiang</b>, Haohao Qu, Yujuan Ding, Wenqi Fan, Xiao-yong Wei,
                  Shanru Lin, Hui Liu, Philip S. Yu, Qing Li
                  <br>
                  <font color='gray'>
                    With the advancement of web techniques, they have significantly revolutionized various aspects of
                    people's lives. Despite the importance of the web, many tasks performed on it are repetitive and
                    time-consuming, negatively impacting overall quality of life. To efficiently handle these tedious
                    daily tasks, one of the most promising approaches is to advance autonomous agents based on
                    Artificial Intelligence (AI) techniques, referred to as AI Agents, as they can operate continuously
                    without fatigue or performance degradation. In the context of the web, leveraging AI Agents --
                    termed WebAgents -- to automatically assist people in handling tedious daily tasks can dramatically
                    enhance productivity and efficiency. Recently, Large Foundation Models (LFMs) containing billions of
                    parameters have exhibited human-like language understanding and reasoning capabilities, showing
                    proficiency in performing various complex tasks. This naturally raises the question: `Can LFMs be
                    utilized to develop powerful AI Agents that automatically handle web tasks, providing significant
                    convenience to users?' To fully explore the potential of LFMs, extensive research has emerged on
                    WebAgents designed to complete daily web tasks according to user instructions, significantly
                    enhancing the convenience of daily human life. In this survey, we comprehensively review existing
                    research studies on WebAgents across three key aspects: architectures, training, and
                    trustworthiness. Additionally, several promising directions for future research are explored to
                    provide deeper insights.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>[KDD'25] HiBench: Benchmarking LLMs Capability on Hierarchical Structure Reasoning
                  </papertitle>
                  [<a href="https://arxiv.org/abs/2503.00912">arxiv</a>] [<a
                    href="https://github.com/jzzzzh/HiBench">GitHub</a>] [<a
                    href="https://huggingface.co/datasets/zhuohang/HiBench">Hugging Face</a>]
                  <br>
                  <b>Zhuohang Jiang</b>, Pangjing Wu, Ziran Liang, Peter Q. Chen, Xu Yuan, Ye Jia, Jiancheng Tu, Chen
                  Li, Peter H.F. Ng, Qing Li
                  <br>
                  <font color='gray'>
                    Structure reasoning is a fundamental capability of large language models (LLMs), enabling them to
                    reason about structured commonsense and answer multi-hop questions. However, existing benchmarks for
                    structure reasoning mainly focus on horizontal and coordinate structures (\emph{e.g.} graphs),
                    overlooking the hierarchical relationships within them. Hierarchical structure reasoning is crucial
                    for human cognition, particularly in memory organization and problem-solving. It also plays a key
                    role in various real-world tasks, such as information extraction and decision-making. To address
                    this gap, we propose HiBench, the first framework spanning from initial structure generation to
                    final proficiency assessment, designed to benchmark the hierarchical reasoning capabilities of LLMs
                    systematically. HiBench encompasses six representative scenarios, covering both fundamental and
                    practical aspects, and consists of 30 tasks with varying hierarchical complexity, totaling 39,519
                    queries. To evaluate LLMs comprehensively, we develop five capability dimensions that depict
                    different facets of hierarchical structure understanding. Through extensive evaluation of 20 LLMs
                    from 10 model families, we reveal key insights into their capabilities and limitations: 1) existing
                    LLMs show proficiency in basic hierarchical reasoning tasks; 2) they still struggle with more
                    complex structures and implicit hierarchical representations, especially in structural modification
                    and textual reasoning. Based on these findings, we create a small yet well-designed instruction
                    dataset, which enhances LLMs' performance on HiBench by an average of 88.84% (Llama-3.1-8B) and
                    31.38% (Qwen2.5-7B) across all tasks. The HiBench dataset and toolkit are available here, this https
                    URL, to encourage evaluation.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>[AAAI'25] Mesoscopic Insights: Orchestrating Multi-Scale & Hybrid Architecture for Image
                    Manipulation Localization</papertitle>
                  [<a href="https://arxiv.org/abs/2412.13753">arxiv</a>]
                  <br>
                  Xuekang Zhu, Xiaochen Ma, Lei Su, <b>Zhuohang Jiang</b>, Bo Du, Xiwen Wang, Zeyu Lei, Wentao Feng,
                  Chi-Man Pun, Jizhe Zhou
                  <br>
                  <font color='gray'>
                    The mesoscopic level serves as a bridge between the macroscopic and microscopic worlds, addressing
                    gaps overlooked by both. Image manipulation localization (IML), a crucial technique to pursue truth
                    from fake images, has long relied on low-level (microscopic-level) traces. However, in practice,
                    most tampering aims to deceive the audience by altering image semantics. As a result, manipulation
                    commonly occurs at the object level (macroscopic level), which is equally important as microscopic
                    traces. Therefore, integrating these two levels into the mesoscopic level presents a new perspective
                    for IML research. Inspired by this, our paper explores how to simultaneously construct mesoscopic
                    representations of micro and macro information for IML and introduces the Mesorch architecture to
                    orchestrate both. Specifically, this architecture i) combines Transformers and CNNs in parallel,
                    with Transformers extracting macro information and CNNs capturing micro details, and ii) explores
                    across different scales, assessing micro and macro information seamlessly. Additionally, based on
                    the Mesorch architecture, the paper introduces two baseline models aimed at solving IML tasks
                    through mesoscopic representation. Extensive experiments across four datasets have demonstrated that
                    our models surpass the current state-of-the-art in terms of performance, computational complexity,
                    and robustness.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <!-- <a href="under-review.html" id="AutoCost"> -->

                  <papertitle>[NIPS'24] IMDL-BenCo: A Comprehensive Benchmark and Codebase for Image Manipulation
                    Detection &
                    Localization</papertitle>
                  [<a href="https://arxiv.org/abs/2406.10580">arxiv</a>]
                  <br>
                  Xiaochen Ma, Xuekang Zhu, Lei Su, Bo Du, <b>Zhuohang Jiang</b>, Bingkui Tong, Zeyu Lei, Xinyu Yang,
                  Chi-Man Pun, Jiancheng Lv, Jizhe Zhou

                  <br>
                  <font color='gray'>
                    A comprehensive benchmark is yet to be established in the Image Manipulation Detection &
                    Localization (IMDL) field. The absence of such a benchmark leads to insufficient and misleading
                    model evaluations, severely undermining the development of this field. However, the scarcity of
                    open-sourced baseline models and inconsistent training and evaluation protocols make conducting
                    rigorous experiments and faithful comparisons among IMDL models challenging. To address these
                    challenges, we introduce IMDL-BenCo, the first comprehensive IMDL benchmark and modular codebase.
                    IMDL-BenCo:i) decomposes the IMDL framework into standardized, reusable components and
                    revises the model construction pipeline, improving coding efficiency and customization
                    flexibility;ii) fully implements or incorporates training code for state-of-the-art models
                    to establish a comprehensive IMDL benchmark; and iii) conducts deep analysis based on the
                    established benchmark and codebase, offering new insights into IMDL model architecture, dataset
                    characteristics, and evaluation standards. Specifically, IMDL-BenCo includes common processing
                    algorithms, 8 state-of-the-art IMDL models (1 of which are reproduced from scratch), 2 sets of
                    standard training and evaluation protocols, 15 GPU-accelerated evaluation metrics, and 3 kinds of
                    robustness evaluation. This benchmark and codebase represent a significant leap forward in
                    calibrating the current progress in the IMDL field and inspiring future breakthroughs.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <!-- <a href="under-review.html" id="AutoCost"> -->

                  <papertitle>Beyond Visual Appearances: Privacy-sensitive Objects Identification via Hybrid Graph
                    Reasoning</papertitle>
                  [<a href="https://arxiv.org/abs/2406.12736">arxiv</a>]
                  <br>
                  <b>Zhuohang Jiang</b>, Bingkui Tong, Xia Du, Ahmed Alhammadi, Jizhe Zhou

                  <br>
                  <font color='gray'>
                    To explicitly derive the objects' privacy class from the scene contexts, in this paper, we interpret
                    the POI task as a visual reasoning task aimed at the privacy of each object in the scene. Following
                    this interpretation, we propose the PrivacyGuard framework for POI. PrivacyGuard contains three
                    stages. i) Structuring: an unstructured image is first converted into a structured, heterogeneous
                    scene graph that embeds rich scene contexts. ii) Data Augmentation: a contextual perturbation
                    oversampling strategy is proposed to create slightly perturbed privacy-sensitive objects in a scene
                    graph, thereby balancing the skewed distribution of privacy classes. iii) Hybrid Graph Generation &
                    Reasoning: the balanced, heterogeneous scene graph is then transformed into a hybrid graph by
                    endowing it with extra "node-node" and "edge-edge" homogeneous paths. These homogeneous paths allow
                    direct message passing between nodes or edges, thereby accelerating reasoning and facilitating the
                    capturing of subtle context changes.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>





          <div class="more-publications" style="display: none;">
            <!-- Remaining publications go here -->

            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                    <!-- <a href="under-review.html" id="AutoCost"> -->

                    <papertitle>[ICONIP'23] TPTGAN: Two-Path Transformer-Based Generative Adversarial Network Using
                      Joint Magnitude Masking and Complex Spectral Mapping for Speech Enhancement</papertitle>
                    [<a href="https://link.springer.com/chapter/10.1007/978-981-99-8082-6_27">paper</a>]
                    <br>
                    Zhaoyi Liu, <b>Zhuohang Jiang</b>, Wendian Luo, Zhuoyao Fan, Haoda Di, Yufan Long, Haizhou Wang

                    <br>
                    <font color='gray'>
                     In this paper, we propose a two-path transformer-based metric generative adversarial network (TPTGAN) for speech enhancement in the time-frequency domain. The generator consists of an encoder, a two-stage transformer module, a magnitude mask decoder and a complex spectrum decoder. Published in ICONIP 2023 (CCF-C).
                    </font>
                  </td>
                </tr>
              </tbody>
            </table>

            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                    <!-- <a href="under-review.html" id="AutoCost"> -->

                    <papertitle>IML-ViT: Benchmarking Image Manipulation Localization by Vision Transformer</papertitle>
                    [<a href="https://arxiv.org/abs/2307.14863">arxiv</a>]
                    <br>
                    Xiaochen Ma, Bo Du,<b>Zhuohang Jiang</b>, Ahmed Y. Al Hammadi, Jizhe Zhou

                    <br>
                    <font color='gray'>
                      Due to limited datasets, there is currently no pure ViT-based approach for IML to serve as a
                      benchmark, and CNNs dominate the entire task. Nevertheless, CNNs suffer from weak long-range and
                      non-semantic modeling. To bridge this gap, based on the fact that artifacts are sensitive to image
                      resolution, amplified under multi-scale features, and massive at the manipulation border, we
                      formulate the answer to the former question as building a ViT with high-resolution capacity,
                      multi-scale feature extraction capability, and manipulation edge supervision that could converge
                      with a small amount of data. We term this simple but effective ViT paradigm IML-ViT, which has
                      significant potential to become a new benchmark for IML. Extensive experiments on five benchmark
                      datasets verified our model outperforms the state-of-the-art manipulation localization methods.
                    </font>
                  </td>
                </tr>
              </tbody>
            </table>


            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                    <!-- <a href="under-review.html" id="AutoCost"> -->

                    <papertitle>Perceptual MAE for Image Manipulation Localization: A High-level Vision Learner Focusing
                      on Low-level Features</papertitle> [<a href="https://arxiv.org/abs/2310.06525">arxiv</a>]
                    <br>
                    Xiaochen Ma, <b>Zhuohang Jiang</b>, Xiong Xu, Chi-Man Pun, Jizhe Zhou
                    <br>
                    <font color='gray'> This necessitates IML models to carry out a semantic understanding of the entire
                      image. In this paper, we
                      reformulate the IML task as a high‑level
                      vision task that greatly benefits from low‑level features. We propose a method to enhance the
                      Masked
                      Autoencoder (MAE) by incorporating
                      high‑resolution inputs and a perceptual loss supervision module, which we term Perceptual MAE
                      (PMAE). While
                      MAE has demonstrated an
                      impressive understanding of object semantics, PMAE can also comprehend low‑level semantics with
                      our
                      proposed
                      enhancements. This
                      paradigm effectively unites the low‑level and high‑level features of the IML task and outperforms
                      state‑of‑the‑art tampering localization methods
                      on five publicly available datasets, as evidenced by extensive experiments.</font>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>

          <div style="text-align: center; margin-top: 20px;">
            <button id="expand-publications-btn" onclick="togglePublications()" class="hvr-grow"
              style="padding: 10px 20px; font-size: 16px; background-color: #007BFF; color: white; border: none; border-radius: 5px; cursor: pointer;">
              More Publications
            </button>
          </div>
          </div>

          <script>
            function togglePublications() {
              var morePublications = document.querySelector('.more-publications');
              var button = document.getElementById('expand-publications-btn');
              button.addEventListener('click', function () {
                if (morePublications.style.display === 'none') {
                  morePublications.style.display = 'block';
                  button.classList.remove('hvr-grow');
                  button.classList.add('hvr-shrink');
                  button.textContent = 'Less Publications';
                  morePublications.classList.add('magictime', 'puffIn');
                } else {
                  morePublications.style.display = 'none';
                  button.classList.remove('hvr-shrink');
                  button.classList.add('hvr-grow');
                  button.textContent = 'More Publications';
                }
              });
            }
          </script>

          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Selected Projects</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>HiBench: Benchmark for Hierarchical Reasoning</papertitle>
                  [<a href="https://arxiv.org/abs/2503.00912">arXiv</a>] [<a
                    href="https://github.com/jzzzzh/HiBench">GitHub</a>] [<a
                    href="https://huggingface.co/datasets/zhuohang/HiBench">Hugging Face</a>]
                  <br>
                  <strong>First Author & Team Leader</strong> • <em>KDD 2025 Benchmark Track</em> • 2025
                  <br>
                  <font color='gray'>
                    Designed and developed the first comprehensive benchmark for evaluating LLMs' capability on
                    hierarchical structure reasoning.
                    The benchmark encompasses six representative scenarios with 39,519 queries across varying
                    hierarchical complexity.
                    <strong>Key contributions:</strong> (1) Led the architectural design and implementation of the
                    evaluation framework,
                    (2) Coordinated a multi-institutional team across different time zones, (3) Open-sourced the
                    complete toolkit
                    including dataset, evaluation metrics, and baseline implementations. The benchmark has been accepted
                    as an
                    <strong>oral presentation</strong> at KDD 2025 and is being adopted by multiple research groups for
                    hierarchical reasoning evaluation.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>Meta CRAG-MM: Multimodal Retrieval Challenge</papertitle>
                  <br>
                  <strong>Team Leader</strong> • <em>KDD Cup 2025</em> • 2025
                  <br>
                  <font color='gray'>
                    Led a team to achieve <strong>12th place globally</strong> among hundreds of international teams in
                    the Meta CRAG-MM
                    Multimodal Retrieval Challenge. <strong>Key contributions:</strong> (1) Designed novel multimodal
                    fusion architectures
                    combining vision and language understanding, (2) Implemented efficient retrieval-augmented
                    generation pipelines,
                    (3) Coordinated team efforts in model development, hyperparameter optimization, and submission
                    strategies.
                    The challenge focused on developing AI systems capable of understanding and retrieving information
                    from
                    multimodal content, which aligns with current trends in large multimodal models.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>IMDL-BenCo: Benchmark for Image Manipulation Detection & Localization</papertitle>
                  [<a href="https://arxiv.org/abs/2406.10580">arXiv</a>] [<a
                    href="https://github.com/scu-zjz/IMDLBenCo">GitHub</a>]
                  <br>
                  <strong>Co-First Author</strong> • <em>NeurIPS 2024 Benchmark Track — Spotlight</em> • 2024
                  <br>
                  <font color='gray'>
                    Developed the first comprehensive benchmark and codebase for Image Manipulation Detection &
                    Localization (IMDL).
                    <strong>Key contributions:</strong> (1) Implemented GPU-accelerated evaluation metrics for fair and
                    efficient comparison,
                    (2) Designed modular codebase architecture enabling easy customization and extension, (3)
                    Co-authored the manuscript
                    that received a <strong>Spotlight Award</strong> at NeurIPS 2024. The benchmark includes 8
                    state-of-the-art models,
                    15 evaluation metrics, and comprehensive robustness evaluation protocols, significantly advancing
                    the field's
                    standardization and reproducibility.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>


          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Invited Talks</heading>
                </td>
              </tr>
              <tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>Understanding Hierarchical Data with Large Language Models: RAG, Structural Reasoning, and
                    Future Directions</papertitle>
                  <br>
                  <strong>Invited Talks</strong> • <em>Reasoning Day @ KDD 2025</em> • Toronto, ON, Canada • Aug 2025
                  <br>
                  <font color='gray'>
                    Invited to deliver a presentation at the prestigious KDD 2025 Reasoning Day workshop.<br>
                    The talk will explore cutting-edge developments in leveraging Large Language Models for hierarchical
                    data understanding,<br>
                    with particular focus on Retrieval-Augmented Generation (RAG) systems and structural reasoning
                    capabilities.<br>
                    <strong>Key topics:</strong><br>
                    (1) Novel approaches to hierarchical data representation in LLM contexts,<br>
                    (2) Integration of structural reasoning with retrieval-augmented generation,<br>
                    (3) Future research directions in reasoning-enhanced AI systems,<br>
                    (4) Practical applications and deployment considerations for hierarchical reasoning in real-world
                    scenarios.<br>
                    This invitation recognizes the impact of our HiBench work and positions our research at the
                    forefront of LLM reasoning capabilities.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>

          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Education</heading>
                </td>
              </tr>
              <tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                  <b>Sichuan University</b>, Chengdu, Sichuan, China
                  <br>
                  B.E. in Computer Science and Technology • Sep. 2020 to Jun. 2024
                  </br>
                </td>
                <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img
                    src="images/SichuanUniversityLOGO.png" width="75" height="75"></td>
              </tr>
            </tbody>
          </table>
          <br>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                  <b>Hong Kong Polytechnic University</b>, Hongkong, China
                  <br>
                  PHD. in Computer Science and Technology • Sep. 2024 to Present
                  </br>
                </td>
                <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img src="images/polyu.jpg"
                    width="75" height="75"></td>
              </tr>
            </tbody>

          </table>

          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Experience</heading>
                </td>
              </tr>
              <tr>
            </tbody>
          </table>



          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                  <b>National University of Singapore (NUS)</b>
                  <br>
                  <strong>Summer School Participant</strong> • Aug. 2023
                  <br>
                  • Participated in intensive research program at School of Computing
                  <br>
                  • Completed face recognition project using CNN-based feature extraction and similarity matching
                  <br>
                  • Gained international research experience and cross-cultural collaboration skills
                  </br>
                </td>
                <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img src="images/nus.png"
                    width="135" height="75"></td>
              </tr>
            </tbody>
          </table>
          <br>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                  <a href="http://dicalab.cn/"> DICALab</a>, <b>Sichuan University</b>
                  <br>
                  <strong>Research Assistant</strong> • Sep. 2022 to Jun. 2024
                  <br>
                  Advisor: <a href="https://scholar.google.com/citations?user=-cNWmJMAAAAJ&hl=zh-CN"> Prof. JiZhe
                    Zhou</a>
                  <br>
                  • Developed graph-based frameworks for privacy-sensitive object detection
                  <br>
                  • Participated in National Natural Science Foundation of China project
                  <br>
                  • Contributed to National Key R&D Program of China
                  <br>
                  • Co-authored multiple publications in top-tier conferences and journals
                  </br>
                </td>
                <td style="padding:0px 20px 0px 20px;width:8%;vertical-align:middle"><img src="images/DICA.png"
                    width="75" height="75"></td>
              </tr>
            </tbody>
          </table>
          <br>
          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"
            style="margin-bottom: 10px; table-layout: fixed;">
            <tbody>
              <tr>
                <td colspan="2" style="padding:20px 0 0px 0;width:100%;vertical-align:middle;text-align:left;">
                  <heading>Selected Awards</heading>
                </td>
              </tr>
              <tr style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>KDD Cup 2025 — Meta CRAG-MM</strong><br>
                  <span style="color: #666; font-size: 0.95em;">Toronto, Canada, 2025</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>12th Place (Global)</strong>
                </td>
              </tr>
              <tr style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>NeurIPS 2024 — IMDL‑BenCo (Co‑first Author)</strong><br>
                  <span style="color: #666; font-size: 0.95em;">Vancouver, Canada, 2024</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>Spotlight Award</strong>
                </td>
              </tr>
              <tr style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>Outstanding Graduate</strong><br>
                  <span style="color: #666; font-size: 0.95em;">Sichuan University &amp; Sichuan Province, 2024</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>Top Achievement</strong>
                </td>
              </tr>
              <tr style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>Tencent Scholarship</strong><br>
                  <span style="color: #666; font-size: 0.95em;">Sichuan University, China, 2023</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>Top 2%</strong>
                </td>
              </tr>
              <tr style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>A-Level Certificate</strong><br>
                  <span style="color: #666; font-size: 0.95em;">Comprehensive Quality Evaluation, China, 2023</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>Excellence</strong>
                </td>
              </tr>
              <tr style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>Computer Design Competition</strong><br>
                  <span style="color: #666; font-size: 0.95em;">China, 2023</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>Provincial First Prize</strong>
                </td>
              </tr>
              <tr style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>Comprehensive First Class Scholarship</strong><br>
                  <span style="color: #666; font-size: 0.95em;">Sichuan University, Sichuan, China, 2022</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>Top 1%</strong>
                </td>
              </tr>
              <tr>
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>Outstanding Students of Sichuan University</strong><br>
                  <span style="color: #666; font-size: 0.95em;">Sichuan University, Sichuan, China, 2022</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>Top 5%</strong>
                </td>
              </tr>
            </tbody>
          </table>






          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"
            style="margin-bottom: 10px; table-layout: fixed;">
            <tbody>
              <tr>
                <td colspan="2" style="padding:20px 0 0px 0;width:100%;vertical-align:middle;text-align:left;">
                  <heading>Professional Service</heading>
                </td>
              </tr>
              <tr style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>Conference & Journal Reviewer</strong><br>
                  <span style="color: #666; font-size: 0.95em;">2023-2025</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>TIP, CVPR, NeurIPS, KDD</strong>
                </td>
              </tr>
              <tr style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>Teaching Assistant</strong><br>
                  <span style="color: #666; font-size: 0.95em;">The Hong Kong Polytechnic University (PolyU)</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>Artificial Intelligence (COMP4431)<br>NLP Practicum (COMP5423)</strong>
                </td>
              </tr>
            </tbody>
          </table>

          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0" style="margin-bottom: 10px; table-layout: fixed;">
            <tbody>
              <tr>
                <td colspan="2" style="padding:20px 0 0px 0;width:100%;vertical-align:middle;text-align:left;">
                  <heading>Skills</heading>
                </td>
              </tr>
              <tr>
                <td style="width:30%;vertical-align:top;padding:10px 0;text-align:left;">
                  <strong>Research Topics</strong>
                </td>
                <td style="width:70%;vertical-align:top;padding:10px 0;text-align:left;">
                  Large Language Models (LLMs), Retrieval‑Augmented Generation (RAG), Recommender Systems (RecSys)
                </td>
              </tr>
              <tr>
                <td style="width:30%;vertical-align:top;padding:10px 0;text-align:left;">
                  <strong>Frameworks &amp; Tools</strong>
                </td>
                <td style="width:70%;vertical-align:top;padding:10px 0;text-align:left;">
                  PyTorch, Hugging Face, NumPy, Docker, Git, Anaconda
                </td>
              </tr>
              <tr>
                <td style="width:30%;vertical-align:top;padding:10px 0;text-align:left;">
                  <strong>Languages</strong>
                </td>
                <td style="width:70%;vertical-align:top;padding:10px 0;text-align:left;">
                  Mandarin (native), English (fluent, IELTS 6.5)
                </td>
              </tr>
            </tbody>
          </table>

          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <div style="text-align: center;">
            <a href="https://clustrmaps.com/site/1buac" title="Visit tracker">
              <img src="//www.clustrmaps.com/map_v2.png?d=arwquO5nouqZwMbhchX9awZ1bM3tO8fPWrcUr78F1E4&cl=ffffff" />
            </a>
          </div>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px">
                  <div style="float:left;">
                    Updated at Jul. 2025
                  </div>
                  <div style="float:right;">
                    Thanks <a href="https://jonbarron.info">Jon Barron</a> for this amazing template
                  </div>
                  <br>
                  <br>
                  <p></p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>


  <script>
    new WOW().init();


    function showMoreNews() {
      var moreNews = document.querySelector('.more-news');
      var button = document.getElementById('expand-btn');
      button.addEventListener('click', function () {
        if (moreNews.style.display === 'none') {
          moreNews.style.display = 'block';
          button.textContent = 'Less News';
          button.classList.remove('hvr-grow');
          button.classList.add('hvr-shrink');
          moreNews.classList.add('magictime', 'puffIn');
        } else {
          moreNews.style.display = 'none';
          button.textContent = 'More News';
          button.classList.remove('hvr-shrink');
          button.classList.add('hvr-grow');
        }
      });
    }



  </script>
</body>


</html>
