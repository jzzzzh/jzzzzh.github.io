<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhuohang Jiang</title>

  <meta name="author" content="Zhuohang Jiang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Zhuohang Jiang</name>
                  </p>
                  <p>I am an undergraduate student at <a href="https://www.scu.edu.cn">Sichuan
                      University(SCU)</a>,majoring in Computer Science & technology. My
                    <strong> Major GPA (CS cources):
                      3.79/4, 89.39/100;</strong>
                    Overall GPA: 3.78/4, 89.25/100
                  </p>
                  <p>
                    In the Sichuan University, I am working as a research assistant at
                    <a href="http://www.machineilab.org/">MachineILab</a>
                    since 2022, advised by
                    <a href="https://scholar.google.com/citations?user=-cNWmJMAAAAJ&hl=zh-CN">Prof. JiZhe Zhou</a>.
                    I am participating in one National Natural Science Foundation of China and one National Key R&D
                    Program of China.
                  </p>
                  <p><strong class="bold-red">üåüExciting:</strong> I am about to go to the <strong> <a
                        href="https://www.polyu.edu.hk/">Hong Kong Polytechnic University</a></strong> to
                    pursue a PhD degree. My supervisors are <a
                      href="https://scholar.google.cz/citations?user=D1LEg-YAAAAJ&hl=zh-CN">Qing Li</a>
                    and <a href="https://scholar.google.cz/citations?user=SQ9UbHIAAAAJ&hl=zh-CN&oi=ao">Wenqi Fan</a>.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:zhuohangjiang2002@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="data/ZhuohangJiang-CV.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.cz/citations?hl=zh-CN&user=h3vEhrgAAAAJ">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/jzzzzh">Github</a>&nbsp/&nbsp
                    <a href="https://www.zhihu.com/people/jzh-8-11">Zhihu</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/ZhuoHangJiang_casual.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/ZhuoHangJiang_circle.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research Topics</heading>
                  <p>
                    My research areas include <strong>recommendation systems(RS), large language models(LLM), computer
                      vision(CV), and graph neural networks(GNN)</strong>.
                    My previous research was primarily focused on topics within computer vision, such as tampering
                    detection and object recognition tasks.
                    Currently, I am diving into the realms of large language models(LLM) and recommendation systems(RS).
                    </span>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>News</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <div>
                    <p> <strong>2024-09-01</strong> - beginning my pursuit of a PhD degree.</p>
                  </div>
                  <div>
                    <p> <strong>2024-06-26</strong> - graduated from Sichuan University with a bachelor's degree.</p>
                  </div>
                  <div>
                    <p> <strong>2024-06-12</strong> - completed the
                      co-work project <a href="https://github.com/scu-zjz/IMDLBenCo"> IMDLBenCo</a> and finish a paper
                      <a href="https://arxiv.org/abs/2406.10580"><strong>IMDL-BenCo: A Comprehensive Benchmark and
                          Codebase for Image Manipulation Detection & Localization</strong></a>
                    </p>
                  </div>
                  <!-- Ââ©‰ΩôÁöÑÊñ∞ÈóªÈªòËÆ§ÈöêËóè -->

                  <div class="more-news">
                    <p> <strong>2024-05-24</strong> - finish a paper <a
                        href="https://arxiv.org/abs/2406.12736"><strong>Beyond
                          Visual Appearances: Privacy-sensitive Objects Identification via Hybrid Graph
                          Reasoning</strong></a> </p>
                  </div>

                  <button id="expand-btn" onclick="showMoreNews()">More News</button>
                </td>
              </tr>
            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Publications</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <!-- <a href="under-review.html" id="AutoCost"> -->

                  <papertitle>IMDL-BenCo: A Comprehensive Benchmark and Codebase for Image Manipulation Detection &
                    Localization</papertitle>
                  [<a href="https://arxiv.org/abs/2406.10580">arxiv</a>]
                  <br>
                  Xiaochen Ma, Xuekang Zhu, Lei Su, Bo Du, <b>Zhuohang Jiang</b>, Bingkui Tong, Zeyu Lei, Xinyu Yang,
                  Chi-Man Pun, Jiancheng Lv, Jizhe Zhou

                  <br>
                  <font color='gray'>
                    A comprehensive benchmark is yet to be established in the Image Manipulation Detection &
                    Localization (IMDL) field. The absence of such a benchmark leads to insufficient and misleading
                    model evaluations, severely undermining the development of this field. However, the scarcity of
                    open-sourced baseline models and inconsistent training and evaluation protocols make conducting
                    rigorous experiments and faithful comparisons among IMDL models challenging. To address these
                    challenges, we introduce IMDL-BenCo, the first comprehensive IMDL benchmark and modular codebase.
                    IMDL-BenCo:i) decomposes the IMDL framework into standardized, reusable components and
                    revises the model construction pipeline, improving coding efficiency and customization
                    flexibility;ii) fully implements or incorporates training code for state-of-the-art models
                    to establish a comprehensive IMDL benchmark; and iii) conducts deep analysis based on the
                    established benchmark and codebase, offering new insights into IMDL model architecture, dataset
                    characteristics, and evaluation standards. Specifically, IMDL-BenCo includes common processing
                    algorithms, 8 state-of-the-art IMDL models (1 of which are reproduced from scratch), 2 sets of
                    standard training and evaluation protocols, 15 GPU-accelerated evaluation metrics, and 3 kinds of
                    robustness evaluation. This benchmark and codebase represent a significant leap forward in
                    calibrating the current progress in the IMDL field and inspiring future breakthroughs.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <!-- <a href="under-review.html" id="AutoCost"> -->

                  <papertitle>Beyond Visual Appearances: Privacy-sensitive Objects Identification via Hybrid Graph
                    Reasoning</papertitle>
                  [<a href="https://arxiv.org/abs/2406.12736">arxiv</a>]
                  <br>
                  <b>Zhuohang Jiang</b>, Bingkui Tong, Xia Du, Ahmed Alhammadi, Jizhe Zhou

                  <br>
                  <font color='gray'>
                    To explicitly derive the objects' privacy class from the scene contexts, in this paper, we interpret
                    the POI task as a visual reasoning task aimed at the privacy of each object in the scene. Following
                    this interpretation, we propose the PrivacyGuard framework for POI. PrivacyGuard contains three
                    stages. i) Structuring: an unstructured image is first converted into a structured, heterogeneous
                    scene graph that embeds rich scene contexts. ii) Data Augmentation: a contextual perturbation
                    oversampling strategy is proposed to create slightly perturbed privacy-sensitive objects in a scene
                    graph, thereby balancing the skewed distribution of privacy classes. iii) Hybrid Graph Generation &
                    Reasoning: the balanced, heterogeneous scene graph is then transformed into a hybrid graph by
                    endowing it with extra "node-node" and "edge-edge" homogeneous paths. These homogeneous paths allow
                    direct message passing between nodes or edges, thereby accelerating reasoning and facilitating the
                    capturing of subtle context changes.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>




          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <!-- <a href="under-review.html" id="AutoCost"> -->

                  <papertitle>IML-ViT: Benchmarking Image Manipulation Localization by Vision Transformer</papertitle>
                  [<a href="https://arxiv.org/abs/2307.14863">arxiv</a>]
                  <br>
                  Xiaochen Ma, Bo Du,<b>Zhuohang Jiang</b>, Ahmed Y. Al Hammadi, Jizhe Zhou

                  <br>
                  <font color='gray'>
                    Due to limited datasets, there is currently no pure ViT-based approach for IML to serve as a
                    benchmark, and CNNs dominate the entire task. Nevertheless, CNNs suffer from weak long-range and
                    non-semantic modeling. To bridge this gap, based on the fact that artifacts are sensitive to image
                    resolution, amplified under multi-scale features, and massive at the manipulation border, we
                    formulate the answer to the former question as building a ViT with high-resolution capacity,
                    multi-scale feature extraction capability, and manipulation edge supervision that could converge
                    with a small amount of data. We term this simple but effective ViT paradigm IML-ViT, which has
                    significant potential to become a new benchmark for IML. Extensive experiments on five benchmark
                    datasets verified our model outperforms the state-of-the-art manipulation localization methods.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <!-- <a href="under-review.html" id="AutoCost"> -->

                  <papertitle>Perceptual MAE for Image Manipulation Localization: A High-level Vision Learner Focusing
                    on Low-level Features</papertitle> [<a href="https://arxiv.org/abs/2310.06525">arxiv</a>]
                  <br>
                  Xiaochen Ma, <b>Zhuohang Jiang</b>, Xiong Xu, Chi-Man Pun, Jizhe Zhou
                  <br>
                  <font color='gray'> This necessitates IML models to carry out a semantic understanding of the entire
                    image. In this paper, we
                    reformulate the IML task as a high‚Äëlevel
                    vision task that greatly benefits from low‚Äëlevel features. We propose a method to enhance the Masked
                    Autoencoder (MAE) by incorporating
                    high‚Äëresolution inputs and a perceptual loss supervision module, which we term Perceptual MAE
                    (PMAE). While
                    MAE has demonstrated an
                    impressive understanding of object semantics, PMAE can also comprehend low‚Äëlevel semantics with our
                    proposed
                    enhancements. This
                    paradigm effectively unites the low‚Äëlevel and high‚Äëlevel features of the IML task and outperforms
                    state‚Äëof‚Äëthe‚Äëart tampering localization methods
                    on five publicly available datasets, as evidenced by extensive experiments.</font>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <!-- <a href="under-review.html" id="AutoCost"> -->
                  <papertitle>Contour‚ÄëAware Contrastive Learning for Image Manipulation Localization</papertitle>
                  <br>
                  Qin Li, Chunfang Yu, <b>Zhuohang Jiang</b>, AND Jizhe Zhou
                  <br>
                  <font color='gray'>We propose a novel Contour‚Äëaware Contrastive Learning Network (CaCL‚ÄëNet) based on
                    the encoder‚Äëdecoder architecture. On the encoder side,
                    since the contour is foremost concerned in IML, we consider the image patches sampled along the
                    manipulation contour are the hard examples
                    and set them as the anchor. The patches of pure tampered and authentic pixels are set as positives
                    and negatives respectively to conduct
                    contrastive learning. The decoder then manages to specify the manipulated regions and restores the
                    explicit contours of the manipulations
                    through the proposed Contour Binary Cross‚ÄëEntropy (CBCE) loss.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Research Projects</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <!-- <a href="under-review.html" id="AutoCost"> -->

                  <papertitle>Research on Scene Graph Structure Learning Method for Private Object Detection
                  </papertitle>

                  <br>
                  Advisor: Jizhe Zhou
                  <br>
                  <strong> Participate as an intern</strong>
                  <br>
                  <em>National Natural Science Foundation of China </em>,&nbsp;2024&nbsp;
                  <br>
                  <font color='gray'>The privacy-sensitive object detection problem reauires the model to locate private
                    objects in bounding boxes on images or videos. Research on privacy-sensitive object detection has
                    imnortant value for personal-privacyprotection. Privacy-sensitive ob ject detection is actually a
                    scene reasoning problem. However existing privacy-sensitive oh iect detection methods are all
                    basedon the object detection framework.Due to the lack of scene reasoning ability,existing methods
                    suffer from detection accuracy,generalizability,and interpretability.This project intends to build a
                    set of privacy-sensitive objectdetection methods with scene reasoning capability through scene
                    graphs. Unlikeother tasks, privacy-sensitive object detection requires a non-parametric scenegraph
                    structure to keep the graph sparseÔºådynamicÔºåand interpretable.Therefore,this project correspondingly
                    proposes the scene graph structure learning methods.By studying 1) the distillation method of the
                    graph structure to sparse the scenegraphÔºå2) the transferring method between the scene graphs of
                    different frames tomake the scene graph dynamicÔºå3) the privacy-rule reasoning method based on
                    thescene graph structureÔºåsolves the problem of scene graph generation with thenon-parametric graph
                    structureÔºå builds a new privacy-sensitive object detectionframework based on scene reasoningÔºå break
                    the bottlenecks of privacy-sensitiveobject detection methods in accuracyÔºågeneralizability,and
                    interpretability.Thisprivacy-sensitive object detection framework also enriched the theoretical
                    framework and application scenarios of neural networks.</font>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <!-- <a href="under-review.html" id="AutoCost"> -->

                  <papertitle>Intelligent control and full life feedforward deduction technology through pre planning
                    and post evaluation</papertitle>

                  <br>
                  Advisor: Jizhe Zhou
                  <br>
                  <strong> Participate as an intern</strong>
                  <br>
                  <em>National Key R&D
                    Program of China</em>,&nbsp;2023&nbsp;
                  <br>
                  <font color='gray'>This topic proposes to adopt the scheme of "first completing information, then path
                    reasoning". Specifically, this sub project intends to improve the initial network diagram based on
                    the data base established in previous projects and further consider the frequency of common
                    occurrence of impact factors. Then, based on the external knowledge causal information completion
                    method of remote supervision, the network graph is again completed and cleaned, and then a path
                    reasoning algorithm based on depth first traversal of the graph is used to achieve feedforward
                    reasoning. Finally, a human-computer interactive network information verification method based on
                    uncertainty reasoning is used to revise the causal relationship in the network diagram again based
                    on artificial feedback on the reasoning path, further improving the accuracy and recall rate of the
                    path reasoning algorithm results.</font>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Education</heading>
                </td>
              </tr>
              <tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                  <b>Sichuan University</b>, Chengdu, Sichuan, China
                  <br>
                  B.E. in Computer Science and Technology ‚Ä¢ Sep. 2020 to Jun. 2024
                  </br>
                </td>
                <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img
                    src="images/SichuanUniversityLOGO.png" width="75" height="75"></td>
              </tr>
            </tbody>
          </table>
          <br>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                  <b>Hong Kong Polytechnic University</b>, Hongkong, China
                  <br>
                  PHD. in Computer Science and Technology ‚Ä¢ Sep. 2024 to Present
                  </br>
                </td>
                <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img src="images/polyu.jpg"
                    width="75" height="75"></td>
              </tr>
            </tbody>

          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Experience</heading>
                </td>
              </tr>
              <tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                  <a href="http://dicalab.cn/"> DICALab</a>, <b>Sichuan University</b>
                  <br>
                  Research Assistant ‚Ä¢ Sep. 2022 to Jun. 2024
                  <br>
                  Advisor: <a href="https://scholar.google.com/citations?user=-cNWmJMAAAAJ&hl=zh-CN"> Prof. JiZhe
                    Zhou</a>
                  </br>
                </td>
                <td style="padding:0px 20px 0px 20px;width:8%;vertical-align:middle"><img src="images/DICA.png"
                    width="75" height="75"></td>
              </tr>
            </tbody>
          </table>
          <br>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/covscript/covscript"> Covariant association</a>, <b>Sichuan University</b>
                  <br>
                  the president of the covariant association ‚Ä¢ Sep. 2022 to Jun. 2023
                  <br>
                  the Covariant association obtain at least <font color="red"><em><strong>400+</strong></em></font>
                  members, in which communicate the technology of the
                  computer science.
                </td>
                <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img src="images/covariant.jpg"
                    width="75" height="75"></td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Award</heading>
                </td>
              </tr>
              <tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <strong>Computer Design Competition</strong> China, 2023
                </td>
                <td style="width:10%;vertical-align:middle; width: 40%;">
                  <strong>Provincial First Prize</strong>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <strong>Comprehensive First Class Scholarship</strong> Sichuan University, Sichuan, China, 2022
                </td>
                <td style="width:10%;vertical-align:middle; width: 40%;">
                  <strong>Top 1%</strong>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <strong>Outstanding students of Sichuan University</strong> Sichuan University, Sichuan, China, 2022
                </td>
                <td style="width:10%;vertical-align:middle; width: 40%;">
                  <strong>Top 5%</strong>
                </td>
              </tr>
            </tbody>
          </table>












          <div style="text-align: center;">
            <a href="https://clustrmaps.com/site/1buac" title="Visit tracker">
              <img src="//www.clustrmaps.com/map_v2.png?d=arwquO5nouqZwMbhchX9awZ1bM3tO8fPWrcUr78F1E4&cl=ffffff" />
            </a>
          </div>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px">
                  <div style="float:left;">
                    Updated at Jun. 2024
                  </div>
                  <div style="float:right;">
                    Thanks <a href="https://jonbarron.info">Jon Barron</a> for this amazing template
                  </div>
                  <br>
                  <br>
                  <p></p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>


  <script>
    function showMoreNews() {
      var moreNews = document.getElementsByClassName('more-news');
      let isAllVisible = true;
      for (let i = 0; i < moreNews.length; i++) {
        if (moreNews[i].style.display === 'none') {
          moreNews[i].style.display = 'block';
          isAllVisible = false;
        } else {
          moreNews[i].style.display = 'none';
        }
      }
      document.getElementById('expand-btn').textContent = isAllVisible ? 'More News' : 'Less News';
    }


  </script>
</body>


</html>