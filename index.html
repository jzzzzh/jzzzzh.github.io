<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhuohang Jiang</title>

  <meta name="author" content="Zhuohang Jiang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🌐</text></svg>">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css" />
  <link rel="stylesheet" href="magic/dist/magic.css">
  <link rel="stylesheet" href="hover.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/wow/1.1.2/wow.min.js"></script>
  <style>
    .aligned-table {
      width: 100% !important;
      table-layout: fixed !important;
      margin-bottom: 10px !important;
    }

    .aligned-table td:first-child {
      width: 60% !important;
      text-align: left !important;
    }

    .aligned-table td:last-child {
      width: 40% !important;
      text-align: right !important;
    }

    /* 确保Award和Professional Service章节的右侧内容完美对齐 */
    table[style*="table-layout: fixed"] td:last-child {
      text-align: right !important;
      padding-right: 0 !important;
    }

    table[style*="table-layout: fixed"] td:first-child {
      text-align: left !important;
      padding-left: 0 !important;
    }

    /* 语言切换按钮样式 */
    .language-toggle {
      position: fixed;
      top: 20px;
      right: 20px;
      z-index: 1000;
      display: flex;
      gap: 10px;
    }

    .lang-btn {
      padding: 8px 15px;
      background-color: #007BFF;
      color: white;
      border: none;
      border-radius: 5px;
      cursor: pointer;
      font-size: 14px;
      transition: all 0.3s ease;
    }

    .lang-btn:hover {
      background-color: #0056b3;
      transform: translateY(-2px);
    }

    .lang-btn.active {
      background-color: #28a745;
    }

    /* 深色模式样式 */
    .dark-mode {
      background-color: #1a1a1a !important;
      color: #e0e0e0 !important;
    }

    .dark-mode table {
      background-color: #1a1a1a !important;
      color: #e0e0e0 !important;
    }

    .dark-mode td {
      background-color: #1a1a1a !important;
      color: #e0e0e0 !important;
    }

    .dark-mode heading {
      color: #ffffff !important;
    }

    .dark-mode papertitle {
      color: #ffffff !important;
    }

    .dark-mode a {
      color: #66b3ff !important;
    }

    .dark-mode a:hover {
      color: #99ccff !important;
    }

    .dark-mode hr {
      background: linear-gradient(to right, transparent, #555, transparent) !important;
    }

    .dark-mode .lang-btn {
      background-color: #333 !important;
      color: #e0e0e0 !important;
    }

    .dark-mode .lang-btn:hover {
      background-color: #555 !important;
    }

    .dark-mode .lang-btn.active {
      background-color: #28a745 !important;
    }

    /* 模式切换动画 */
    body {
      transition: background-color 0.5s ease, color 0.5s ease;
    }

    /* 隐藏/显示语言内容 */
    .lang-content {
      display: block;
    }

    .lang-content.hidden {
      display: none;
    }

    /* 确保表格行元素的语言切换正常工作 */
    tr.lang-content {
      display: table-row;
    }

    tr.lang-content.hidden {
      display: none;
    }
  </style>
</head>

<body>
  <!-- 语言切换按钮 -->
  <div class="language-toggle">
    <button class="lang-btn active" onclick="switchLanguage('en')" id="en-btn">English</button>
    <button class="lang-btn" onclick="switchLanguage('zh')" id="zh-btn">中文</button>
  </div>

  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                  <div class="wow animate__animated animate__fadeInDown" style="font-size: 2.5em; font-weight: bold;">
                    <name>
                      <span class="lang-content lang-en">Zhuohang Jiang</span>
                      <span class="lang-content lang-zh hidden">蒋卓航</span>
                    </name>
                  </div>

                  </p>
                  <div>
                    <!-- 英文版本 -->
                    <div class="lang-content lang-en">
                      <p>I am currently pursuing a PhD degree at the <strong> <a href="https://www.polyu.edu.hk/">Hong
                            Kong Polytechnic University</a></strong>. My supervisors are <a
                          href="https://scholar.google.cz/citations?user=D1LEg-YAAAAJ&hl=zh-CN">Qing Li</a>
                        and <a href="https://scholar.google.cz/citations?user=SQ9UbHIAAAAJ&hl=zh-CN&oi=ao">Wenqi
                          Fan</a>.
                        My current Cumulative GPA is <strong>3.60/4.00</strong>.
                      </p>
                      <p>I studied at <a href="https://www.scu.edu.cn">Sichuan University (SCU)</a> from 2020 to 2024,
                        where I majored in Computer Science & Technology. My
                        <strong> Major GPA (CS courses):
                          3.79/4, 89.39/100;</strong>
                        Overall GPA: 3.78/4, 89.25/100
                      </p>
                      <p>
                        During my time at Sichuan University, I worked as a research assistant at
                        <a href="http://www.machineilab.org/">MachineILab</a>
                        from 2022 to 2024, advised by
                        <a href="https://scholar.google.com/citations?user=-cNWmJMAAAAJ&hl=zh-CN">Prof. JiZhe Zhou</a>.
                        I participated in one National Natural Science Foundation of China project and one National Key
                        R&D
                        Program of China.
                      </p>
                    </div>

                    <!-- 中文版本 -->
                    <div class="lang-content lang-zh hidden">
                      <p>我目前在<strong><a href="https://www.polyu.edu.hk/">香港理工大学</a></strong>攻读博士学位。我的导师是<a
                          href="https://scholar.google.cz/citations?user=D1LEg-YAAAAJ&hl=zh-CN">李青教授</a>
                        和<a href="https://scholar.google.cz/citations?user=SQ9UbHIAAAAJ&hl=zh-CN&oi=ao">范文琦教授</a>。
                        我目前的累计GPA是<strong>3.60/4.00</strong>。
                      </p>
                      <p>我于2020年至2024年在<a href="https://www.scu.edu.cn">四川大学</a>就读，主修计算机科学与技术专业。我的
                        <strong>专业GPA（计算机课程）：3.79/4，89.39/100；</strong>
                        综合GPA：3.78/4，89.25/100
                      </p>
                      <p>
                        在四川大学期间，我于2022年至2024年在
                        <a href="http://www.machineilab.org/">MachineILab</a>
                        担任研究助理，导师是
                        <a href="https://scholar.google.com/citations?user=-cNWmJMAAAAJ&hl=zh-CN">周吉喆教授</a>。
                        我参与了一项国家自然科学基金项目和一项国家重点研发计划项目。
                      </p>
                    </div>
                  </div>

                  <p style="text-align:center">
                    <a href="mailto:zhuohangjiang2002@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="data/ZhuohangJiang-CV.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.cz/citations?hl=zh-CN&user=h3vEhrgAAAAJ">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/jzzzzh">Github</a>&nbsp
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/zhuohang.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/zhuohang.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>
                    <span class="lang-content lang-en">Research Topics</span>
                    <span class="lang-content lang-zh hidden">研究方向</span>
                  </heading>
                  <!-- 英文版本 -->
                  <div class="lang-content lang-en">
                    <p>
                      My research interests lie in <strong>large language models (LLMs), retrieval-augmented generation
                        (RAG),
                        and Recommender Systems (RecSys)</strong>. I focus on both <strong>theoretical
                        foundations</strong> and
                      practical applications of LLM-based systems.
                    </p>
                    <p>
                      My previous research was primarily focused on topics within computer vision, such as tampering
                      detection and object recognition tasks. I have contributed to the design of high-impact
                      benchmarks,
                      such as <strong>HiBench</strong>, and comprehensive surveys like <strong>WebAgents</strong>.
                      My work has been published in <strong>top-tier conferences</strong>, including
                      <strong>NeurIPS 2024</strong> (spotlight) and <strong>AAAI 2025</strong>, and has accumulated
                      <strong>100+ citations</strong>, with an <strong>h-index of 4</strong>.
                    </p>
                  </div>
                  <!-- 中文版本 -->
                  <div class="lang-content lang-zh hidden">
                    <p>
                      我的研究兴趣主要集中在<strong>大语言模型（LLMs）、检索增强生成（RAG）和推荐系统（RecSys）</strong>。
                      我专注于基于LLM系统的<strong>理论基础</strong>和实际应用。
                    </p>
                    <p>
                      我之前的研究主要集中在计算机视觉领域，如图像篡改检测和目标识别任务。我为高影响力基准测试的设计做出了贡献，
                      如<strong>HiBench</strong>，以及综合性调研如<strong>WebAgents</strong>。
                      我的工作发表在<strong>顶级会议</strong>上，包括<strong>NeurIPS 2024</strong>（spotlight）和<strong>AAAI
                        2025</strong>，
                      累计获得<strong>100+引用</strong>，<strong>h指数为4</strong>。
                    </p>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>
          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>
                    <span class="lang-content lang-en">News</span>
                    <span class="lang-content lang-zh hidden">新闻动态</span>
                  </heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <!-- 英文版本 -->
                  <div class="lang-content lang-en">
                    <div>
                      <p> <strong>🏆 2025-08-20</strong> - Our work <strong><a
                            href="https://arxiv.org/abs/2508.05197">QA-Dragon</a></strong> was accepted by <strong>KDD
                          2025 Workshop for Multimodal Retrieval Augmented Generation</strong>.</p>
                    </div>
                    <div>
                      <p> <strong>🎤 2025-08-07</strong> - Invited to give a talk "Understanding Hierarchical Data with
                        Large Language Models: RAG, Structural Reasoning, and Future Directions" at <strong>KDD 2025
                          Reasoning Day</strong> in Toronto, Canada! 🎙️</p>
                    </div>
                    <div>
                      <p> <strong>🏅 2025-06-18</strong> - Achieved <strong>12th place globally</strong> in KDD Cup 2025
                        -
                        Meta CRAG-MM Multimodal Retrieval Challenge among hundreds of international teams! 🌍</p>
                    </div>
                    <div>
                      <p> <strong>🏆 2025-05-16</strong> - Our benchmark paper <a
                          href="https://arxiv.org/abs/2503.00912">HiBench</a> was accepted to <strong>KDD Benchmark
                          Track</strong>! 🎉</p>
                    </div>
                    <div>
                      <p> <strong>🎉 2025-05-07</strong> - Our survey paper <a href="https://arxiv.org/abs/2503.23350">A
                          Survey
                          of WebAgents: Towards Next-Generation AI Agents for Web Automation with Large Foundation
                          Models</a> was accepted to <strong>KDD Tutorial Track</strong>! 🎊</p>
                    </div>
                    <div>
                      <p> <strong>📜 2025-03-30</strong> - Completed the survey paper <a
                          href="https://arxiv.org/abs/2503.23350">A Survey of WebAgents: Towards Next-Generation AI
                          Agents
                          for Web Automation with Large Foundation Models</a>.</p>
                    </div>
                    <div>
                      <p> <strong>📘 2025-03-01</strong> - Completed the <a
                          href="https://arxiv.org/abs/2503.00912">HiBench</a> paper and released the code and dataset on
                        <a href="https://github.com/jzzzzh/HiBench">GitHub</a> and
                        <a href="https://huggingface.co/datasets/zhuohang/HiBench">Hugging Face</a>.
                      </p>
                    </div>

                    <!-- 剩余的新闻默认隐藏 -->

                    <div class="more-news">

                      <p> <strong>🌟 2025-01-15</strong> - <a
                          href="https://ojs.aaai.org/index.php/AAAI/article/view/33198">Mesoscopic Insights:
                          Orchestrating
                          Multi-Scale & Hybrid Architecture for Image Manipulation Localization</a> was published in
                        AAAI
                        2025.</p>

                      <p> <strong>🏆 2024-12-01</strong> - <a href="https://arxiv.org/abs/2406.10580">IMDL-BenCo </a>was
                        published in NeurIPS 2024 Benchmark Tracks and received a Spotlight award.</p>

                      <p> <strong>🎓 2024-09-01</strong> - Beginning my pursuit of a PhD degree in Hong Kong PolyU.</p>

                      <p> <strong>🎓 2024-06-26</strong> - Got <strong>Outstanding Graduate Award</strong> from Sichuan
                        University and Sichuan Province! 🎉</p>

                      <p> <strong>🎓 2024-06-26</strong> - Graduated from Sichuan University with a bachelor's degree.
                      </p>

                      <p> <strong>🛠️ 2024-06-12</strong> - Completed the co-work project <a
                          href="https://github.com/scu-zjz/IMDLBenCo"> IMDLBenCo</a> and finished a paper
                        <a href="https://arxiv.org/abs/2406.10580"><strong>IMDL-BenCo: A Comprehensive Benchmark and
                            Codebase for Image Manipulation Detection & Localization</strong></a>
                      </p>

                      <p> <strong>🔍 2024-05-24</strong> - Finished a paper <a
                          href="https://arxiv.org/abs/2406.12736"><strong>Beyond
                            Visual Appearances: Privacy-sensitive Objects Identification via Hybrid Graph
                            Reasoning</strong></a> </p <p> <strong>📚 2023-08-01</strong> - Participated in <strong>NUS
                        Summer School</strong> research
                      program at National University of Singapore, completed face recognition project! 🇸🇬</p>
                    </div>
                  </div>

                  <!-- 中文版本 -->
                  <div class="lang-content lang-zh hidden">
                    <div>
                      <p> <strong>🏆 2025-08-20</strong> - 我们的工作 <strong><a
                            href="https://arxiv.org/abs/2508.05197">QA-Dragon</a></strong> 被<strong>KDD
                          2025多模态检索增强生成研讨会</strong>接收。</p>
                    </div>
                    <div>
                      <p> <strong>🎤 2025-08-07</strong> - 受邀在加拿大多伦多<strong>KDD
                          2025推理日</strong>做题为"用大语言模型理解层次化数据：RAG、结构推理与未来方向"的报告！🎙️</p>
                    </div>
                    <div>
                      <p> <strong>🏅 2025-06-18</strong> - 在KDD Cup 2025 - Meta
                        CRAG-MM多模态检索挑战赛中，在数百个国际团队中取得<strong>全球第12名</strong>的成绩！🌍</p>
                    </div>
                    <div>
                      <p> <strong>🏆 2025-05-16</strong> - 我们的基准测试论文<a
                          href="https://arxiv.org/abs/2503.00912">HiBench</a>被<strong>KDD基准测试赛道</strong>接收！🎉</p>
                    </div>
                    <div>
                      <p> <strong>🎉 2025-05-07</strong> - 我们的调研论文<a
                          href="https://arxiv.org/abs/2503.23350">网络智能体调研：基于大型基础模型的下一代网络自动化AI智能体</a>被<strong>KDD教程赛道</strong>接收！🎊
                      </p>
                    </div>
                    <div>
                      <p> <strong>📜 2025-03-30</strong> - 完成调研论文<a
                          href="https://arxiv.org/abs/2503.23350">网络智能体调研：基于大型基础模型的下一代网络自动化AI智能体</a>。</p>
                    </div>
                    <div>
                      <p> <strong>📘 2025-03-01</strong> - 完成<a href="https://arxiv.org/abs/2503.00912">HiBench</a>论文，并在
                        <a href="https://github.com/jzzzzh/HiBench">GitHub</a>和
                        <a href="https://huggingface.co/datasets/zhuohang/HiBench">Hugging Face</a>上发布代码和数据集。
                      </p>
                    </div>

                    <!-- 剩余的新闻默认隐藏 -->

                    <div class="more-news">

                      <p> <strong>🌟 2025-01-15</strong> - <a
                          href="https://ojs.aaai.org/index.php/AAAI/article/view/33198">中观洞察：面向图像操作定位的多尺度混合架构编排</a>在AAAI
                        2025上发表。</p>

                      <p> <strong>🏆 2024-12-01</strong> - <a
                          href="https://arxiv.org/abs/2406.10580">IMDL-BenCo</a>在NeurIPS 2024基准测试赛道发表并获得Spotlight奖。</p>

                      <p> <strong>🎓 2024-09-01</strong> - 开始在香港理工大学攻读博士学位。</p>

                      <p> <strong>🎓 2024-06-26</strong> - 获得四川大学和四川省<strong>优秀毕业生</strong>称号！🎉</p>

                      <p> <strong>🎓 2024-06-26</strong> - 从四川大学获得学士学位。</p>

                      <p> <strong>🛠️ 2024-06-12</strong> - 完成合作项目<a
                          href="https://github.com/scu-zjz/IMDLBenCo">IMDLBenCo</a>并完成论文
                        <a href="https://arxiv.org/abs/2406.10580"><strong>IMDL-BenCo：图像操作检测与定位的综合基准测试和代码库</strong></a>
                      </p>

                      <p> <strong>🔍 2024-05-24</strong> - 完成论文<a
                          href="https://arxiv.org/abs/2406.12736"><strong>超越视觉外观：基于混合图推理的隐私敏感对象识别</strong></a></p>
                      <p> <strong>📚 2023-08-01</strong> - 参加新加坡国立大学<strong>暑期学校</strong>研究项目，完成人脸识别项目！🇸🇬</p>
                    </div>
                  </div>

                  <div style="text-align: center; margin-top: 20px;">
                    <button id="expand-btn" onclick="showMoreNews()" class="hvr-grow"
                      style="padding: 10px 20px; font-size: 16px; background-color: #007BFF; color: white; border: none; border-radius: 5px; cursor: pointer;">
                      <span class="lang-content lang-en">More News</span>
                      <span class="lang-content lang-zh hidden">更多新闻</span>
                    </button>
                  </div>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>
          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>
                    <span class="lang-content lang-en">Selected Publications</span>
                    <span class="lang-content lang-zh hidden">主要论文发表</span>
                  </heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>[KDD'25 Workshop] QA‑Dragon: Query‑Aware Dynamic RAG System for Knowledge‑Intensive Visual
                    Question Answering</papertitle>
                  <br>
                  <b>Zhuohang Jiang*</b>, Pangjing Wu*, Xu Yuan*, Wenqi Fan, Qing Li
                  <br>
                  <img src="https://img.shields.io/badge/CCF_A-KDD-1f77b4?style=flat-square" alt="CCF-A">
                  <a href="https://arxiv.org/abs/2508.05197"><img
                      src="https://img.shields.io/badge/arXiv-2508.05197-b31b1b?style=flat-square" alt="arXiv"></a>
                  <br>
                  <font color='gray'>
                    Retrieval-Augmented Generation (RAG) has been introduced to mitigate hallucinations in Multimodal
                    Large Language Models (MLLMs) by incorporating external knowledge into the generation process, and
                    it has become a widely adopted approach for knowledge-intensive Visual Question Answering (VQA).
                    However, existing RAG methods typically retrieve from either text or images in isolation, limiting
                    their ability to address complex queries that require multi-hop reasoning or up-to-date factual
                    knowledge. To address this limitation, we propose QA-Dragon, a Query-Aware Dynamic RAG System for
                    Knowledge-Intensive VQA. Specifically, QA-Dragon introduces a domain router to identify the query's
                    subject domain for domain-specific reasoning, along with a search router that dynamically selects
                    optimal retrieval strategies. By orchestrating both text and image search agents in a hybrid setup,
                    our system supports multimodal, multi-turn, and multi-hop reasoning, enabling it to tackle complex
                    VQA tasks effectively. We evaluate our QA-Dragon on the Meta CRAG-MM Challenge at KDD Cup 2025,
                    where it significantly enhances the reasoning performance of base models under challenging
                    scenarios. Our framework achieves substantial improvements in both answer accuracy and knowledge
                    overlap scores, outperforming baselines by 5.06% on the single-source task, 6.35% on the
                    multi-source task, and 5.03% on the multi-turn task.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>[KDD'25] A Survey of WebAgents: Towards Next-Generation AI Agents for Web Automation with
                    Large
                    Foundation Models</papertitle>
                  <br>
                  Liangbo Ning, Ziran Liang, <b>Zhuohang Jiang</b>, Haohao Qu, Yujuan Ding, Wenqi Fan, Xiao-yong Wei,
                  Shanru Lin, Hui Liu, Philip S. Yu, Qing Li
                  <br>
                  <img src="https://img.shields.io/badge/CCF_A-KDD-1f77b4?style=flat-square" alt="CCF-A">
                  <a href="https://arxiv.org/abs/2503.23350"><img
                      src="https://img.shields.io/badge/arXiv-2503.23350-b31b1b?style=flat-square" alt="arXiv"></a>
                  <br>
                  <font color='gray'>
                    With the advancement of web techniques, they have significantly revolutionized various aspects of
                    people's lives. Despite the importance of the web, many tasks performed on it are repetitive and
                    time-consuming, negatively impacting overall quality of life. To efficiently handle these tedious
                    daily tasks, one of the most promising approaches is to advance autonomous agents based on
                    Artificial Intelligence (AI) techniques, referred to as AI Agents, as they can operate continuously
                    without fatigue or performance degradation. In the context of the web, leveraging AI Agents --
                    termed WebAgents -- to automatically assist people in handling tedious daily tasks can dramatically
                    enhance productivity and efficiency. Recently, Large Foundation Models (LFMs) containing billions of
                    parameters have exhibited human-like language understanding and reasoning capabilities, showing
                    proficiency in performing various complex tasks. This naturally raises the question: `Can LFMs be
                    utilized to develop powerful AI Agents that automatically handle web tasks, providing significant
                    convenience to users?' To fully explore the potential of LFMs, extensive research has emerged on
                    WebAgents designed to complete daily web tasks according to user instructions, significantly
                    enhancing the convenience of daily human life. In this survey, we comprehensively review existing
                    research studies on WebAgents across three key aspects: architectures, training, and
                    trustworthiness. Additionally, several promising directions for future research are explored to
                    provide deeper insights.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>[KDD'25] HiBench: Benchmarking LLMs Capability on Hierarchical Structure Reasoning
                  </papertitle>

                  <br>
                  <b>Zhuohang Jiang*</b>, Pangjing Wu*, Ziran Liang*, Peter Q. Chen*, Xu Yuan*, Ye Jia*, Jiancheng Tu*,
                  Chen
                  Li, Peter H.F. Ng, Qing Li
                  <br>
                  <img src="https://img.shields.io/badge/CCF_A-KDD-1f77b4?style=flat-square" alt="CCF-A">
                  <a href="https://arxiv.org/abs/2503.00912"><img
                      src="https://img.shields.io/badge/arXiv-2503.00912-b31b1b?style=flat-square" alt="arXiv"></a>
                  <a href="https://github.com/jzzzzh/HiBench"><img
                      src="https://img.shields.io/badge/GitHub-HiBench-181717?style=flat-square&logo=github"
                      alt="GitHub"></a>
                  <a href="https://huggingface.co/datasets/zhuohang/HiBench"><img
                      src="https://img.shields.io/badge/Hugging%20Face-Dataset-yellow?style=flat-square&logo=huggingface"
                      alt="Hugging Face"></a>
                  <br>
                  <font color='gray'>
                    Structure reasoning is a fundamental capability of large language models (LLMs), enabling them to
                    reason about structured commonsense and answer multi-hop questions. However, existing benchmarks for
                    structure reasoning mainly focus on horizontal and coordinate structures (\emph{e.g.} graphs),
                    overlooking the hierarchical relationships within them. Hierarchical structure reasoning is crucial
                    for human cognition, particularly in memory organization and problem-solving. It also plays a key
                    role in various real-world tasks, such as information extraction and decision-making. To address
                    this gap, we propose HiBench, the first framework spanning from initial structure generation to
                    final proficiency assessment, designed to benchmark the hierarchical reasoning capabilities of LLMs
                    systematically. HiBench encompasses six representative scenarios, covering both fundamental and
                    practical aspects, and consists of 30 tasks with varying hierarchical complexity, totaling 39,519
                    queries. To evaluate LLMs comprehensively, we develop five capability dimensions that depict
                    different facets of hierarchical structure understanding. Through extensive evaluation of 20 LLMs
                    from 10 model families, we reveal key insights into their capabilities and limitations: 1) existing
                    LLMs show proficiency in basic hierarchical reasoning tasks; 2) they still struggle with more
                    complex structures and implicit hierarchical representations, especially in structural modification
                    and textual reasoning. Based on these findings, we create a small yet well-designed instruction
                    dataset, which enhances LLMs' performance on HiBench by an average of 88.84% (Llama-3.1-8B) and
                    31.38% (Qwen2.5-7B) across all tasks. The HiBench dataset and toolkit are available here, this https
                    URL, to encourage evaluation.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>[AAAI'25] Mesoscopic Insights: Orchestrating Multi-Scale & Hybrid Architecture for Image
                    Manipulation Localization</papertitle>
                  <br>
                  Xuekang Zhu, Xiaochen Ma, Lei Su, <b>Zhuohang Jiang</b>, Bo Du, Xiwen Wang, Zeyu Lei, Wentao Feng,
                  Chi-Man Pun, Jizhe Zhou
                  <br>
                  <img src="https://img.shields.io/badge/CCF_A-AAAI-1f77b4?style=flat-square" alt="CCF-A">
                  <a href="https://arxiv.org/abs/2412.13753"><img
                      src="https://img.shields.io/badge/arXiv-2412.13753-b31b1b?style=flat-square" alt="arXiv"></a>
                  <br>
                  <font color='gray'>
                    The mesoscopic level serves as a bridge between the macroscopic and microscopic worlds, addressing
                    gaps overlooked by both. Image manipulation localization (IML), a crucial technique to pursue truth
                    from fake images, has long relied on low-level (microscopic-level) traces. However, in practice,
                    most tampering aims to deceive the audience by altering image semantics. As a result, manipulation
                    commonly occurs at the object level (macroscopic level), which is equally important as microscopic
                    traces. Therefore, integrating these two levels into the mesoscopic level presents a new perspective
                    for IML research. Inspired by this, our paper explores how to simultaneously construct mesoscopic
                    representations of micro and macro information for IML and introduces the Mesorch architecture to
                    orchestrate both. Specifically, this architecture i) combines Transformers and CNNs in parallel,
                    with Transformers extracting macro information and CNNs capturing micro details, and ii) explores
                    across different scales, assessing micro and macro information seamlessly. Additionally, based on
                    the Mesorch architecture, the paper introduces two baseline models aimed at solving IML tasks
                    through mesoscopic representation. Extensive experiments across four datasets have demonstrated that
                    our models surpass the current state-of-the-art in terms of performance, computational complexity,
                    and robustness.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <!-- <a href="under-review.html" id="AutoCost"> -->

                  <papertitle>[NIPS'24] IMDL-BenCo: A Comprehensive Benchmark and Codebase for Image Manipulation
                    Detection &
                    Localization</papertitle>
                  <br>
                  Xiaochen Ma*, Xuekang Zhu*, Lei Su*, Bo Du*, <b>Zhuohang Jiang*</b>, Bingkui Tong*, Zeyu Lei*, Xinyu
                  Yang*,
                  Chi-Man Pun, Jiancheng Lv, Jizhe Zhou
                  <br>
                  <img src="https://img.shields.io/badge/CCF_A-NIPS-1f77b4?style=flat-square" alt="CCF-A">
                  <a href="https://arxiv.org/abs/2406.10580"><img
                      src="https://img.shields.io/badge/arXiv-2406.10580-b31b1b?style=flat-square" alt="arXiv"></a>
                  <a href="https://github.com/scu-zjz/IMDLBenCo"><img
                      src="https://img.shields.io/badge/GitHub-IMDL--BenCo-181717?style=flat-square&logo=github"
                      alt="GitHub"></a>
                  <br>
                  <font color='gray'>
                    A comprehensive benchmark is yet to be established in the Image Manipulation Detection &
                    Localization (IMDL) field. The absence of such a benchmark leads to insufficient and misleading
                    model evaluations, severely undermining the development of this field. However, the scarcity of
                    open-sourced baseline models and inconsistent training and evaluation protocols make conducting
                    rigorous experiments and faithful comparisons among IMDL models challenging. To address these
                    challenges, we introduce IMDL-BenCo, the first comprehensive IMDL benchmark and modular codebase.
                    IMDL-BenCo:i) decomposes the IMDL framework into standardized, reusable components and
                    revises the model construction pipeline, improving coding efficiency and customization
                    flexibility;ii) fully implements or incorporates training code for state-of-the-art models
                    to establish a comprehensive IMDL benchmark; and iii) conducts deep analysis based on the
                    established benchmark and codebase, offering new insights into IMDL model architecture, dataset
                    characteristics, and evaluation standards. Specifically, IMDL-BenCo includes common processing
                    algorithms, 8 state-of-the-art IMDL models (1 of which are reproduced from scratch), 2 sets of
                    standard training and evaluation protocols, 15 GPU-accelerated evaluation metrics, and 3 kinds of
                    robustness evaluation. This benchmark and codebase represent a significant leap forward in
                    calibrating the current progress in the IMDL field and inspiring future breakthroughs.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <!-- <a href="under-review.html" id="AutoCost"> -->

                  <papertitle>Beyond Visual Appearances: Privacy-sensitive Objects Identification via Hybrid Graph
                    Reasoning</papertitle>
                  <br>
                  <b>Zhuohang Jiang*</b>, Bingkui Tong*, Xia Du, Ahmed Alhammadi, Jizhe Zhou
                  <br>
                  <a href="https://arxiv.org/abs/2406.12736"><img
                      src="https://img.shields.io/badge/arXiv-2406.12736-b31b1b?style=flat-square" alt="arXiv"></a>
                  <br>
                  <b>Zhuohang Jiang*</b>, Bingkui Tong*, Xia Du, Ahmed Alhammadi, Jizhe Zhou

                  <br>
                  <font color='gray'>
                    To explicitly derive the objects' privacy class from the scene contexts, in this paper, we interpret
                    the POI task as a visual reasoning task aimed at the privacy of each object in the scene. Following
                    this interpretation, we propose the PrivacyGuard framework for POI. PrivacyGuard contains three
                    stages. i) Structuring: an unstructured image is first converted into a structured, heterogeneous
                    scene graph that embeds rich scene contexts. ii) Data Augmentation: a contextual perturbation
                    oversampling strategy is proposed to create slightly perturbed privacy-sensitive objects in a scene
                    graph, thereby balancing the skewed distribution of privacy classes. iii) Hybrid Graph Generation &
                    Reasoning: the balanced, heterogeneous scene graph is then transformed into a hybrid graph by
                    endowing it with extra "node-node" and "edge-edge" homogeneous paths. These homogeneous paths allow
                    direct message passing between nodes or edges, thereby accelerating reasoning and facilitating the
                    capturing of subtle context changes.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>





          <div class="more-publications" style="display: none;">
            <!-- Remaining publications go here -->

            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                    <!-- <a href="under-review.html" id="AutoCost"> -->

                    <papertitle>[ICONIP'23] TPTGAN: Two-Path Transformer-Based Generative Adversarial Network Using
                      Joint Magnitude Masking and Complex Spectral Mapping for Speech Enhancement</papertitle>
                    <br>
                    Zhaoyi Liu, <b>Zhuohang Jiang</b>, Wendian Luo, Zhuoyao Fan, Haoda Di, Yufan Long, Haizhou Wang
                    <br>
                    <img src="https://img.shields.io/badge/CCF_C-ICONIP-ff7f0e?style=flat-square" alt="CCF-C">
                    <a href="https://link.springer.com/chapter/10.1007/978-981-99-8138-0_5"><img
                        src="https://img.shields.io/badge/Paper-springer-2e8b57?style=flat-square" alt="Paper"></a>
                    <br>
                    <font color='gray'>
                      In this paper, we propose a two-path transformer-based metric generative adversarial network
                      (TPTGAN) for speech enhancement in the time-frequency domain. The generator consists of an
                      encoder, a two-stage transformer module, a magnitude mask decoder and a complex spectrum decoder.
                      Published in ICONIP 2023.
                    </font>
                  </td>
                </tr>
              </tbody>
            </table>

            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                    <!-- <a href="under-review.html" id="AutoCost"> -->

                    <papertitle>IML-ViT: Benchmarking Image Manipulation Localization by Vision Transformer</papertitle>
                    <br>
                    Xiaochen Ma, Bo Du, <b>Zhuohang Jiang</b>, Ahmed Y. Al Hammadi, Jizhe Zhou
                    <br>
                    <a href="https://arxiv.org/abs/2307.14863"><img
                        src="https://img.shields.io/badge/arXiv-2307.14863-b31b1b?style=flat-square" alt="arXiv"></a>
                    <br>
                    <font color='gray'>
                      Due to limited datasets, there is currently no pure ViT-based approach for IML to serve as a
                      benchmark, and CNNs dominate the entire task. Nevertheless, CNNs suffer from weak long-range and
                      non-semantic modeling. To bridge this gap, based on the fact that artifacts are sensitive to image
                      resolution, amplified under multi-scale features, and massive at the manipulation border, we
                      formulate the answer to the former question as building a ViT with high-resolution capacity,
                      multi-scale feature extraction capability, and manipulation edge supervision that could converge
                      with a small amount of data. We term this simple but effective ViT paradigm IML-ViT, which has
                      significant potential to become a new benchmark for IML. Extensive experiments on five benchmark
                      datasets verified our model outperforms the state-of-the-art manipulation localization methods.
                    </font>
                  </td>
                </tr>
              </tbody>
            </table>


            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                    <!-- <a href="under-review.html" id="AutoCost"> -->

                    <papertitle>Perceptual MAE for Image Manipulation Localization: A High-level Vision Learner Focusing
                      on Low-level Features</papertitle>
                    <br>
                    Xiaochen Ma, <b>Zhuohang Jiang</b>, Xiong Xu, Chi-Man Pun, Jizhe Zhou
                    <br>
                    <a href="https://arxiv.org/abs/2310.06525"><img
                        src="https://img.shields.io/badge/arXiv-2310.06525-b31b1b?style=flat-square" alt="arXiv"></a>
                    <br>
                    <font color='gray'> This necessitates IML models to carry out a semantic understanding of the entire
                      image. In this paper, we
                      reformulate the IML task as a high‑level
                      vision task that greatly benefits from low‑level features. We propose a method to enhance the
                      Masked
                      Autoencoder (MAE) by incorporating
                      high‑resolution inputs and a perceptual loss supervision module, which we term Perceptual MAE
                      (PMAE). While
                      MAE has demonstrated an
                      impressive understanding of object semantics, PMAE can also comprehend low‑level semantics with
                      our
                      proposed
                      enhancements. This
                      paradigm effectively unites the low‑level and high‑level features of the IML task and outperforms
                      state‑of‑the‑art tampering localization methods
                      on five publicly available datasets, as evidenced by extensive experiments.</font>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>

          <div style="text-align: center; margin-top: 20px;">
            <button id="expand-publications-btn" onclick="togglePublications()" class="hvr-grow"
              style="padding: 10px 20px; font-size: 16px; background-color: #007BFF; color: white; border: none; border-radius: 5px; cursor: pointer;">
              <span class="lang-content lang-en">More Publications</span>
              <span class="lang-content lang-zh hidden">更多论文</span>
            </button>
          </div>
          </div>

          <script>
            function togglePublications() {
              var morePublications = document.querySelector('.more-publications');
              var button = document.getElementById('expand-publications-btn');
              button.addEventListener('click', function () {
                if (morePublications.style.display === 'none') {
                  morePublications.style.display = 'block';
                  button.classList.remove('hvr-grow');
                  button.classList.add('hvr-shrink');
                  // 更新按钮文本 - 支持多语言
                  const enText = button.querySelector('.lang-en');
                  const zhText = button.querySelector('.lang-zh');
                  if (enText) enText.textContent = 'Less Publications';
                  if (zhText) zhText.textContent = '收起论文';
                  morePublications.classList.add('magictime', 'puffIn');
                } else {
                  morePublications.style.display = 'none';
                  button.classList.remove('hvr-shrink');
                  button.classList.add('hvr-grow');
                  // 更新按钮文本 - 支持多语言
                  const enText = button.querySelector('.lang-en');
                  const zhText = button.querySelector('.lang-zh');
                  if (enText) enText.textContent = 'More Publications';
                  if (zhText) zhText.textContent = '更多论文';
                }
              });
            }
          </script>

          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>
                    <span class="lang-content lang-en">Selected Projects</span>
                    <span class="lang-content lang-zh hidden">主要项目</span>
                  </heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>
                    <span class="lang-content lang-en">HiBench: Benchmark for Hierarchical Reasoning</span>
                    <span class="lang-content lang-zh hidden">HiBench：层次化推理基准</span>
                  </papertitle>
                  <br>
                  <strong>First Author & Team Leader</strong> • <em>KDD 2025 Benchmark Track</em> • 2025
                  <br>
                  <a href="https://arxiv.org/abs/2503.00912"><img
                      src="https://img.shields.io/badge/arXiv-2503.00912-b31b1b?style=flat-square" alt="arXiv"></a>
                  <a href="https://github.com/jzzzzh/HiBench"><img
                      src="https://img.shields.io/badge/GitHub-HiBench-181717?style=flat-square&logo=github"
                      alt="GitHub"></a>
                  <a href="https://huggingface.co/datasets/zhuohang/HiBench"><img
                      src="https://img.shields.io/badge/Hugging%20Face-Dataset-yellow?style=flat-square&logo=huggingface"
                      alt="Hugging Face"></a>
                  <br>
                  <!-- 英文版本 -->
                  <div class="lang-content lang-en">
                    <font color='gray'>
                      Designed and developed the first comprehensive benchmark for evaluating LLMs' capability on
                      hierarchical structure reasoning.
                      The benchmark encompasses six representative scenarios with 39,519 queries across varying
                      hierarchical complexity.
                      <strong>Key contributions:</strong> (1) Led the architectural design and implementation of the
                      evaluation framework,
                      (2) Coordinated a multi-institutional team across different time zones, (3) Open-sourced the
                      complete toolkit
                      including dataset, evaluation metrics, and baseline implementations. The benchmark has been
                      accepted
                      as an
                      <strong>oral presentation</strong> at KDD 2025 and is being adopted by multiple research groups
                      for
                      hierarchical reasoning evaluation.
                    </font>
                  </div>

                  <!-- 中文版本 -->
                  <div class="lang-content lang-zh hidden">
                    <font color='gray'>
                      设计并开发了首个用于评估大语言模型层次化结构推理能力的综合基准。
                      该基准涵盖6个代表性场景，包含39,519个不同层次复杂度的查询。
                      <strong>主要贡献：</strong> (1) 领导了评估框架的架构设计和实现，
                      (2) 协调了跨时区的多机构团队，(3) 开源了完整的工具包，
                      包括数据集、评估指标和基线实现。该基准已被KDD 2025接收为
                      <strong>口头报告</strong>，正被多个研究团队采用进行层次推理评估。
                    </font>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>
                    <span class="lang-content lang-en">Meta CRAG-MM: Multimodal Retrieval Challenge</span>
                    <span class="lang-content lang-zh hidden">Meta CRAG-MM：多模态检索挑战</span>
                  </papertitle>
                  <br>
                  <strong>Team Leader</strong> • <em>KDD Cup 2025</em> • 2025
                  <br>
                  <!-- 英文版本 -->
                  <div class="lang-content lang-en">
                    <font color='gray'>
                      Led a team to achieve <strong>12th place globally</strong> among hundreds of international teams
                      in
                      the Meta CRAG-MM
                      Multimodal Retrieval Challenge. <strong>Key contributions:</strong> (1) Designed novel multimodal
                      fusion architectures
                      combining vision and language understanding, (2) Implemented efficient retrieval-augmented
                      generation pipelines,
                      (3) Coordinated team efforts in model development, hyperparameter optimization, and submission
                      strategies.
                      The challenge focused on developing AI systems capable of understanding and retrieving information
                      from
                      multimodal content, which aligns with current trends in large multimodal models.
                    </font>
                  </div>

                  <!-- 中文版本 -->
                  <div class="lang-content lang-zh hidden">
                    <font color='gray'>
                      领导团队在Meta CRAG-MM多模态检索挑战赛中，在数百个国际团队中取得<strong>全球第12名</strong>的成绩。
                      <strong>主要贡献：</strong> (1) 设计了结合视觉和语言理解的新颖多模态融合架构，
                      (2) 实现了高效的检索增强生成管道，
                      (3) 协调团队在模型开发、超参数优化和提交策略方面的努力。
                      该挑战专注于开发能够理解和检索多模态内容信息的AI系统，
                      这与大型多模态模型的当前趋势相符。
                    </font>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>
                    <span class="lang-content lang-en">IMDL-BenCo: Benchmark for Image Manipulation Detection &
                      Localization</span>
                    <span class="lang-content lang-zh hidden">IMDL-BenCo：图像操作检测与定位基准</span>
                  </papertitle>
                  <br>
                  <strong>Co-First Author</strong> • <em>NeurIPS 2024 Benchmark Track — Spotlight</em> • 2024
                  <br>
                  <a href="https://arxiv.org/abs/2406.10580"><img
                      src="https://img.shields.io/badge/arXiv-2406.10580-b31b1b?style=flat-square" alt="arXiv"></a>
                  <a href="https://github.com/scu-zjz/IMDLBenCo"><img
                      src="https://img.shields.io/badge/GitHub-IMDLBenCo-181717?style=flat-square&logo=github"
                      alt="GitHub"></a>
                  <br>
                  <!-- 英文版本 -->
                  <div class="lang-content lang-en">
                    <font color='gray'>
                      Developed the first comprehensive benchmark and codebase for Image Manipulation Detection &
                      Localization (IMDL).
                      <strong>Key contributions:</strong> (1) Implemented GPU-accelerated evaluation metrics for fair
                      and
                      efficient comparison,
                      (2) Designed modular codebase architecture enabling easy customization and extension, (3)
                      Co-authored the manuscript
                      that received a <strong>Spotlight Award</strong> at NeurIPS 2024. The benchmark includes 8
                      state-of-the-art models,
                      15 evaluation metrics, and comprehensive robustness evaluation protocols, significantly advancing
                      the field's
                      standardization and reproducibility.
                    </font>
                  </div>

                  <!-- 中文版本 -->
                  <div class="lang-content lang-zh hidden">
                    <font color='gray'>
                      开发了首个用于图像操作检测与定位（IMDL）的综合基准和代码库。
                      <strong>主要贡献：</strong> (1) 实现了GPU加速的评估指标，实现公平高效的比较，
                      (2) 设计了模块化代码库架构，便于定制和扩展，(3)
                      合著了获得NeurIPS 2024 <strong>Spotlight奖</strong>的手稿。该基准包含8个最先进的模型、
                      15个评估指标和全面的鲁棒性评估协议，显著推进了该领域的
                      标准化和可重现性。
                    </font>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>


          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>
                    <span class="lang-content lang-en">Invited Talks</span>
                    <span class="lang-content lang-zh hidden">特邀报告</span>
                  </heading>
                </td>
              </tr>
              <tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>
                    <span class="lang-content lang-en">Understanding Hierarchical Data with Large Language Models: RAG,
                      Structural Reasoning, and Future Directions</span>
                    <span class="lang-content lang-zh hidden">用大语言模型理解层次化数据：RAG、结构推理与未来方向</span>
                  </papertitle>
                  <br>
                  <strong>Invited Talks</strong> • <em>Reasoning Day @ KDD 2025</em> • Toronto, ON, Canada • Aug 2025
                  <br>
                  <!-- 英文版本 -->
                  <div class="lang-content lang-en">
                    <font color='gray'>
                      Invited to deliver a presentation at the prestigious KDD 2025 Reasoning Day workshop.<br>
                      The talk will explore cutting-edge developments in leveraging Large Language Models for
                      hierarchical
                      data understanding,<br>
                      with particular focus on Retrieval-Augmented Generation (RAG) systems and structural reasoning
                      capabilities.<br>
                      <strong>Key topics:</strong><br>
                      (1) Novel approaches to hierarchical data representation in LLM contexts,<br>
                      (2) Integration of structural reasoning with retrieval-augmented generation,<br>
                      (3) Future research directions in reasoning-enhanced AI systems,<br>
                      (4) Practical applications and deployment considerations for hierarchical reasoning in real-world
                      scenarios.<br>
                      This invitation recognizes the impact of our HiBench work and positions our research at the
                      forefront of LLM reasoning capabilities.
                    </font>
                  </div>

                  <!-- 中文版本 -->
                  <div class="lang-content lang-zh hidden">
                    <font color='gray'>
                      受邀在享有盛誉的KDD 2025推理日研讨会上发表演讲。<br>
                      该报告将探讨利用大语言模型进行层次化数据理解的前沿发展，<br>
                      特别关注检索增强生成（RAG）系统和结构推理能力。<br>
                      <strong>主要议题：</strong><br>
                      (1) 在LLM语境下层次化数据表示的新颖方法，<br>
                      (2) 结构推理与检索增强生成的集成，<br>
                      (3) 推理增强AI系统的未来研究方向，<br>
                      (4) 在现实场景中层次推理的实际应用和部署考虑。<br>
                      这一邀请认可了我们HiBench工作的影响力，并将我们的研究置于
                      LLM推理能力的前沿。
                    </font>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>

          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>
                    <span class="lang-content lang-en">Education</span>
                    <span class="lang-content lang-zh hidden">教育背景</span>
                  </heading>
                </td>
              </tr>
              <tr>
            </tbody>
          </table>

          <!-- 英文版本 -->
          <div class="lang-content lang-en">
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                    <b>Sichuan University</b>, Chengdu, Sichuan, China
                    <br>
                    B.E. in Computer Science and Technology • Sep. 2020 to Jun. 2024
                    </br>
                  </td>
                  <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img
                      src="images/SichuanUniversityLOGO.png" width="75" height="75"></td>
                </tr>
              </tbody>
            </table>
            <br>
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                    <b>Hong Kong Polytechnic University</b>, Hongkong, China
                    <br>
                    PHD. in Computer Science and Technology • Sep. 2024 to Present
                    </br>
                  </td>
                  <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img src="images/polyu.jpg"
                      width="75" height="75"></td>
                </tr>
              </tbody>
            </table>
          </div>

          <!-- 中文版本 -->
          <div class="lang-content lang-zh hidden">
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                    <b>四川大学</b>，成都，四川，中国
                    <br>
                    计算机科学与技术学士 • 2020年9月至2024年6月
                    </br>
                  </td>
                  <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img
                      src="images/SichuanUniversityLOGO.png" width="75" height="75"></td>
                </tr>
              </tbody>
            </table>
            <br>
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                    <b>香港理工大学</b>，香港，中国
                    <br>
                    计算机科学与技术博士 • 2024年9月至今
                    </br>
                  </td>
                  <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img src="images/polyu.jpg"
                      width="75" height="75"></td>
                </tr>
              </tbody>
            </table>
          </div>

          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>
                    <span class="lang-content lang-en">Experience</span>
                    <span class="lang-content lang-zh hidden">工作经历</span>
                  </heading>
                </td>
              </tr>
              <tr>
            </tbody>
          </table>



          <!-- 英文版本 -->
          <div class="lang-content lang-en">
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                    <b>National University of Singapore (NUS)</b>
                    <br>
                    <strong>Summer School Participant</strong> • Aug. 2023
                    <br>
                    • Participated in intensive research program at School of Computing
                    <br>
                    • Completed face recognition project using CNN-based feature extraction and similarity matching
                    <br>
                    • Gained international research experience and cross-cultural collaboration skills
                    </br>
                  </td>
                  <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img src="images/nus.png"
                      width="135" height="75"></td>
                </tr>
              </tbody>
            </table>
            <br>

            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                    <a href="http://dicalab.cn/"> DICALab</a>, <b>Sichuan University</b>
                    <br>
                    <strong>Research Assistant</strong> • Sep. 2022 to Jun. 2024
                    <br>
                    Advisor: <a href="https://scholar.google.com/citations?user=-cNWmJMAAAAJ&hl=zh-CN"> Prof. JiZhe
                      Zhou</a>
                    <br>
                    • Developed graph-based frameworks for privacy-sensitive object detection
                    <br>
                    • Participated in National Natural Science Foundation of China project
                    <br>
                    • Contributed to National Key R&D Program of China
                    <br>
                    • Co-authored multiple publications in top-tier conferences and journals
                    </br>
                  </td>
                  <td style="padding:0px 20px 0px 20px;width:8%;vertical-align:middle"><img src="images/DICA.png"
                      width="75" height="75"></td>
                </tr>
              </tbody>
            </table>
          </div>

          <!-- 中文版本 -->
          <div class="lang-content lang-zh hidden">
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                    <b>新加坡国立大学 (NUS)</b>
                    <br>
                    <strong>暑期学校学员</strong> • 2023年8月
                    <br>
                    • 参与计算机学院的密集研究项目
                    <br>
                    • 完成基于CNN特征提取和相似度匹配的人脸识别项目
                    <br>
                    • 获得国际研究经验和跨文化合作技能
                    </br>
                  </td>
                  <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img src="images/nus.png"
                      width="135" height="75"></td>
                </tr>
              </tbody>
            </table>
            <br>

            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                    <a href="http://dicalab.cn/"> DICALab</a>, <b>四川大学</b>
                    <br>
                    <strong>研究助理</strong> • 2022年9月至2024年6月
                    <br>
                    导师：<a href="https://scholar.google.com/citations?user=-cNWmJMAAAAJ&hl=zh-CN"> 周吉喆教授</a>
                    <br>
                    • 开发用于隐私敏感对象检测的基于图的框架
                    <br>
                    • 参与国家自然科学基金项目
                    <br>
                    • 参与国家重点研发计划项目
                    <br>
                    • 在顶级会议和期刊上合著多篇论文
                    </br>
                  </td>
                  <td style="padding:0px 20px 0px 20px;width:8%;vertical-align:middle"><img src="images/DICA.png"
                      width="75" height="75"></td>
                </tr>
              </tbody>
            </table>
          </div>
          <br>
          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"
            style="margin-bottom: 10px; table-layout: fixed;">
            <tbody>
              <tr>
                <td colspan="2" style="padding:20px 0 0px 0;width:100%;vertical-align:middle;text-align:left;">
                  <heading>
                    <span class="lang-content lang-en">Selected Awards</span>
                    <span class="lang-content lang-zh hidden">主要奖项</span>
                  </heading>
                </td>
              </tr>
              <!-- 英文版本 -->
              <tr class="lang-content lang-en" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>KDD Cup 2025 — Meta CRAG-MM</strong><br>
                  <span style="color: #666; font-size: 0.95em;">Toronto, Canada, 2025</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>12th Place (Global)</strong>
                </td>
              </tr>
              <tr class="lang-content lang-en" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>NeurIPS 2024 — IMDL‑BenCo (Co‑first Author)</strong><br>
                  <span style="color: #666; font-size: 0.95em;">Vancouver, Canada, 2024</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>Spotlight Award</strong>
                </td>
              </tr>
              <tr class="lang-content lang-en" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>Outstanding Graduate</strong><br>
                  <span style="color: #666; font-size: 0.95em;">Sichuan University &amp; Sichuan Province, 2024</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>Top Achievement</strong>
                </td>
              </tr>
              <tr class="lang-content lang-en" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>Tencent Scholarship</strong><br>
                  <span style="color: #666; font-size: 0.95em;">Sichuan University, China, 2023</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>Top 2%</strong>
                </td>
              </tr>
              <tr class="lang-content lang-en" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>A-Level Certificate</strong><br>
                  <span style="color: #666; font-size: 0.95em;">Comprehensive Quality Evaluation, China, 2023</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>Excellence</strong>
                </td>
              </tr>
              <tr class="lang-content lang-en" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>Comprehensive First Class Scholarship</strong><br>
                  <span style="color: #666; font-size: 0.95em;">Sichuan University, Sichuan, China, 2022</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>Top 1%</strong>
                </td>
              </tr>
              <tr class="lang-content lang-en">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>Outstanding Students of Sichuan University</strong><br>
                  <span style="color: #666; font-size: 0.95em;">Sichuan University, Sichuan, China, 2022</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>Top 5%</strong>
                </td>
              </tr>

              <!-- 中文版本 -->
              <tr class="lang-content lang-zh hidden" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>KDD Cup 2025 — Meta CRAG-MM</strong><br>
                  <span style="color: #666; font-size: 0.95em;">加拿大多伦多，2025</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>全球第12名</strong>
                </td>
              </tr>
              <tr class="lang-content lang-zh hidden" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>NeurIPS 2024 — IMDL‑BenCo（共同第一作者）</strong><br>
                  <span style="color: #666; font-size: 0.95em;">加拿大温哥华，2024</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>Spotlight奖</strong>
                </td>
              </tr>
              <tr class="lang-content lang-zh hidden" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>优秀毕业生</strong><br>
                  <span style="color: #666; font-size: 0.95em;">四川大学 &amp; 四川省，2024</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>最高成就</strong>
                </td>
              </tr>
              <tr class="lang-content lang-zh hidden" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>腾讯奖学金</strong><br>
                  <span style="color: #666; font-size: 0.95em;">四川大学，中国，2023</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>前2%</strong>
                </td>
              </tr>
              <tr class="lang-content lang-zh hidden" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>A级证书</strong><br>
                  <span style="color: #666; font-size: 0.95em;">综合素质评价，中国，2023</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>优秀</strong>
                </td>
              </tr>
              <tr class="lang-content lang-zh hidden" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>综合一等奖学金</strong><br>
                  <span style="color: #666; font-size: 0.95em;">四川大学，四川，中国，2022</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>前1%</strong>
                </td>
              </tr>
              <tr class="lang-content lang-zh hidden">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>四川大学优秀学生</strong><br>
                  <span style="color: #666; font-size: 0.95em;">四川大学，四川，中国，2022</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>前5%</strong>
                </td>
              </tr>
            </tbody>
          </table>






          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"
            style="margin-bottom: 10px; table-layout: fixed;">
            <tbody>
              <tr>
                <td colspan="2" style="padding:20px 0 0px 0;width:100%;vertical-align:middle;text-align:left;">
                  <heading>
                    <span class="lang-content lang-en">Professional Service</span>
                    <span class="lang-content lang-zh hidden">专业服务</span>
                  </heading>
                </td>
              </tr>
              <!-- 英文版本 -->
              <tr class="lang-content lang-en" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>Conference & Journal Reviewer</strong><br>
                  <span style="color: #666; font-size: 0.95em;">2023-2025</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>TIP, ECCV, NeurIPS, KDD, AAAI, IoTJ,</strong>
                </td>
              </tr>
              <tr class="lang-content lang-en" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>Teaching Assistant</strong><br>
                  <span style="color: #666; font-size: 0.95em;">The Hong Kong Polytechnic University (PolyU)</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>Artificial Intelligence (COMP4431)<br>NLP Practicum (COMP5423)</strong>
                </td>
              </tr>

              <!-- 中文版本 -->
              <tr class="lang-content lang-zh hidden" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>会议和期刊审稿人</strong><br>
                  <span style="color: #666; font-size: 0.95em;">2023-2025</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>TIP, ECCV, NeurIPS, KDD, AAAI, IoTJ</strong>
                </td>
              </tr>
              <tr class="lang-content lang-zh hidden" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>教学助理</strong><br>
                  <span style="color: #666; font-size: 0.95em;">香港理工大学</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>人工智能 (COMP4431)<br>自然语言处理实践 (COMP5423)</strong>
                </td>
              </tr>
            </tbody>
          </table>

          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"
            style="margin-bottom: 10px; table-layout: fixed;">
            <tbody>
              <tr>
                <td colspan="2" style="padding:20px 0 0px 0;width:100%;vertical-align:middle;text-align:left;">
                  <heading>
                    <span class="lang-content lang-en">Skills</span>
                    <span class="lang-content lang-zh hidden">技能</span>
                  </heading>
                </td>
              </tr>
              <!-- 英文版本 -->
              <tr class="lang-content lang-en">
                <td style="width:30%;vertical-align:top;padding:10px 0;text-align:left;">
                  <strong>Research Topics</strong>
                </td>
                <td style="width:70%;vertical-align:top;padding:10px 0;text-align:left;">
                  Large Language Models (LLMs), Retrieval‑Augmented Generation (RAG), Recommender Systems (RecSys)
                </td>
              </tr>
              <tr class="lang-content lang-en">
                <td style="width:30%;vertical-align:top;padding:10px 0;text-align:left;">
                  <strong>Frameworks &amp; Tools</strong>
                </td>
                <td style="width:70%;vertical-align:top;padding:10px 0;text-align:left;">
                  PyTorch, Hugging Face, NumPy, Docker, Git, Anaconda
                </td>
              </tr>
              <tr class="lang-content lang-en">
                <td style="width:30%;vertical-align:top;padding:10px 0;text-align:left;">
                  <strong>Languages</strong>
                </td>
                <td style="width:70%;vertical-align:top;padding:10px 0;text-align:left;">
                  Mandarin (native), English (fluent, IELTS 6.5)
                </td>
              </tr>

              <!-- 中文版本 -->
              <tr class="lang-content lang-zh hidden">
                <td style="width:30%;vertical-align:top;padding:10px 0;text-align:left;">
                  <strong>研究方向</strong>
                </td>
                <td style="width:70%;vertical-align:top;padding:10px 0;text-align:left;">
                  大语言模型 (LLMs)，检索增强生成 (RAG)，推荐系统 (RecSys)
                </td>
              </tr>
              <tr class="lang-content lang-zh hidden">
                <td style="width:30%;vertical-align:top;padding:10px 0;text-align:left;">
                  <strong>框架和工具</strong>
                </td>
                <td style="width:70%;vertical-align:top;padding:10px 0;text-align:left;">
                  PyTorch, Hugging Face, NumPy, Docker, Git, Anaconda
                </td>
              </tr>
              <tr class="lang-content lang-zh hidden">
                <td style="width:30%;vertical-align:top;padding:10px 0;text-align:left;">
                  <strong>语言能力</strong>
                </td>
                <td style="width:70%;vertical-align:top;padding:10px 0;text-align:left;">
                  中文（母语），英文（流利，雅思6.5）
                </td>
              </tr>
            </tbody>
          </table>

          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <div style="text-align: center;">
            <a href="https://clustrmaps.com/site/1buac" title="Visit tracker">
              <img src="//www.clustrmaps.com/map_v2.png?d=arwquO5nouqZwMbhchX9awZ1bM3tO8fPWrcUr78F1E4&cl=ffffff" />
            </a>
          </div>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px">
                  <div style="float:left;">
                    Updated at Aug. 2025
                  </div>
                  <div style="float:right;">
                    Thanks <a href="https://jonbarron.info">Jon Barron</a> for this amazing template
                  </div>
                  <br>
                  <br>
                  <p></p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>


  <script>
    new WOW().init();


    function showMoreNews() {
      var button = document.getElementById('expand-btn');

      // 确保只添加一次事件监听器
      if (button.hasEventListener) return;
      button.hasEventListener = true;

      button.addEventListener('click', function () {
        // 获取所有的more-news元素
        const allMoreNews = document.querySelectorAll('.more-news');

        // 检查是否已展开（通过检查第一个more-news的display状态）
        const isExpanded = allMoreNews[0].style.display === 'block';

        if (!isExpanded) {
          // 展开所有more-news区域
          allMoreNews.forEach(moreNews => {
            moreNews.style.display = 'block';
            moreNews.classList.add('magictime', 'puffIn');
          });

          // 更新按钮文本 - 支持多语言
          const enText = button.querySelector('.lang-en');
          const zhText = button.querySelector('.lang-zh');
          if (enText) enText.textContent = 'Less News';
          if (zhText) zhText.textContent = '收起新闻';
          button.classList.remove('hvr-grow');
          button.classList.add('hvr-shrink');
        } else {
          // 收起所有more-news区域
          allMoreNews.forEach(moreNews => {
            moreNews.style.display = 'none';
          });

          // 更新按钮文本 - 支持多语言
          const enText = button.querySelector('.lang-en');
          const zhText = button.querySelector('.lang-zh');
          if (enText) enText.textContent = 'More News';
          if (zhText) zhText.textContent = '更多新闻';
          button.classList.remove('hvr-shrink');
          button.classList.add('hvr-grow');
        }
      });
    }

    // 语言切换功能
    function switchLanguage(lang) {
      const enBtn = document.getElementById('en-btn');
      const zhBtn = document.getElementById('zh-btn');
      const langContent = document.querySelectorAll('.lang-content');
      const hiddenClass = 'hidden';

      langContent.forEach(content => {
        if (content.classList.contains(`lang-${lang}`)) {
          content.classList.remove(hiddenClass);
        } else {
          content.classList.add(hiddenClass);
        }
      });

      if (lang === 'en') {
        enBtn.classList.add('active');
        zhBtn.classList.remove('active');
      } else {
        zhBtn.classList.add('active');
        enBtn.classList.remove('active');
      }

      // 保存语言选择到本地存储
      localStorage.setItem('language', lang);
    }

    // 基于时间的自动模式切换
    function setThemeBasedOnTime() {
      const now = new Date();
      const hour = now.getHours();
      const themeToggle = document.getElementById('theme-toggle');

      // 6:00-18:00 为白天模式，18:00-6:00 为夜间模式
      const isNightTime = hour < 6 || hour >= 18;

      if (isNightTime) {
        document.body.classList.add('dark-mode');
        if (themeToggle) themeToggle.textContent = '☀️';
      } else {
        document.body.classList.remove('dark-mode');
        if (themeToggle) themeToggle.textContent = '🌙';
      }

      return isNightTime ? 'dark' : 'light';
    }

    // 手动切换主题
    function toggleTheme() {
      const isDark = document.body.classList.toggle('dark-mode');
      const themeToggle = document.getElementById('theme-toggle');
      themeToggle.textContent = isDark ? '☀️' : '🌙';

      // 保存手动设置，覆盖自动模式
      localStorage.setItem('theme', isDark ? 'dark' : 'light');
      localStorage.setItem('themeAutoMode', 'false');
    }

    // 初始化页面
    document.addEventListener('DOMContentLoaded', () => {
      // 语言初始化
      const savedLang = localStorage.getItem('language') || 'en';
      switchLanguage(savedLang);

      // 初始化more-news和more-publications区域为隐藏状态
      const allMoreNews = document.querySelectorAll('.more-news');
      allMoreNews.forEach(moreNews => {
        moreNews.style.display = 'none';
      });

      const allMorePublications = document.querySelectorAll('.more-publications');
      allMorePublications.forEach(morePubs => {
        morePubs.style.display = 'none';
      });

      // 创建主题切换按钮
      const themeToggle = document.createElement('button');
      themeToggle.id = 'theme-toggle';
      themeToggle.className = 'lang-btn';
      themeToggle.style.position = 'fixed';
      themeToggle.style.top = '20px';
      themeToggle.style.right = '180px';
      themeToggle.style.zIndex = '1000';
      themeToggle.onclick = toggleTheme;
      document.body.appendChild(themeToggle);

      // 主题初始化
      const savedTheme = localStorage.getItem('theme');
      const isAutoMode = localStorage.getItem('themeAutoMode') !== 'false';

      if (savedTheme && !isAutoMode) {
        // 使用保存的手动设置
        if (savedTheme === 'dark') {
          document.body.classList.add('dark-mode');
          themeToggle.textContent = '☀️';
        } else {
          document.body.classList.remove('dark-mode');
          themeToggle.textContent = '🌙';
        }
      } else {
        // 使用基于时间的自动模式
        setThemeBasedOnTime();
        localStorage.setItem('themeAutoMode', 'true');
      }

      // 每分钟检查一次时间变化（仅在自动模式下）
      setInterval(() => {
        const isAutoMode = localStorage.getItem('themeAutoMode') !== 'false';
        if (isAutoMode) {
          setThemeBasedOnTime();
        }
      }, 60000); // 每分钟检查一次
    });

  </script>
</body>


</html>