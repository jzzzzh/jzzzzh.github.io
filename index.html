<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhuohang Jiang</title>

  <meta name="author" content="Zhuohang Jiang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ğŸŒ</text></svg>">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css" />
  <link rel="stylesheet" href="magic/dist/magic.css">
  <link rel="stylesheet" href="hover.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/wow/1.1.2/wow.min.js"></script>
  <style>
    .aligned-table {
      width: 100% !important;
      table-layout: fixed !important;
      margin-bottom: 10px !important;
    }

    .aligned-table td:first-child {
      width: 60% !important;
      text-align: left !important;
    }

    .aligned-table td:last-child {
      width: 40% !important;
      text-align: right !important;
    }

    /* ç¡®ä¿Awardå’ŒProfessional Serviceç« èŠ‚çš„å³ä¾§å†…å®¹å®Œç¾å¯¹é½ */
    table[style*="table-layout: fixed"] td:last-child {
      text-align: right !important;
      padding-right: 0 !important;
    }

    table[style*="table-layout: fixed"] td:first-child {
      text-align: left !important;
      padding-left: 0 !important;
    }

    /* è¯­è¨€åˆ‡æ¢æŒ‰é’®æ ·å¼ */
    .language-toggle {
      position: fixed;
      top: 20px;
      right: 20px;
      z-index: 1000;
      display: flex;
      gap: 10px;
    }

    .lang-btn {
      padding: 8px 15px;
      background-color: #007BFF;
      color: white;
      border: none;
      border-radius: 5px;
      cursor: pointer;
      font-size: 14px;
      transition: all 0.3s ease;
    }

    .lang-btn:hover {
      background-color: #0056b3;
      transform: translateY(-2px);
    }

    .lang-btn.active {
      background-color: #28a745;
    }

    /* æ·±è‰²æ¨¡å¼æ ·å¼ */
    .dark-mode {
      background-color: #1a1a1a !important;
      color: #e0e0e0 !important;
    }

    .dark-mode table {
      background-color: #1a1a1a !important;
      color: #e0e0e0 !important;
    }

    .dark-mode td {
      background-color: #1a1a1a !important;
      color: #e0e0e0 !important;
    }

    .dark-mode heading {
      color: #ffffff !important;
    }

    .dark-mode papertitle {
      color: #ffffff !important;
    }

    .dark-mode a {
      color: #66b3ff !important;
    }

    .dark-mode a:hover {
      color: #99ccff !important;
    }

    .dark-mode hr {
      background: linear-gradient(to right, transparent, #555, transparent) !important;
    }

    .dark-mode .lang-btn {
      background-color: #333 !important;
      color: #e0e0e0 !important;
    }

    .dark-mode .lang-btn:hover {
      background-color: #555 !important;
    }

    .dark-mode .lang-btn.active {
      background-color: #28a745 !important;
    }

    /* æ¨¡å¼åˆ‡æ¢åŠ¨ç”» */
    body {
      transition: background-color 0.5s ease, color 0.5s ease;
    }

    /* éšè—/æ˜¾ç¤ºè¯­è¨€å†…å®¹ */
    .lang-content {
      display: block;
    }

    .lang-content.hidden {
      display: none;
    }

    /* ç¡®ä¿è¡¨æ ¼è¡Œå…ƒç´ çš„è¯­è¨€åˆ‡æ¢æ­£å¸¸å·¥ä½œ */
    tr.lang-content {
      display: table-row;
    }

    tr.lang-content.hidden {
      display: none;
    }
  </style>
</head>

<body>
  <!-- è¯­è¨€åˆ‡æ¢æŒ‰é’® -->
  <div class="language-toggle">
    <button class="lang-btn active" onclick="switchLanguage('en')" id="en-btn">English</button>
    <button class="lang-btn" onclick="switchLanguage('zh')" id="zh-btn">ä¸­æ–‡</button>
  </div>

  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                  <div class="wow animate__animated animate__fadeInDown" style="font-size: 2.5em; font-weight: bold;">
                    <name>
                      <span class="lang-content lang-en">Zhuohang Jiang</span>
                      <span class="lang-content lang-zh hidden">è’‹å“èˆª</span>
                    </name>
                  </div>

                  </p>
                  <div>
                    <!-- è‹±æ–‡ç‰ˆæœ¬ -->
                    <div class="lang-content lang-en">
                      <p>I am currently pursuing a PhD degree at the <strong> <a href="https://www.polyu.edu.hk/">Hong
                            Kong Polytechnic University</a></strong>. My supervisors are <a
                          href="https://scholar.google.cz/citations?user=D1LEg-YAAAAJ&hl=zh-CN">Qing Li</a>
                        and <a href="https://scholar.google.cz/citations?user=SQ9UbHIAAAAJ&hl=zh-CN&oi=ao">Wenqi
                          Fan</a>.
                        My current Cumulative GPA is <strong>3.60/4.00</strong>.
                      </p>
                      <p>I studied at <a href="https://www.scu.edu.cn">Sichuan University (SCU)</a> from 2020 to 2024,
                        where I majored in Computer Science & Technology. My
                        <strong> Major GPA (CS courses):
                          3.79/4, 89.39/100;</strong>
                        Overall GPA: 3.78/4, 89.25/100
                      </p>
                      <p>
                        During my time at Sichuan University, I worked as a research assistant at
                        <a href="http://www.machineilab.org/">MachineILab</a>
                        from 2022 to 2024, advised by
                        <a href="https://scholar.google.com/citations?user=-cNWmJMAAAAJ&hl=zh-CN">Prof. JiZhe Zhou</a>.
                        I participated in one National Natural Science Foundation of China project and one National Key
                        R&D
                        Program of China.
                      </p>
                    </div>

                    <!-- ä¸­æ–‡ç‰ˆæœ¬ -->
                    <div class="lang-content lang-zh hidden">
                      <p>æˆ‘ç›®å‰åœ¨<strong><a href="https://www.polyu.edu.hk/">é¦™æ¸¯ç†å·¥å¤§å­¦</a></strong>æ”»è¯»åšå£«å­¦ä½ã€‚æˆ‘çš„å¯¼å¸ˆæ˜¯<a
                          href="https://scholar.google.cz/citations?user=D1LEg-YAAAAJ&hl=zh-CN">æé’æ•™æˆ</a>
                        å’Œ<a href="https://scholar.google.cz/citations?user=SQ9UbHIAAAAJ&hl=zh-CN&oi=ao">èŒƒæ–‡ç¦æ•™æˆ</a>ã€‚
                        æˆ‘ç›®å‰çš„ç´¯è®¡GPAæ˜¯<strong>3.60/4.00</strong>ã€‚
                      </p>
                      <p>æˆ‘äº2020å¹´è‡³2024å¹´åœ¨<a href="https://www.scu.edu.cn">å››å·å¤§å­¦</a>å°±è¯»ï¼Œä¸»ä¿®è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ä¸“ä¸šã€‚æˆ‘çš„
                        <strong>ä¸“ä¸šGPAï¼ˆè®¡ç®—æœºè¯¾ç¨‹ï¼‰ï¼š3.79/4ï¼Œ89.39/100ï¼›</strong>
                        ç»¼åˆGPAï¼š3.78/4ï¼Œ89.25/100
                      </p>
                      <p>
                        åœ¨å››å·å¤§å­¦æœŸé—´ï¼Œæˆ‘äº2022å¹´è‡³2024å¹´åœ¨
                        <a href="http://www.machineilab.org/">MachineILab</a>
                        æ‹…ä»»ç ”ç©¶åŠ©ç†ï¼Œå¯¼å¸ˆæ˜¯
                        <a href="https://scholar.google.com/citations?user=-cNWmJMAAAAJ&hl=zh-CN">å‘¨å‰å–†æ•™æˆ</a>ã€‚
                        æˆ‘å‚ä¸äº†ä¸€é¡¹å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘é¡¹ç›®å’Œä¸€é¡¹å›½å®¶é‡ç‚¹ç ”å‘è®¡åˆ’é¡¹ç›®ã€‚
                      </p>
                    </div>
                  </div>

                  <p style="text-align:center">
                    <a href="mailto:zhuohangjiang2002@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="data/ZhuohangJiang-CV.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.cz/citations?hl=zh-CN&user=h3vEhrgAAAAJ">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/jzzzzh">Github</a>&nbsp
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/zhuohang.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/zhuohang.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>
                    <span class="lang-content lang-en">Research Topics</span>
                    <span class="lang-content lang-zh hidden">ç ”ç©¶æ–¹å‘</span>
                  </heading>
                  <!-- è‹±æ–‡ç‰ˆæœ¬ -->
                  <div class="lang-content lang-en">
                    <p>
                      My research interests lie in <strong>large language models (LLMs), retrieval-augmented generation
                        (RAG),
                        and Recommender Systems (RecSys)</strong>. I focus on both <strong>theoretical
                        foundations</strong> and
                      practical applications of LLM-based systems.
                    </p>
                    <p>
                      My previous research was primarily focused on topics within computer vision, such as tampering
                      detection and object recognition tasks. I have contributed to the design of high-impact
                      benchmarks,
                      such as <strong>HiBench</strong>, and comprehensive surveys like <strong>WebAgents</strong>.
                      My work has been published in <strong>top-tier conferences</strong>, including
                      <strong>NeurIPS 2024</strong> (spotlight) and <strong>AAAI 2025</strong>, and has accumulated
                      <strong>100+ citations</strong>, with an <strong>h-index of 4</strong>.
                    </p>
                  </div>
                  <!-- ä¸­æ–‡ç‰ˆæœ¬ -->
                  <div class="lang-content lang-zh hidden">
                    <p>
                      æˆ‘çš„ç ”ç©¶å…´è¶£ä¸»è¦é›†ä¸­åœ¨<strong>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œæ¨èç³»ç»Ÿï¼ˆRecSysï¼‰</strong>ã€‚
                      æˆ‘ä¸“æ³¨äºåŸºäºLLMç³»ç»Ÿçš„<strong>ç†è®ºåŸºç¡€</strong>å’Œå®é™…åº”ç”¨ã€‚
                    </p>
                    <p>
                      æˆ‘ä¹‹å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸï¼Œå¦‚å›¾åƒç¯¡æ”¹æ£€æµ‹å’Œç›®æ ‡è¯†åˆ«ä»»åŠ¡ã€‚æˆ‘ä¸ºé«˜å½±å“åŠ›åŸºå‡†æµ‹è¯•çš„è®¾è®¡åšå‡ºäº†è´¡çŒ®ï¼Œ
                      å¦‚<strong>HiBench</strong>ï¼Œä»¥åŠç»¼åˆæ€§è°ƒç ”å¦‚<strong>WebAgents</strong>ã€‚
                      æˆ‘çš„å·¥ä½œå‘è¡¨åœ¨<strong>é¡¶çº§ä¼šè®®</strong>ä¸Šï¼ŒåŒ…æ‹¬<strong>NeurIPS 2024</strong>ï¼ˆspotlightï¼‰å’Œ<strong>AAAI
                        2025</strong>ï¼Œ
                      ç´¯è®¡è·å¾—<strong>100+å¼•ç”¨</strong>ï¼Œ<strong>hæŒ‡æ•°ä¸º4</strong>ã€‚
                    </p>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>
          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>
                    <span class="lang-content lang-en">News</span>
                    <span class="lang-content lang-zh hidden">æ–°é—»åŠ¨æ€</span>
                  </heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <!-- è‹±æ–‡ç‰ˆæœ¬ -->
                  <div class="lang-content lang-en">
                    <div>
                      <p> <strong>ğŸ† 2025-08-20</strong> - Our work <strong><a
                            href="https://arxiv.org/abs/2508.05197">QA-Dragon</a></strong> was accepted by <strong>KDD
                          2025 Workshop for Multimodal Retrieval Augmented Generation</strong>.</p>
                    </div>
                    <div>
                      <p> <strong>ğŸ¤ 2025-08-07</strong> - Invited to give a talk "Understanding Hierarchical Data with
                        Large Language Models: RAG, Structural Reasoning, and Future Directions" at <strong>KDD 2025
                          Reasoning Day</strong> in Toronto, Canada! ğŸ™ï¸</p>
                    </div>
                    <div>
                      <p> <strong>ğŸ… 2025-06-18</strong> - Achieved <strong>12th place globally</strong> in KDD Cup 2025
                        -
                        Meta CRAG-MM Multimodal Retrieval Challenge among hundreds of international teams! ğŸŒ</p>
                    </div>
                    <div>
                      <p> <strong>ğŸ† 2025-05-16</strong> - Our benchmark paper <a
                          href="https://arxiv.org/abs/2503.00912">HiBench</a> was accepted to <strong>KDD Benchmark
                          Track</strong>! ğŸ‰</p>
                    </div>
                    <div>
                      <p> <strong>ğŸ‰ 2025-05-07</strong> - Our survey paper <a href="https://arxiv.org/abs/2503.23350">A
                          Survey
                          of WebAgents: Towards Next-Generation AI Agents for Web Automation with Large Foundation
                          Models</a> was accepted to <strong>KDD Tutorial Track</strong>! ğŸŠ</p>
                    </div>
                    <div>
                      <p> <strong>ğŸ“œ 2025-03-30</strong> - Completed the survey paper <a
                          href="https://arxiv.org/abs/2503.23350">A Survey of WebAgents: Towards Next-Generation AI
                          Agents
                          for Web Automation with Large Foundation Models</a>.</p>
                    </div>
                    <div>
                      <p> <strong>ğŸ“˜ 2025-03-01</strong> - Completed the <a
                          href="https://arxiv.org/abs/2503.00912">HiBench</a> paper and released the code and dataset on
                        <a href="https://github.com/jzzzzh/HiBench">GitHub</a> and
                        <a href="https://huggingface.co/datasets/zhuohang/HiBench">Hugging Face</a>.
                      </p>
                    </div>

                    <!-- å‰©ä½™çš„æ–°é—»é»˜è®¤éšè— -->

                    <div class="more-news">

                      <p> <strong>ğŸŒŸ 2025-01-15</strong> - <a
                          href="https://ojs.aaai.org/index.php/AAAI/article/view/33198">Mesoscopic Insights:
                          Orchestrating
                          Multi-Scale & Hybrid Architecture for Image Manipulation Localization</a> was published in
                        AAAI
                        2025.</p>

                      <p> <strong>ğŸ† 2024-12-01</strong> - <a href="https://arxiv.org/abs/2406.10580">IMDL-BenCo </a>was
                        published in NeurIPS 2024 Benchmark Tracks and received a Spotlight award.</p>

                      <p> <strong>ğŸ“ 2024-09-01</strong> - Beginning my pursuit of a PhD degree in Hong Kong PolyU.</p>

                      <p> <strong>ğŸ“ 2024-06-26</strong> - Got <strong>Outstanding Graduate Award</strong> from Sichuan
                        University and Sichuan Province! ğŸ‰</p>

                      <p> <strong>ğŸ“ 2024-06-26</strong> - Graduated from Sichuan University with a bachelor's degree.
                      </p>

                      <p> <strong>ğŸ› ï¸ 2024-06-12</strong> - Completed the co-work project <a
                          href="https://github.com/scu-zjz/IMDLBenCo"> IMDLBenCo</a> and finished a paper
                        <a href="https://arxiv.org/abs/2406.10580"><strong>IMDL-BenCo: A Comprehensive Benchmark and
                            Codebase for Image Manipulation Detection & Localization</strong></a>
                      </p>

                      <p> <strong>ğŸ” 2024-05-24</strong> - Finished a paper <a
                          href="https://arxiv.org/abs/2406.12736"><strong>Beyond
                            Visual Appearances: Privacy-sensitive Objects Identification via Hybrid Graph
                            Reasoning</strong></a> </p <p> <strong>ğŸ“š 2023-08-01</strong> - Participated in <strong>NUS
                        Summer School</strong> research
                      program at National University of Singapore, completed face recognition project! ğŸ‡¸ğŸ‡¬</p>
                    </div>
                  </div>

                  <!-- ä¸­æ–‡ç‰ˆæœ¬ -->
                  <div class="lang-content lang-zh hidden">
                    <div>
                      <p> <strong>ğŸ† 2025-08-20</strong> - æˆ‘ä»¬çš„å·¥ä½œ <strong><a
                            href="https://arxiv.org/abs/2508.05197">QA-Dragon</a></strong> è¢«<strong>KDD
                          2025å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆç ”è®¨ä¼š</strong>æ¥æ”¶ã€‚</p>
                    </div>
                    <div>
                      <p> <strong>ğŸ¤ 2025-08-07</strong> - å—é‚€åœ¨åŠ æ‹¿å¤§å¤šä¼¦å¤š<strong>KDD
                          2025æ¨ç†æ—¥</strong>åšé¢˜ä¸º"ç”¨å¤§è¯­è¨€æ¨¡å‹ç†è§£å±‚æ¬¡åŒ–æ•°æ®ï¼šRAGã€ç»“æ„æ¨ç†ä¸æœªæ¥æ–¹å‘"çš„æŠ¥å‘Šï¼ğŸ™ï¸</p>
                    </div>
                    <div>
                      <p> <strong>ğŸ… 2025-06-18</strong> - åœ¨KDD Cup 2025 - Meta
                        CRAG-MMå¤šæ¨¡æ€æ£€ç´¢æŒ‘æˆ˜èµ›ä¸­ï¼Œåœ¨æ•°ç™¾ä¸ªå›½é™…å›¢é˜Ÿä¸­å–å¾—<strong>å…¨çƒç¬¬12å</strong>çš„æˆç»©ï¼ğŸŒ</p>
                    </div>
                    <div>
                      <p> <strong>ğŸ† 2025-05-16</strong> - æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•è®ºæ–‡<a
                          href="https://arxiv.org/abs/2503.00912">HiBench</a>è¢«<strong>KDDåŸºå‡†æµ‹è¯•èµ›é“</strong>æ¥æ”¶ï¼ğŸ‰</p>
                    </div>
                    <div>
                      <p> <strong>ğŸ‰ 2025-05-07</strong> - æˆ‘ä»¬çš„è°ƒç ”è®ºæ–‡<a
                          href="https://arxiv.org/abs/2503.23350">ç½‘ç»œæ™ºèƒ½ä½“è°ƒç ”ï¼šåŸºäºå¤§å‹åŸºç¡€æ¨¡å‹çš„ä¸‹ä¸€ä»£ç½‘ç»œè‡ªåŠ¨åŒ–AIæ™ºèƒ½ä½“</a>è¢«<strong>KDDæ•™ç¨‹èµ›é“</strong>æ¥æ”¶ï¼ğŸŠ
                      </p>
                    </div>
                    <div>
                      <p> <strong>ğŸ“œ 2025-03-30</strong> - å®Œæˆè°ƒç ”è®ºæ–‡<a
                          href="https://arxiv.org/abs/2503.23350">ç½‘ç»œæ™ºèƒ½ä½“è°ƒç ”ï¼šåŸºäºå¤§å‹åŸºç¡€æ¨¡å‹çš„ä¸‹ä¸€ä»£ç½‘ç»œè‡ªåŠ¨åŒ–AIæ™ºèƒ½ä½“</a>ã€‚</p>
                    </div>
                    <div>
                      <p> <strong>ğŸ“˜ 2025-03-01</strong> - å®Œæˆ<a href="https://arxiv.org/abs/2503.00912">HiBench</a>è®ºæ–‡ï¼Œå¹¶åœ¨
                        <a href="https://github.com/jzzzzh/HiBench">GitHub</a>å’Œ
                        <a href="https://huggingface.co/datasets/zhuohang/HiBench">Hugging Face</a>ä¸Šå‘å¸ƒä»£ç å’Œæ•°æ®é›†ã€‚
                      </p>
                    </div>

                    <!-- å‰©ä½™çš„æ–°é—»é»˜è®¤éšè— -->

                    <div class="more-news">

                      <p> <strong>ğŸŒŸ 2025-01-15</strong> - <a
                          href="https://ojs.aaai.org/index.php/AAAI/article/view/33198">ä¸­è§‚æ´å¯Ÿï¼šé¢å‘å›¾åƒæ“ä½œå®šä½çš„å¤šå°ºåº¦æ··åˆæ¶æ„ç¼–æ’</a>åœ¨AAAI
                        2025ä¸Šå‘è¡¨ã€‚</p>

                      <p> <strong>ğŸ† 2024-12-01</strong> - <a
                          href="https://arxiv.org/abs/2406.10580">IMDL-BenCo</a>åœ¨NeurIPS 2024åŸºå‡†æµ‹è¯•èµ›é“å‘è¡¨å¹¶è·å¾—Spotlightå¥–ã€‚</p>

                      <p> <strong>ğŸ“ 2024-09-01</strong> - å¼€å§‹åœ¨é¦™æ¸¯ç†å·¥å¤§å­¦æ”»è¯»åšå£«å­¦ä½ã€‚</p>

                      <p> <strong>ğŸ“ 2024-06-26</strong> - è·å¾—å››å·å¤§å­¦å’Œå››å·çœ<strong>ä¼˜ç§€æ¯•ä¸šç”Ÿ</strong>ç§°å·ï¼ğŸ‰</p>

                      <p> <strong>ğŸ“ 2024-06-26</strong> - ä»å››å·å¤§å­¦è·å¾—å­¦å£«å­¦ä½ã€‚</p>

                      <p> <strong>ğŸ› ï¸ 2024-06-12</strong> - å®Œæˆåˆä½œé¡¹ç›®<a
                          href="https://github.com/scu-zjz/IMDLBenCo">IMDLBenCo</a>å¹¶å®Œæˆè®ºæ–‡
                        <a href="https://arxiv.org/abs/2406.10580"><strong>IMDL-BenCoï¼šå›¾åƒæ“ä½œæ£€æµ‹ä¸å®šä½çš„ç»¼åˆåŸºå‡†æµ‹è¯•å’Œä»£ç åº“</strong></a>
                      </p>

                      <p> <strong>ğŸ” 2024-05-24</strong> - å®Œæˆè®ºæ–‡<a
                          href="https://arxiv.org/abs/2406.12736"><strong>è¶…è¶Šè§†è§‰å¤–è§‚ï¼šåŸºäºæ··åˆå›¾æ¨ç†çš„éšç§æ•æ„Ÿå¯¹è±¡è¯†åˆ«</strong></a></p>
                      <p> <strong>ğŸ“š 2023-08-01</strong> - å‚åŠ æ–°åŠ å¡å›½ç«‹å¤§å­¦<strong>æš‘æœŸå­¦æ ¡</strong>ç ”ç©¶é¡¹ç›®ï¼Œå®Œæˆäººè„¸è¯†åˆ«é¡¹ç›®ï¼ğŸ‡¸ğŸ‡¬</p>
                    </div>
                  </div>

                  <div style="text-align: center; margin-top: 20px;">
                    <button id="expand-btn" onclick="showMoreNews()" class="hvr-grow"
                      style="padding: 10px 20px; font-size: 16px; background-color: #007BFF; color: white; border: none; border-radius: 5px; cursor: pointer;">
                      <span class="lang-content lang-en">More News</span>
                      <span class="lang-content lang-zh hidden">æ›´å¤šæ–°é—»</span>
                    </button>
                  </div>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>
          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>
                    <span class="lang-content lang-en">Selected Publications</span>
                    <span class="lang-content lang-zh hidden">ä¸»è¦è®ºæ–‡å‘è¡¨</span>
                  </heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>[KDD'25 Workshop] QAâ€‘Dragon: Queryâ€‘Aware Dynamic RAG System for Knowledgeâ€‘Intensive Visual
                    Question Answering</papertitle>
                  <br>
                  <b>Zhuohang Jiang*</b>, Pangjing Wu*, Xu Yuan*, Wenqi Fan, Qing Li
                  <br>
                  <img src="https://img.shields.io/badge/CCF_A-KDD-1f77b4?style=flat-square" alt="CCF-A">
                  <a href="https://arxiv.org/abs/2508.05197"><img
                      src="https://img.shields.io/badge/arXiv-2508.05197-b31b1b?style=flat-square" alt="arXiv"></a>
                  <br>
                  <font color='gray'>
                    Retrieval-Augmented Generation (RAG) has been introduced to mitigate hallucinations in Multimodal
                    Large Language Models (MLLMs) by incorporating external knowledge into the generation process, and
                    it has become a widely adopted approach for knowledge-intensive Visual Question Answering (VQA).
                    However, existing RAG methods typically retrieve from either text or images in isolation, limiting
                    their ability to address complex queries that require multi-hop reasoning or up-to-date factual
                    knowledge. To address this limitation, we propose QA-Dragon, a Query-Aware Dynamic RAG System for
                    Knowledge-Intensive VQA. Specifically, QA-Dragon introduces a domain router to identify the query's
                    subject domain for domain-specific reasoning, along with a search router that dynamically selects
                    optimal retrieval strategies. By orchestrating both text and image search agents in a hybrid setup,
                    our system supports multimodal, multi-turn, and multi-hop reasoning, enabling it to tackle complex
                    VQA tasks effectively. We evaluate our QA-Dragon on the Meta CRAG-MM Challenge at KDD Cup 2025,
                    where it significantly enhances the reasoning performance of base models under challenging
                    scenarios. Our framework achieves substantial improvements in both answer accuracy and knowledge
                    overlap scores, outperforming baselines by 5.06% on the single-source task, 6.35% on the
                    multi-source task, and 5.03% on the multi-turn task.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>[KDD'25] A Survey of WebAgents: Towards Next-Generation AI Agents for Web Automation with
                    Large
                    Foundation Models</papertitle>
                  <br>
                  Liangbo Ning, Ziran Liang, <b>Zhuohang Jiang</b>, Haohao Qu, Yujuan Ding, Wenqi Fan, Xiao-yong Wei,
                  Shanru Lin, Hui Liu, Philip S. Yu, Qing Li
                  <br>
                  <img src="https://img.shields.io/badge/CCF_A-KDD-1f77b4?style=flat-square" alt="CCF-A">
                  <a href="https://arxiv.org/abs/2503.23350"><img
                      src="https://img.shields.io/badge/arXiv-2503.23350-b31b1b?style=flat-square" alt="arXiv"></a>
                  <br>
                  <font color='gray'>
                    With the advancement of web techniques, they have significantly revolutionized various aspects of
                    people's lives. Despite the importance of the web, many tasks performed on it are repetitive and
                    time-consuming, negatively impacting overall quality of life. To efficiently handle these tedious
                    daily tasks, one of the most promising approaches is to advance autonomous agents based on
                    Artificial Intelligence (AI) techniques, referred to as AI Agents, as they can operate continuously
                    without fatigue or performance degradation. In the context of the web, leveraging AI Agents --
                    termed WebAgents -- to automatically assist people in handling tedious daily tasks can dramatically
                    enhance productivity and efficiency. Recently, Large Foundation Models (LFMs) containing billions of
                    parameters have exhibited human-like language understanding and reasoning capabilities, showing
                    proficiency in performing various complex tasks. This naturally raises the question: `Can LFMs be
                    utilized to develop powerful AI Agents that automatically handle web tasks, providing significant
                    convenience to users?' To fully explore the potential of LFMs, extensive research has emerged on
                    WebAgents designed to complete daily web tasks according to user instructions, significantly
                    enhancing the convenience of daily human life. In this survey, we comprehensively review existing
                    research studies on WebAgents across three key aspects: architectures, training, and
                    trustworthiness. Additionally, several promising directions for future research are explored to
                    provide deeper insights.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>[KDD'25] HiBench: Benchmarking LLMs Capability on Hierarchical Structure Reasoning
                  </papertitle>

                  <br>
                  <b>Zhuohang Jiang*</b>, Pangjing Wu*, Ziran Liang*, Peter Q. Chen*, Xu Yuan*, Ye Jia*, Jiancheng Tu*,
                  Chen
                  Li, Peter H.F. Ng, Qing Li
                  <br>
                  <img src="https://img.shields.io/badge/CCF_A-KDD-1f77b4?style=flat-square" alt="CCF-A">
                  <a href="https://arxiv.org/abs/2503.00912"><img
                      src="https://img.shields.io/badge/arXiv-2503.00912-b31b1b?style=flat-square" alt="arXiv"></a>
                  <a href="https://github.com/jzzzzh/HiBench"><img
                      src="https://img.shields.io/badge/GitHub-HiBench-181717?style=flat-square&logo=github"
                      alt="GitHub"></a>
                  <a href="https://huggingface.co/datasets/zhuohang/HiBench"><img
                      src="https://img.shields.io/badge/Hugging%20Face-Dataset-yellow?style=flat-square&logo=huggingface"
                      alt="Hugging Face"></a>
                  <br>
                  <font color='gray'>
                    Structure reasoning is a fundamental capability of large language models (LLMs), enabling them to
                    reason about structured commonsense and answer multi-hop questions. However, existing benchmarks for
                    structure reasoning mainly focus on horizontal and coordinate structures (\emph{e.g.} graphs),
                    overlooking the hierarchical relationships within them. Hierarchical structure reasoning is crucial
                    for human cognition, particularly in memory organization and problem-solving. It also plays a key
                    role in various real-world tasks, such as information extraction and decision-making. To address
                    this gap, we propose HiBench, the first framework spanning from initial structure generation to
                    final proficiency assessment, designed to benchmark the hierarchical reasoning capabilities of LLMs
                    systematically. HiBench encompasses six representative scenarios, covering both fundamental and
                    practical aspects, and consists of 30 tasks with varying hierarchical complexity, totaling 39,519
                    queries. To evaluate LLMs comprehensively, we develop five capability dimensions that depict
                    different facets of hierarchical structure understanding. Through extensive evaluation of 20 LLMs
                    from 10 model families, we reveal key insights into their capabilities and limitations: 1) existing
                    LLMs show proficiency in basic hierarchical reasoning tasks; 2) they still struggle with more
                    complex structures and implicit hierarchical representations, especially in structural modification
                    and textual reasoning. Based on these findings, we create a small yet well-designed instruction
                    dataset, which enhances LLMs' performance on HiBench by an average of 88.84% (Llama-3.1-8B) and
                    31.38% (Qwen2.5-7B) across all tasks. The HiBench dataset and toolkit are available here, this https
                    URL, to encourage evaluation.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>[AAAI'25] Mesoscopic Insights: Orchestrating Multi-Scale & Hybrid Architecture for Image
                    Manipulation Localization</papertitle>
                  <br>
                  Xuekang Zhu, Xiaochen Ma, Lei Su, <b>Zhuohang Jiang</b>, Bo Du, Xiwen Wang, Zeyu Lei, Wentao Feng,
                  Chi-Man Pun, Jizhe Zhou
                  <br>
                  <img src="https://img.shields.io/badge/CCF_A-AAAI-1f77b4?style=flat-square" alt="CCF-A">
                  <a href="https://arxiv.org/abs/2412.13753"><img
                      src="https://img.shields.io/badge/arXiv-2412.13753-b31b1b?style=flat-square" alt="arXiv"></a>
                  <br>
                  <font color='gray'>
                    The mesoscopic level serves as a bridge between the macroscopic and microscopic worlds, addressing
                    gaps overlooked by both. Image manipulation localization (IML), a crucial technique to pursue truth
                    from fake images, has long relied on low-level (microscopic-level) traces. However, in practice,
                    most tampering aims to deceive the audience by altering image semantics. As a result, manipulation
                    commonly occurs at the object level (macroscopic level), which is equally important as microscopic
                    traces. Therefore, integrating these two levels into the mesoscopic level presents a new perspective
                    for IML research. Inspired by this, our paper explores how to simultaneously construct mesoscopic
                    representations of micro and macro information for IML and introduces the Mesorch architecture to
                    orchestrate both. Specifically, this architecture i) combines Transformers and CNNs in parallel,
                    with Transformers extracting macro information and CNNs capturing micro details, and ii) explores
                    across different scales, assessing micro and macro information seamlessly. Additionally, based on
                    the Mesorch architecture, the paper introduces two baseline models aimed at solving IML tasks
                    through mesoscopic representation. Extensive experiments across four datasets have demonstrated that
                    our models surpass the current state-of-the-art in terms of performance, computational complexity,
                    and robustness.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <!-- <a href="under-review.html" id="AutoCost"> -->

                  <papertitle>[NIPS'24] IMDL-BenCo: A Comprehensive Benchmark and Codebase for Image Manipulation
                    Detection &
                    Localization</papertitle>
                  <br>
                  Xiaochen Ma*, Xuekang Zhu*, Lei Su*, Bo Du*, <b>Zhuohang Jiang*</b>, Bingkui Tong*, Zeyu Lei*, Xinyu
                  Yang*,
                  Chi-Man Pun, Jiancheng Lv, Jizhe Zhou
                  <br>
                  <img src="https://img.shields.io/badge/CCF_A-NIPS-1f77b4?style=flat-square" alt="CCF-A">
                  <a href="https://arxiv.org/abs/2406.10580"><img
                      src="https://img.shields.io/badge/arXiv-2406.10580-b31b1b?style=flat-square" alt="arXiv"></a>
                  <a href="https://github.com/scu-zjz/IMDLBenCo"><img
                      src="https://img.shields.io/badge/GitHub-IMDL--BenCo-181717?style=flat-square&logo=github"
                      alt="GitHub"></a>
                  <br>
                  <font color='gray'>
                    A comprehensive benchmark is yet to be established in the Image Manipulation Detection &
                    Localization (IMDL) field. The absence of such a benchmark leads to insufficient and misleading
                    model evaluations, severely undermining the development of this field. However, the scarcity of
                    open-sourced baseline models and inconsistent training and evaluation protocols make conducting
                    rigorous experiments and faithful comparisons among IMDL models challenging. To address these
                    challenges, we introduce IMDL-BenCo, the first comprehensive IMDL benchmark and modular codebase.
                    IMDL-BenCo:i) decomposes the IMDL framework into standardized, reusable components and
                    revises the model construction pipeline, improving coding efficiency and customization
                    flexibility;ii) fully implements or incorporates training code for state-of-the-art models
                    to establish a comprehensive IMDL benchmark; and iii) conducts deep analysis based on the
                    established benchmark and codebase, offering new insights into IMDL model architecture, dataset
                    characteristics, and evaluation standards. Specifically, IMDL-BenCo includes common processing
                    algorithms, 8 state-of-the-art IMDL models (1 of which are reproduced from scratch), 2 sets of
                    standard training and evaluation protocols, 15 GPU-accelerated evaluation metrics, and 3 kinds of
                    robustness evaluation. This benchmark and codebase represent a significant leap forward in
                    calibrating the current progress in the IMDL field and inspiring future breakthroughs.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <!-- <a href="under-review.html" id="AutoCost"> -->

                  <papertitle>Beyond Visual Appearances: Privacy-sensitive Objects Identification via Hybrid Graph
                    Reasoning</papertitle>
                  <br>
                  <b>Zhuohang Jiang*</b>, Bingkui Tong*, Xia Du, Ahmed Alhammadi, Jizhe Zhou
                  <br>
                  <a href="https://arxiv.org/abs/2406.12736"><img
                      src="https://img.shields.io/badge/arXiv-2406.12736-b31b1b?style=flat-square" alt="arXiv"></a>
                  <br>
                  <b>Zhuohang Jiang*</b>, Bingkui Tong*, Xia Du, Ahmed Alhammadi, Jizhe Zhou

                  <br>
                  <font color='gray'>
                    To explicitly derive the objects' privacy class from the scene contexts, in this paper, we interpret
                    the POI task as a visual reasoning task aimed at the privacy of each object in the scene. Following
                    this interpretation, we propose the PrivacyGuard framework for POI. PrivacyGuard contains three
                    stages. i) Structuring: an unstructured image is first converted into a structured, heterogeneous
                    scene graph that embeds rich scene contexts. ii) Data Augmentation: a contextual perturbation
                    oversampling strategy is proposed to create slightly perturbed privacy-sensitive objects in a scene
                    graph, thereby balancing the skewed distribution of privacy classes. iii) Hybrid Graph Generation &
                    Reasoning: the balanced, heterogeneous scene graph is then transformed into a hybrid graph by
                    endowing it with extra "node-node" and "edge-edge" homogeneous paths. These homogeneous paths allow
                    direct message passing between nodes or edges, thereby accelerating reasoning and facilitating the
                    capturing of subtle context changes.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>





          <div class="more-publications" style="display: none;">
            <!-- Remaining publications go here -->

            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                    <!-- <a href="under-review.html" id="AutoCost"> -->

                    <papertitle>[ICONIP'23] TPTGAN: Two-Path Transformer-Based Generative Adversarial Network Using
                      Joint Magnitude Masking and Complex Spectral Mapping for Speech Enhancement</papertitle>
                    <br>
                    Zhaoyi Liu, <b>Zhuohang Jiang</b>, Wendian Luo, Zhuoyao Fan, Haoda Di, Yufan Long, Haizhou Wang
                    <br>
                    <img src="https://img.shields.io/badge/CCF_C-ICONIP-ff7f0e?style=flat-square" alt="CCF-C">
                    <a href="https://link.springer.com/chapter/10.1007/978-981-99-8138-0_5"><img
                        src="https://img.shields.io/badge/Paper-springer-2e8b57?style=flat-square" alt="Paper"></a>
                    <br>
                    <font color='gray'>
                      In this paper, we propose a two-path transformer-based metric generative adversarial network
                      (TPTGAN) for speech enhancement in the time-frequency domain. The generator consists of an
                      encoder, a two-stage transformer module, a magnitude mask decoder and a complex spectrum decoder.
                      Published in ICONIP 2023.
                    </font>
                  </td>
                </tr>
              </tbody>
            </table>

            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                    <!-- <a href="under-review.html" id="AutoCost"> -->

                    <papertitle>IML-ViT: Benchmarking Image Manipulation Localization by Vision Transformer</papertitle>
                    <br>
                    Xiaochen Ma, Bo Du, <b>Zhuohang Jiang</b>, Ahmed Y. Al Hammadi, Jizhe Zhou
                    <br>
                    <a href="https://arxiv.org/abs/2307.14863"><img
                        src="https://img.shields.io/badge/arXiv-2307.14863-b31b1b?style=flat-square" alt="arXiv"></a>
                    <br>
                    <font color='gray'>
                      Due to limited datasets, there is currently no pure ViT-based approach for IML to serve as a
                      benchmark, and CNNs dominate the entire task. Nevertheless, CNNs suffer from weak long-range and
                      non-semantic modeling. To bridge this gap, based on the fact that artifacts are sensitive to image
                      resolution, amplified under multi-scale features, and massive at the manipulation border, we
                      formulate the answer to the former question as building a ViT with high-resolution capacity,
                      multi-scale feature extraction capability, and manipulation edge supervision that could converge
                      with a small amount of data. We term this simple but effective ViT paradigm IML-ViT, which has
                      significant potential to become a new benchmark for IML. Extensive experiments on five benchmark
                      datasets verified our model outperforms the state-of-the-art manipulation localization methods.
                    </font>
                  </td>
                </tr>
              </tbody>
            </table>


            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                    <!-- <a href="under-review.html" id="AutoCost"> -->

                    <papertitle>Perceptual MAE for Image Manipulation Localization: A High-level Vision Learner Focusing
                      on Low-level Features</papertitle>
                    <br>
                    Xiaochen Ma, <b>Zhuohang Jiang</b>, Xiong Xu, Chi-Man Pun, Jizhe Zhou
                    <br>
                    <a href="https://arxiv.org/abs/2310.06525"><img
                        src="https://img.shields.io/badge/arXiv-2310.06525-b31b1b?style=flat-square" alt="arXiv"></a>
                    <br>
                    <font color='gray'> This necessitates IML models to carry out a semantic understanding of the entire
                      image. In this paper, we
                      reformulate the IML task as a highâ€‘level
                      vision task that greatly benefits from lowâ€‘level features. We propose a method to enhance the
                      Masked
                      Autoencoder (MAE) by incorporating
                      highâ€‘resolution inputs and a perceptual loss supervision module, which we term Perceptual MAE
                      (PMAE). While
                      MAE has demonstrated an
                      impressive understanding of object semantics, PMAE can also comprehend lowâ€‘level semantics with
                      our
                      proposed
                      enhancements. This
                      paradigm effectively unites the lowâ€‘level and highâ€‘level features of the IML task and outperforms
                      stateâ€‘ofâ€‘theâ€‘art tampering localization methods
                      on five publicly available datasets, as evidenced by extensive experiments.</font>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>

          <div style="text-align: center; margin-top: 20px;">
            <button id="expand-publications-btn" onclick="togglePublications()" class="hvr-grow"
              style="padding: 10px 20px; font-size: 16px; background-color: #007BFF; color: white; border: none; border-radius: 5px; cursor: pointer;">
              <span class="lang-content lang-en">More Publications</span>
              <span class="lang-content lang-zh hidden">æ›´å¤šè®ºæ–‡</span>
            </button>
          </div>
          </div>

          <script>
            function togglePublications() {
              var morePublications = document.querySelector('.more-publications');
              var button = document.getElementById('expand-publications-btn');
              button.addEventListener('click', function () {
                if (morePublications.style.display === 'none') {
                  morePublications.style.display = 'block';
                  button.classList.remove('hvr-grow');
                  button.classList.add('hvr-shrink');
                  // æ›´æ–°æŒ‰é’®æ–‡æœ¬ - æ”¯æŒå¤šè¯­è¨€
                  const enText = button.querySelector('.lang-en');
                  const zhText = button.querySelector('.lang-zh');
                  if (enText) enText.textContent = 'Less Publications';
                  if (zhText) zhText.textContent = 'æ”¶èµ·è®ºæ–‡';
                  morePublications.classList.add('magictime', 'puffIn');
                } else {
                  morePublications.style.display = 'none';
                  button.classList.remove('hvr-shrink');
                  button.classList.add('hvr-grow');
                  // æ›´æ–°æŒ‰é’®æ–‡æœ¬ - æ”¯æŒå¤šè¯­è¨€
                  const enText = button.querySelector('.lang-en');
                  const zhText = button.querySelector('.lang-zh');
                  if (enText) enText.textContent = 'More Publications';
                  if (zhText) zhText.textContent = 'æ›´å¤šè®ºæ–‡';
                }
              });
            }
          </script>

          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>
                    <span class="lang-content lang-en">Selected Projects</span>
                    <span class="lang-content lang-zh hidden">ä¸»è¦é¡¹ç›®</span>
                  </heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>
                    <span class="lang-content lang-en">HiBench: Benchmark for Hierarchical Reasoning</span>
                    <span class="lang-content lang-zh hidden">HiBenchï¼šå±‚æ¬¡åŒ–æ¨ç†åŸºå‡†</span>
                  </papertitle>
                  <br>
                  <strong>First Author & Team Leader</strong> â€¢ <em>KDD 2025 Benchmark Track</em> â€¢ 2025
                  <br>
                  <a href="https://arxiv.org/abs/2503.00912"><img
                      src="https://img.shields.io/badge/arXiv-2503.00912-b31b1b?style=flat-square" alt="arXiv"></a>
                  <a href="https://github.com/jzzzzh/HiBench"><img
                      src="https://img.shields.io/badge/GitHub-HiBench-181717?style=flat-square&logo=github"
                      alt="GitHub"></a>
                  <a href="https://huggingface.co/datasets/zhuohang/HiBench"><img
                      src="https://img.shields.io/badge/Hugging%20Face-Dataset-yellow?style=flat-square&logo=huggingface"
                      alt="Hugging Face"></a>
                  <br>
                  <!-- è‹±æ–‡ç‰ˆæœ¬ -->
                  <div class="lang-content lang-en">
                    <font color='gray'>
                      Designed and developed the first comprehensive benchmark for evaluating LLMs' capability on
                      hierarchical structure reasoning.
                      The benchmark encompasses six representative scenarios with 39,519 queries across varying
                      hierarchical complexity.
                      <strong>Key contributions:</strong> (1) Led the architectural design and implementation of the
                      evaluation framework,
                      (2) Coordinated a multi-institutional team across different time zones, (3) Open-sourced the
                      complete toolkit
                      including dataset, evaluation metrics, and baseline implementations. The benchmark has been
                      accepted
                      as an
                      <strong>oral presentation</strong> at KDD 2025 and is being adopted by multiple research groups
                      for
                      hierarchical reasoning evaluation.
                    </font>
                  </div>

                  <!-- ä¸­æ–‡ç‰ˆæœ¬ -->
                  <div class="lang-content lang-zh hidden">
                    <font color='gray'>
                      è®¾è®¡å¹¶å¼€å‘äº†é¦–ä¸ªç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹å±‚æ¬¡åŒ–ç»“æ„æ¨ç†èƒ½åŠ›çš„ç»¼åˆåŸºå‡†ã€‚
                      è¯¥åŸºå‡†æ¶µç›–6ä¸ªä»£è¡¨æ€§åœºæ™¯ï¼ŒåŒ…å«39,519ä¸ªä¸åŒå±‚æ¬¡å¤æ‚åº¦çš„æŸ¥è¯¢ã€‚
                      <strong>ä¸»è¦è´¡çŒ®ï¼š</strong> (1) é¢†å¯¼äº†è¯„ä¼°æ¡†æ¶çš„æ¶æ„è®¾è®¡å’Œå®ç°ï¼Œ
                      (2) åè°ƒäº†è·¨æ—¶åŒºçš„å¤šæœºæ„å›¢é˜Ÿï¼Œ(3) å¼€æºäº†å®Œæ•´çš„å·¥å…·åŒ…ï¼Œ
                      åŒ…æ‹¬æ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡å’ŒåŸºçº¿å®ç°ã€‚è¯¥åŸºå‡†å·²è¢«KDD 2025æ¥æ”¶ä¸º
                      <strong>å£å¤´æŠ¥å‘Š</strong>ï¼Œæ­£è¢«å¤šä¸ªç ”ç©¶å›¢é˜Ÿé‡‡ç”¨è¿›è¡Œå±‚æ¬¡æ¨ç†è¯„ä¼°ã€‚
                    </font>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>
                    <span class="lang-content lang-en">Meta CRAG-MM: Multimodal Retrieval Challenge</span>
                    <span class="lang-content lang-zh hidden">Meta CRAG-MMï¼šå¤šæ¨¡æ€æ£€ç´¢æŒ‘æˆ˜</span>
                  </papertitle>
                  <br>
                  <strong>Team Leader</strong> â€¢ <em>KDD Cup 2025</em> â€¢ 2025
                  <br>
                  <!-- è‹±æ–‡ç‰ˆæœ¬ -->
                  <div class="lang-content lang-en">
                    <font color='gray'>
                      Led a team to achieve <strong>12th place globally</strong> among hundreds of international teams
                      in
                      the Meta CRAG-MM
                      Multimodal Retrieval Challenge. <strong>Key contributions:</strong> (1) Designed novel multimodal
                      fusion architectures
                      combining vision and language understanding, (2) Implemented efficient retrieval-augmented
                      generation pipelines,
                      (3) Coordinated team efforts in model development, hyperparameter optimization, and submission
                      strategies.
                      The challenge focused on developing AI systems capable of understanding and retrieving information
                      from
                      multimodal content, which aligns with current trends in large multimodal models.
                    </font>
                  </div>

                  <!-- ä¸­æ–‡ç‰ˆæœ¬ -->
                  <div class="lang-content lang-zh hidden">
                    <font color='gray'>
                      é¢†å¯¼å›¢é˜Ÿåœ¨Meta CRAG-MMå¤šæ¨¡æ€æ£€ç´¢æŒ‘æˆ˜èµ›ä¸­ï¼Œåœ¨æ•°ç™¾ä¸ªå›½é™…å›¢é˜Ÿä¸­å–å¾—<strong>å…¨çƒç¬¬12å</strong>çš„æˆç»©ã€‚
                      <strong>ä¸»è¦è´¡çŒ®ï¼š</strong> (1) è®¾è®¡äº†ç»“åˆè§†è§‰å’Œè¯­è¨€ç†è§£çš„æ–°é¢–å¤šæ¨¡æ€èåˆæ¶æ„ï¼Œ
                      (2) å®ç°äº†é«˜æ•ˆçš„æ£€ç´¢å¢å¼ºç”Ÿæˆç®¡é“ï¼Œ
                      (3) åè°ƒå›¢é˜Ÿåœ¨æ¨¡å‹å¼€å‘ã€è¶…å‚æ•°ä¼˜åŒ–å’Œæäº¤ç­–ç•¥æ–¹é¢çš„åŠªåŠ›ã€‚
                      è¯¥æŒ‘æˆ˜ä¸“æ³¨äºå¼€å‘èƒ½å¤Ÿç†è§£å’Œæ£€ç´¢å¤šæ¨¡æ€å†…å®¹ä¿¡æ¯çš„AIç³»ç»Ÿï¼Œ
                      è¿™ä¸å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„å½“å‰è¶‹åŠ¿ç›¸ç¬¦ã€‚
                    </font>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>
                    <span class="lang-content lang-en">IMDL-BenCo: Benchmark for Image Manipulation Detection &
                      Localization</span>
                    <span class="lang-content lang-zh hidden">IMDL-BenCoï¼šå›¾åƒæ“ä½œæ£€æµ‹ä¸å®šä½åŸºå‡†</span>
                  </papertitle>
                  <br>
                  <strong>Co-First Author</strong> â€¢ <em>NeurIPS 2024 Benchmark Track â€” Spotlight</em> â€¢ 2024
                  <br>
                  <a href="https://arxiv.org/abs/2406.10580"><img
                      src="https://img.shields.io/badge/arXiv-2406.10580-b31b1b?style=flat-square" alt="arXiv"></a>
                  <a href="https://github.com/scu-zjz/IMDLBenCo"><img
                      src="https://img.shields.io/badge/GitHub-IMDLBenCo-181717?style=flat-square&logo=github"
                      alt="GitHub"></a>
                  <br>
                  <!-- è‹±æ–‡ç‰ˆæœ¬ -->
                  <div class="lang-content lang-en">
                    <font color='gray'>
                      Developed the first comprehensive benchmark and codebase for Image Manipulation Detection &
                      Localization (IMDL).
                      <strong>Key contributions:</strong> (1) Implemented GPU-accelerated evaluation metrics for fair
                      and
                      efficient comparison,
                      (2) Designed modular codebase architecture enabling easy customization and extension, (3)
                      Co-authored the manuscript
                      that received a <strong>Spotlight Award</strong> at NeurIPS 2024. The benchmark includes 8
                      state-of-the-art models,
                      15 evaluation metrics, and comprehensive robustness evaluation protocols, significantly advancing
                      the field's
                      standardization and reproducibility.
                    </font>
                  </div>

                  <!-- ä¸­æ–‡ç‰ˆæœ¬ -->
                  <div class="lang-content lang-zh hidden">
                    <font color='gray'>
                      å¼€å‘äº†é¦–ä¸ªç”¨äºå›¾åƒæ“ä½œæ£€æµ‹ä¸å®šä½ï¼ˆIMDLï¼‰çš„ç»¼åˆåŸºå‡†å’Œä»£ç åº“ã€‚
                      <strong>ä¸»è¦è´¡çŒ®ï¼š</strong> (1) å®ç°äº†GPUåŠ é€Ÿçš„è¯„ä¼°æŒ‡æ ‡ï¼Œå®ç°å…¬å¹³é«˜æ•ˆçš„æ¯”è¾ƒï¼Œ
                      (2) è®¾è®¡äº†æ¨¡å—åŒ–ä»£ç åº“æ¶æ„ï¼Œä¾¿äºå®šåˆ¶å’Œæ‰©å±•ï¼Œ(3)
                      åˆè‘—äº†è·å¾—NeurIPS 2024 <strong>Spotlightå¥–</strong>çš„æ‰‹ç¨¿ã€‚è¯¥åŸºå‡†åŒ…å«8ä¸ªæœ€å…ˆè¿›çš„æ¨¡å‹ã€
                      15ä¸ªè¯„ä¼°æŒ‡æ ‡å’Œå…¨é¢çš„é²æ£’æ€§è¯„ä¼°åè®®ï¼Œæ˜¾è‘—æ¨è¿›äº†è¯¥é¢†åŸŸçš„
                      æ ‡å‡†åŒ–å’Œå¯é‡ç°æ€§ã€‚
                    </font>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>


          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>
                    <span class="lang-content lang-en">Invited Talks</span>
                    <span class="lang-content lang-zh hidden">ç‰¹é‚€æŠ¥å‘Š</span>
                  </heading>
                </td>
              </tr>
              <tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>
                    <span class="lang-content lang-en">Understanding Hierarchical Data with Large Language Models: RAG,
                      Structural Reasoning, and Future Directions</span>
                    <span class="lang-content lang-zh hidden">ç”¨å¤§è¯­è¨€æ¨¡å‹ç†è§£å±‚æ¬¡åŒ–æ•°æ®ï¼šRAGã€ç»“æ„æ¨ç†ä¸æœªæ¥æ–¹å‘</span>
                  </papertitle>
                  <br>
                  <strong>Invited Talks</strong> â€¢ <em>Reasoning Day @ KDD 2025</em> â€¢ Toronto, ON, Canada â€¢ Aug 2025
                  <br>
                  <!-- è‹±æ–‡ç‰ˆæœ¬ -->
                  <div class="lang-content lang-en">
                    <font color='gray'>
                      Invited to deliver a presentation at the prestigious KDD 2025 Reasoning Day workshop.<br>
                      The talk will explore cutting-edge developments in leveraging Large Language Models for
                      hierarchical
                      data understanding,<br>
                      with particular focus on Retrieval-Augmented Generation (RAG) systems and structural reasoning
                      capabilities.<br>
                      <strong>Key topics:</strong><br>
                      (1) Novel approaches to hierarchical data representation in LLM contexts,<br>
                      (2) Integration of structural reasoning with retrieval-augmented generation,<br>
                      (3) Future research directions in reasoning-enhanced AI systems,<br>
                      (4) Practical applications and deployment considerations for hierarchical reasoning in real-world
                      scenarios.<br>
                      This invitation recognizes the impact of our HiBench work and positions our research at the
                      forefront of LLM reasoning capabilities.
                    </font>
                  </div>

                  <!-- ä¸­æ–‡ç‰ˆæœ¬ -->
                  <div class="lang-content lang-zh hidden">
                    <font color='gray'>
                      å—é‚€åœ¨äº«æœ‰ç››èª‰çš„KDD 2025æ¨ç†æ—¥ç ”è®¨ä¼šä¸Šå‘è¡¨æ¼”è®²ã€‚<br>
                      è¯¥æŠ¥å‘Šå°†æ¢è®¨åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå±‚æ¬¡åŒ–æ•°æ®ç†è§£çš„å‰æ²¿å‘å±•ï¼Œ<br>
                      ç‰¹åˆ«å…³æ³¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿå’Œç»“æ„æ¨ç†èƒ½åŠ›ã€‚<br>
                      <strong>ä¸»è¦è®®é¢˜ï¼š</strong><br>
                      (1) åœ¨LLMè¯­å¢ƒä¸‹å±‚æ¬¡åŒ–æ•°æ®è¡¨ç¤ºçš„æ–°é¢–æ–¹æ³•ï¼Œ<br>
                      (2) ç»“æ„æ¨ç†ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆçš„é›†æˆï¼Œ<br>
                      (3) æ¨ç†å¢å¼ºAIç³»ç»Ÿçš„æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œ<br>
                      (4) åœ¨ç°å®åœºæ™¯ä¸­å±‚æ¬¡æ¨ç†çš„å®é™…åº”ç”¨å’Œéƒ¨ç½²è€ƒè™‘ã€‚<br>
                      è¿™ä¸€é‚€è¯·è®¤å¯äº†æˆ‘ä»¬HiBenchå·¥ä½œçš„å½±å“åŠ›ï¼Œå¹¶å°†æˆ‘ä»¬çš„ç ”ç©¶ç½®äº
                      LLMæ¨ç†èƒ½åŠ›çš„å‰æ²¿ã€‚
                    </font>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>

          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>
                    <span class="lang-content lang-en">Education</span>
                    <span class="lang-content lang-zh hidden">æ•™è‚²èƒŒæ™¯</span>
                  </heading>
                </td>
              </tr>
              <tr>
            </tbody>
          </table>

          <!-- è‹±æ–‡ç‰ˆæœ¬ -->
          <div class="lang-content lang-en">
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                    <b>Sichuan University</b>, Chengdu, Sichuan, China
                    <br>
                    B.E. in Computer Science and Technology â€¢ Sep. 2020 to Jun. 2024
                    </br>
                  </td>
                  <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img
                      src="images/SichuanUniversityLOGO.png" width="75" height="75"></td>
                </tr>
              </tbody>
            </table>
            <br>
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                    <b>Hong Kong Polytechnic University</b>, Hongkong, China
                    <br>
                    PHD. in Computer Science and Technology â€¢ Sep. 2024 to Present
                    </br>
                  </td>
                  <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img src="images/polyu.jpg"
                      width="75" height="75"></td>
                </tr>
              </tbody>
            </table>
          </div>

          <!-- ä¸­æ–‡ç‰ˆæœ¬ -->
          <div class="lang-content lang-zh hidden">
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                    <b>å››å·å¤§å­¦</b>ï¼Œæˆéƒ½ï¼Œå››å·ï¼Œä¸­å›½
                    <br>
                    è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯å­¦å£« â€¢ 2020å¹´9æœˆè‡³2024å¹´6æœˆ
                    </br>
                  </td>
                  <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img
                      src="images/SichuanUniversityLOGO.png" width="75" height="75"></td>
                </tr>
              </tbody>
            </table>
            <br>
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                    <b>é¦™æ¸¯ç†å·¥å¤§å­¦</b>ï¼Œé¦™æ¸¯ï¼Œä¸­å›½
                    <br>
                    è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯åšå£« â€¢ 2024å¹´9æœˆè‡³ä»Š
                    </br>
                  </td>
                  <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img src="images/polyu.jpg"
                      width="75" height="75"></td>
                </tr>
              </tbody>
            </table>
          </div>

          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>
                    <span class="lang-content lang-en">Experience</span>
                    <span class="lang-content lang-zh hidden">å·¥ä½œç»å†</span>
                  </heading>
                </td>
              </tr>
              <tr>
            </tbody>
          </table>



          <!-- è‹±æ–‡ç‰ˆæœ¬ -->
          <div class="lang-content lang-en">
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                    <b>National University of Singapore (NUS)</b>
                    <br>
                    <strong>Summer School Participant</strong> â€¢ Aug. 2023
                    <br>
                    â€¢ Participated in intensive research program at School of Computing
                    <br>
                    â€¢ Completed face recognition project using CNN-based feature extraction and similarity matching
                    <br>
                    â€¢ Gained international research experience and cross-cultural collaboration skills
                    </br>
                  </td>
                  <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img src="images/nus.png"
                      width="135" height="75"></td>
                </tr>
              </tbody>
            </table>
            <br>

            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                    <a href="http://dicalab.cn/"> DICALab</a>, <b>Sichuan University</b>
                    <br>
                    <strong>Research Assistant</strong> â€¢ Sep. 2022 to Jun. 2024
                    <br>
                    Advisor: <a href="https://scholar.google.com/citations?user=-cNWmJMAAAAJ&hl=zh-CN"> Prof. JiZhe
                      Zhou</a>
                    <br>
                    â€¢ Developed graph-based frameworks for privacy-sensitive object detection
                    <br>
                    â€¢ Participated in National Natural Science Foundation of China project
                    <br>
                    â€¢ Contributed to National Key R&D Program of China
                    <br>
                    â€¢ Co-authored multiple publications in top-tier conferences and journals
                    </br>
                  </td>
                  <td style="padding:0px 20px 0px 20px;width:8%;vertical-align:middle"><img src="images/DICA.png"
                      width="75" height="75"></td>
                </tr>
              </tbody>
            </table>
          </div>

          <!-- ä¸­æ–‡ç‰ˆæœ¬ -->
          <div class="lang-content lang-zh hidden">
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                    <b>æ–°åŠ å¡å›½ç«‹å¤§å­¦ (NUS)</b>
                    <br>
                    <strong>æš‘æœŸå­¦æ ¡å­¦å‘˜</strong> â€¢ 2023å¹´8æœˆ
                    <br>
                    â€¢ å‚ä¸è®¡ç®—æœºå­¦é™¢çš„å¯†é›†ç ”ç©¶é¡¹ç›®
                    <br>
                    â€¢ å®ŒæˆåŸºäºCNNç‰¹å¾æå–å’Œç›¸ä¼¼åº¦åŒ¹é…çš„äººè„¸è¯†åˆ«é¡¹ç›®
                    <br>
                    â€¢ è·å¾—å›½é™…ç ”ç©¶ç»éªŒå’Œè·¨æ–‡åŒ–åˆä½œæŠ€èƒ½
                    </br>
                  </td>
                  <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img src="images/nus.png"
                      width="135" height="75"></td>
                </tr>
              </tbody>
            </table>
            <br>

            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                    <a href="http://dicalab.cn/"> DICALab</a>, <b>å››å·å¤§å­¦</b>
                    <br>
                    <strong>ç ”ç©¶åŠ©ç†</strong> â€¢ 2022å¹´9æœˆè‡³2024å¹´6æœˆ
                    <br>
                    å¯¼å¸ˆï¼š<a href="https://scholar.google.com/citations?user=-cNWmJMAAAAJ&hl=zh-CN"> å‘¨å‰å–†æ•™æˆ</a>
                    <br>
                    â€¢ å¼€å‘ç”¨äºéšç§æ•æ„Ÿå¯¹è±¡æ£€æµ‹çš„åŸºäºå›¾çš„æ¡†æ¶
                    <br>
                    â€¢ å‚ä¸å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘é¡¹ç›®
                    <br>
                    â€¢ å‚ä¸å›½å®¶é‡ç‚¹ç ”å‘è®¡åˆ’é¡¹ç›®
                    <br>
                    â€¢ åœ¨é¡¶çº§ä¼šè®®å’ŒæœŸåˆŠä¸Šåˆè‘—å¤šç¯‡è®ºæ–‡
                    </br>
                  </td>
                  <td style="padding:0px 20px 0px 20px;width:8%;vertical-align:middle"><img src="images/DICA.png"
                      width="75" height="75"></td>
                </tr>
              </tbody>
            </table>
          </div>
          <br>
          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"
            style="margin-bottom: 10px; table-layout: fixed;">
            <tbody>
              <tr>
                <td colspan="2" style="padding:20px 0 0px 0;width:100%;vertical-align:middle;text-align:left;">
                  <heading>
                    <span class="lang-content lang-en">Selected Awards</span>
                    <span class="lang-content lang-zh hidden">ä¸»è¦å¥–é¡¹</span>
                  </heading>
                </td>
              </tr>
              <!-- è‹±æ–‡ç‰ˆæœ¬ -->
              <tr class="lang-content lang-en" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>KDD Cup 2025 â€” Meta CRAG-MM</strong><br>
                  <span style="color: #666; font-size: 0.95em;">Toronto, Canada, 2025</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>12th Place (Global)</strong>
                </td>
              </tr>
              <tr class="lang-content lang-en" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>NeurIPS 2024 â€” IMDLâ€‘BenCo (Coâ€‘first Author)</strong><br>
                  <span style="color: #666; font-size: 0.95em;">Vancouver, Canada, 2024</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>Spotlight Award</strong>
                </td>
              </tr>
              <tr class="lang-content lang-en" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>Outstanding Graduate</strong><br>
                  <span style="color: #666; font-size: 0.95em;">Sichuan University &amp; Sichuan Province, 2024</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>Top Achievement</strong>
                </td>
              </tr>
              <tr class="lang-content lang-en" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>Tencent Scholarship</strong><br>
                  <span style="color: #666; font-size: 0.95em;">Sichuan University, China, 2023</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>Top 2%</strong>
                </td>
              </tr>
              <tr class="lang-content lang-en" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>A-Level Certificate</strong><br>
                  <span style="color: #666; font-size: 0.95em;">Comprehensive Quality Evaluation, China, 2023</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>Excellence</strong>
                </td>
              </tr>
              <tr class="lang-content lang-en" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>Comprehensive First Class Scholarship</strong><br>
                  <span style="color: #666; font-size: 0.95em;">Sichuan University, Sichuan, China, 2022</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>Top 1%</strong>
                </td>
              </tr>
              <tr class="lang-content lang-en">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>Outstanding Students of Sichuan University</strong><br>
                  <span style="color: #666; font-size: 0.95em;">Sichuan University, Sichuan, China, 2022</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>Top 5%</strong>
                </td>
              </tr>

              <!-- ä¸­æ–‡ç‰ˆæœ¬ -->
              <tr class="lang-content lang-zh hidden" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>KDD Cup 2025 â€” Meta CRAG-MM</strong><br>
                  <span style="color: #666; font-size: 0.95em;">åŠ æ‹¿å¤§å¤šä¼¦å¤šï¼Œ2025</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>å…¨çƒç¬¬12å</strong>
                </td>
              </tr>
              <tr class="lang-content lang-zh hidden" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>NeurIPS 2024 â€” IMDLâ€‘BenCoï¼ˆå…±åŒç¬¬ä¸€ä½œè€…ï¼‰</strong><br>
                  <span style="color: #666; font-size: 0.95em;">åŠ æ‹¿å¤§æ¸©å“¥åï¼Œ2024</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>Spotlightå¥–</strong>
                </td>
              </tr>
              <tr class="lang-content lang-zh hidden" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>ä¼˜ç§€æ¯•ä¸šç”Ÿ</strong><br>
                  <span style="color: #666; font-size: 0.95em;">å››å·å¤§å­¦ &amp; å››å·çœï¼Œ2024</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>æœ€é«˜æˆå°±</strong>
                </td>
              </tr>
              <tr class="lang-content lang-zh hidden" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>è…¾è®¯å¥–å­¦é‡‘</strong><br>
                  <span style="color: #666; font-size: 0.95em;">å››å·å¤§å­¦ï¼Œä¸­å›½ï¼Œ2023</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>å‰2%</strong>
                </td>
              </tr>
              <tr class="lang-content lang-zh hidden" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>Açº§è¯ä¹¦</strong><br>
                  <span style="color: #666; font-size: 0.95em;">ç»¼åˆç´ è´¨è¯„ä»·ï¼Œä¸­å›½ï¼Œ2023</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>ä¼˜ç§€</strong>
                </td>
              </tr>
              <tr class="lang-content lang-zh hidden" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>ç»¼åˆä¸€ç­‰å¥–å­¦é‡‘</strong><br>
                  <span style="color: #666; font-size: 0.95em;">å››å·å¤§å­¦ï¼Œå››å·ï¼Œä¸­å›½ï¼Œ2022</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>å‰1%</strong>
                </td>
              </tr>
              <tr class="lang-content lang-zh hidden">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>å››å·å¤§å­¦ä¼˜ç§€å­¦ç”Ÿ</strong><br>
                  <span style="color: #666; font-size: 0.95em;">å››å·å¤§å­¦ï¼Œå››å·ï¼Œä¸­å›½ï¼Œ2022</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>å‰5%</strong>
                </td>
              </tr>
            </tbody>
          </table>






          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"
            style="margin-bottom: 10px; table-layout: fixed;">
            <tbody>
              <tr>
                <td colspan="2" style="padding:20px 0 0px 0;width:100%;vertical-align:middle;text-align:left;">
                  <heading>
                    <span class="lang-content lang-en">Professional Service</span>
                    <span class="lang-content lang-zh hidden">ä¸“ä¸šæœåŠ¡</span>
                  </heading>
                </td>
              </tr>
              <!-- è‹±æ–‡ç‰ˆæœ¬ -->
              <tr class="lang-content lang-en" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>Conference & Journal Reviewer</strong><br>
                  <span style="color: #666; font-size: 0.95em;">2023-2025</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>TIP, ECCV, NeurIPS, KDD, AAAI, IoTJ,</strong>
                </td>
              </tr>
              <tr class="lang-content lang-en" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>Teaching Assistant</strong><br>
                  <span style="color: #666; font-size: 0.95em;">The Hong Kong Polytechnic University (PolyU)</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>Artificial Intelligence (COMP4431)<br>NLP Practicum (COMP5423)</strong>
                </td>
              </tr>

              <!-- ä¸­æ–‡ç‰ˆæœ¬ -->
              <tr class="lang-content lang-zh hidden" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>ä¼šè®®å’ŒæœŸåˆŠå®¡ç¨¿äºº</strong><br>
                  <span style="color: #666; font-size: 0.95em;">2023-2025</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>TIP, ECCV, NeurIPS, KDD, AAAI, IoTJ</strong>
                </td>
              </tr>
              <tr class="lang-content lang-zh hidden" style="border-bottom:1px solid #eee;">
                <td style="width:60%;vertical-align:middle;padding:10px 0;text-align:left;">
                  <strong>æ•™å­¦åŠ©ç†</strong><br>
                  <span style="color: #666; font-size: 0.95em;">é¦™æ¸¯ç†å·¥å¤§å­¦</span>
                </td>
                <td style="width:40%;vertical-align:middle;padding:10px 0;text-align:right;">
                  <strong>äººå·¥æ™ºèƒ½ (COMP4431)<br>è‡ªç„¶è¯­è¨€å¤„ç†å®è·µ (COMP5423)</strong>
                </td>
              </tr>
            </tbody>
          </table>

          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"
            style="margin-bottom: 10px; table-layout: fixed;">
            <tbody>
              <tr>
                <td colspan="2" style="padding:20px 0 0px 0;width:100%;vertical-align:middle;text-align:left;">
                  <heading>
                    <span class="lang-content lang-en">Skills</span>
                    <span class="lang-content lang-zh hidden">æŠ€èƒ½</span>
                  </heading>
                </td>
              </tr>
              <!-- è‹±æ–‡ç‰ˆæœ¬ -->
              <tr class="lang-content lang-en">
                <td style="width:30%;vertical-align:top;padding:10px 0;text-align:left;">
                  <strong>Research Topics</strong>
                </td>
                <td style="width:70%;vertical-align:top;padding:10px 0;text-align:left;">
                  Large Language Models (LLMs), Retrievalâ€‘Augmented Generation (RAG), Recommender Systems (RecSys)
                </td>
              </tr>
              <tr class="lang-content lang-en">
                <td style="width:30%;vertical-align:top;padding:10px 0;text-align:left;">
                  <strong>Frameworks &amp; Tools</strong>
                </td>
                <td style="width:70%;vertical-align:top;padding:10px 0;text-align:left;">
                  PyTorch, Hugging Face, NumPy, Docker, Git, Anaconda
                </td>
              </tr>
              <tr class="lang-content lang-en">
                <td style="width:30%;vertical-align:top;padding:10px 0;text-align:left;">
                  <strong>Languages</strong>
                </td>
                <td style="width:70%;vertical-align:top;padding:10px 0;text-align:left;">
                  Mandarin (native), English (fluent, IELTS 6.5)
                </td>
              </tr>

              <!-- ä¸­æ–‡ç‰ˆæœ¬ -->
              <tr class="lang-content lang-zh hidden">
                <td style="width:30%;vertical-align:top;padding:10px 0;text-align:left;">
                  <strong>ç ”ç©¶æ–¹å‘</strong>
                </td>
                <td style="width:70%;vertical-align:top;padding:10px 0;text-align:left;">
                  å¤§è¯­è¨€æ¨¡å‹ (LLMs)ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG)ï¼Œæ¨èç³»ç»Ÿ (RecSys)
                </td>
              </tr>
              <tr class="lang-content lang-zh hidden">
                <td style="width:30%;vertical-align:top;padding:10px 0;text-align:left;">
                  <strong>æ¡†æ¶å’Œå·¥å…·</strong>
                </td>
                <td style="width:70%;vertical-align:top;padding:10px 0;text-align:left;">
                  PyTorch, Hugging Face, NumPy, Docker, Git, Anaconda
                </td>
              </tr>
              <tr class="lang-content lang-zh hidden">
                <td style="width:30%;vertical-align:top;padding:10px 0;text-align:left;">
                  <strong>è¯­è¨€èƒ½åŠ›</strong>
                </td>
                <td style="width:70%;vertical-align:top;padding:10px 0;text-align:left;">
                  ä¸­æ–‡ï¼ˆæ¯è¯­ï¼‰ï¼Œè‹±æ–‡ï¼ˆæµåˆ©ï¼Œé›…æ€6.5ï¼‰
                </td>
              </tr>
            </tbody>
          </table>

          <hr
            style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <div style="text-align: center;">
            <a href="https://clustrmaps.com/site/1buac" title="Visit tracker">
              <img src="//www.clustrmaps.com/map_v2.png?d=arwquO5nouqZwMbhchX9awZ1bM3tO8fPWrcUr78F1E4&cl=ffffff" />
            </a>
          </div>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px">
                  <div style="float:left;">
                    Updated at Aug. 2025
                  </div>
                  <div style="float:right;">
                    Thanks <a href="https://jonbarron.info">Jon Barron</a> for this amazing template
                  </div>
                  <br>
                  <br>
                  <p></p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>


  <script>
    new WOW().init();


    function showMoreNews() {
      var button = document.getElementById('expand-btn');

      // ç¡®ä¿åªæ·»åŠ ä¸€æ¬¡äº‹ä»¶ç›‘å¬å™¨
      if (button.hasEventListener) return;
      button.hasEventListener = true;

      button.addEventListener('click', function () {
        // è·å–æ‰€æœ‰çš„more-newså…ƒç´ 
        const allMoreNews = document.querySelectorAll('.more-news');

        // æ£€æŸ¥æ˜¯å¦å·²å±•å¼€ï¼ˆé€šè¿‡æ£€æŸ¥ç¬¬ä¸€ä¸ªmore-newsçš„displayçŠ¶æ€ï¼‰
        const isExpanded = allMoreNews[0].style.display === 'block';

        if (!isExpanded) {
          // å±•å¼€æ‰€æœ‰more-newsåŒºåŸŸ
          allMoreNews.forEach(moreNews => {
            moreNews.style.display = 'block';
            moreNews.classList.add('magictime', 'puffIn');
          });

          // æ›´æ–°æŒ‰é’®æ–‡æœ¬ - æ”¯æŒå¤šè¯­è¨€
          const enText = button.querySelector('.lang-en');
          const zhText = button.querySelector('.lang-zh');
          if (enText) enText.textContent = 'Less News';
          if (zhText) zhText.textContent = 'æ”¶èµ·æ–°é—»';
          button.classList.remove('hvr-grow');
          button.classList.add('hvr-shrink');
        } else {
          // æ”¶èµ·æ‰€æœ‰more-newsåŒºåŸŸ
          allMoreNews.forEach(moreNews => {
            moreNews.style.display = 'none';
          });

          // æ›´æ–°æŒ‰é’®æ–‡æœ¬ - æ”¯æŒå¤šè¯­è¨€
          const enText = button.querySelector('.lang-en');
          const zhText = button.querySelector('.lang-zh');
          if (enText) enText.textContent = 'More News';
          if (zhText) zhText.textContent = 'æ›´å¤šæ–°é—»';
          button.classList.remove('hvr-shrink');
          button.classList.add('hvr-grow');
        }
      });
    }

    // è¯­è¨€åˆ‡æ¢åŠŸèƒ½
    function switchLanguage(lang) {
      const enBtn = document.getElementById('en-btn');
      const zhBtn = document.getElementById('zh-btn');
      const langContent = document.querySelectorAll('.lang-content');
      const hiddenClass = 'hidden';

      langContent.forEach(content => {
        if (content.classList.contains(`lang-${lang}`)) {
          content.classList.remove(hiddenClass);
        } else {
          content.classList.add(hiddenClass);
        }
      });

      if (lang === 'en') {
        enBtn.classList.add('active');
        zhBtn.classList.remove('active');
      } else {
        zhBtn.classList.add('active');
        enBtn.classList.remove('active');
      }

      // ä¿å­˜è¯­è¨€é€‰æ‹©åˆ°æœ¬åœ°å­˜å‚¨
      localStorage.setItem('language', lang);
    }

    // åŸºäºæ—¶é—´çš„è‡ªåŠ¨æ¨¡å¼åˆ‡æ¢
    function setThemeBasedOnTime() {
      const now = new Date();
      const hour = now.getHours();
      const themeToggle = document.getElementById('theme-toggle');

      // 6:00-18:00 ä¸ºç™½å¤©æ¨¡å¼ï¼Œ18:00-6:00 ä¸ºå¤œé—´æ¨¡å¼
      const isNightTime = hour < 6 || hour >= 18;

      if (isNightTime) {
        document.body.classList.add('dark-mode');
        if (themeToggle) themeToggle.textContent = 'â˜€ï¸';
      } else {
        document.body.classList.remove('dark-mode');
        if (themeToggle) themeToggle.textContent = 'ğŸŒ™';
      }

      return isNightTime ? 'dark' : 'light';
    }

    // æ‰‹åŠ¨åˆ‡æ¢ä¸»é¢˜
    function toggleTheme() {
      const isDark = document.body.classList.toggle('dark-mode');
      const themeToggle = document.getElementById('theme-toggle');
      themeToggle.textContent = isDark ? 'â˜€ï¸' : 'ğŸŒ™';

      // ä¿å­˜æ‰‹åŠ¨è®¾ç½®ï¼Œè¦†ç›–è‡ªåŠ¨æ¨¡å¼
      localStorage.setItem('theme', isDark ? 'dark' : 'light');
      localStorage.setItem('themeAutoMode', 'false');
    }

    // åˆå§‹åŒ–é¡µé¢
    document.addEventListener('DOMContentLoaded', () => {
      // è¯­è¨€åˆå§‹åŒ–
      const savedLang = localStorage.getItem('language') || 'en';
      switchLanguage(savedLang);

      // åˆå§‹åŒ–more-newså’Œmore-publicationsåŒºåŸŸä¸ºéšè—çŠ¶æ€
      const allMoreNews = document.querySelectorAll('.more-news');
      allMoreNews.forEach(moreNews => {
        moreNews.style.display = 'none';
      });

      const allMorePublications = document.querySelectorAll('.more-publications');
      allMorePublications.forEach(morePubs => {
        morePubs.style.display = 'none';
      });

      // åˆ›å»ºä¸»é¢˜åˆ‡æ¢æŒ‰é’®
      const themeToggle = document.createElement('button');
      themeToggle.id = 'theme-toggle';
      themeToggle.className = 'lang-btn';
      themeToggle.style.position = 'fixed';
      themeToggle.style.top = '20px';
      themeToggle.style.right = '180px';
      themeToggle.style.zIndex = '1000';
      themeToggle.onclick = toggleTheme;
      document.body.appendChild(themeToggle);

      // ä¸»é¢˜åˆå§‹åŒ–
      const savedTheme = localStorage.getItem('theme');
      const isAutoMode = localStorage.getItem('themeAutoMode') !== 'false';

      if (savedTheme && !isAutoMode) {
        // ä½¿ç”¨ä¿å­˜çš„æ‰‹åŠ¨è®¾ç½®
        if (savedTheme === 'dark') {
          document.body.classList.add('dark-mode');
          themeToggle.textContent = 'â˜€ï¸';
        } else {
          document.body.classList.remove('dark-mode');
          themeToggle.textContent = 'ğŸŒ™';
        }
      } else {
        // ä½¿ç”¨åŸºäºæ—¶é—´çš„è‡ªåŠ¨æ¨¡å¼
        setThemeBasedOnTime();
        localStorage.setItem('themeAutoMode', 'true');
      }

      // æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡æ—¶é—´å˜åŒ–ï¼ˆä»…åœ¨è‡ªåŠ¨æ¨¡å¼ä¸‹ï¼‰
      setInterval(() => {
        const isAutoMode = localStorage.getItem('themeAutoMode') !== 'false';
        if (isAutoMode) {
          setThemeBasedOnTime();
        }
      }, 60000); // æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
    });

  </script>
</body>


</html>