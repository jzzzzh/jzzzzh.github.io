<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhuohang Jiang</title>

  <meta name="author" content="Zhuohang Jiang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css"/>
  <link rel="stylesheet" href="magic/dist/magic.css">
  <link rel="stylesheet" href="hover.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/wow/1.1.2/wow.min.js"></script>
</head>



<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" >
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center" >
                    <div class="wow animate__animated animate__fadeInDown" style="font-size: 2.5em; font-weight: bold;">
                      <name>Zhuohang Jiang</name>
                    </div>
                    
                  </p>
                  <div>  
                    <p>I am currently pursuing a PhD degree at the <strong> <a
                      href="https://www.polyu.edu.hk/">Hong Kong Polytechnic University</a></strong>. My supervisors are <a
                      href="https://scholar.google.cz/citations?user=D1LEg-YAAAAJ&hl=zh-CN">Qing Li</a>
                    and <a href="https://scholar.google.cz/citations?user=SQ9UbHIAAAAJ&hl=zh-CN&oi=ao">Wenqi Fan</a>.
                    </p>
                    <p>I studied at <a href="https://www.scu.edu.cn">Sichuan University (SCU)</a> from 2020 to 2024, where I majored in Computer Science & Technology. My
                    <strong> Major GPA (CS courses):
                      3.79/4, 89.39/100;</strong>
                    Overall GPA: 3.78/4, 89.25/100
                    </p>
                    <p>
                    During my time at Sichuan University, I worked as a research assistant at
                    <a href="http://www.machineilab.org/">MachineILab</a>
                    from 2022 to 2024, advised by
                    <a href="https://scholar.google.com/citations?user=-cNWmJMAAAAJ&hl=zh-CN">Prof. JiZhe Zhou</a>.
                    I participated in one National Natural Science Foundation of China project and one National Key R&D
                    Program of China.
                    </p>
                  </div>
                    
                  <p style="text-align:center">
                    <a href="mailto:zhuohangjiang2002@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="data/ZhuohangJiang-CV.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.cz/citations?hl=zh-CN&user=h3vEhrgAAAAJ">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/jzzzzh">Github</a>&nbsp
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/zhuohang.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/zhuohang.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

            <hr style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">
          
            <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research Topics</heading>
                  <p>
                    My research areas include <strong>recommendation systems(RS), large language models(LLM), computer
                      vision(CV), and graph neural networks(GNN)</strong>.
                    My previous research was primarily focused on topics within computer vision, such as tampering
                    detection and object recognition tasks.
                    Currently, I am diving into the realms of large language models (LLM) and Retrieval Augument Generation (RAG) .
                    </span>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <hr style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
              <td>
                <heading>News</heading>
              </td>
              </tr>
            </tbody>
            </table>
            <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
              <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                <div>
                <p> <strong>üìú 2025-03-30</strong> - Completed the survey paper <a href="https://arxiv.org/abs/2503.23350">A Survey of WebAgents: Towards Next-Generation AI Agents for Web Automation with Large Foundation Models</a>.</p>
                </div>
                <div>
                <p> <strong>üìò 2025-03-01</strong> - Completed the <a href="https://arxiv.org/abs/2503.00912">HiBench</a> paper and released the code and dataset on 
                  <a href="https://github.com/jzzzzh/HiBench">GitHub</a> and 
                  <a href="https://huggingface.co/datasets/zhuohang/HiBench">Hugging Face</a>.
                </p>
                </div>
                <div>
                <p> <strong>üèÜ 2024-12-01</strong> - <a href="https://arxiv.org/abs/2406.10580">IMDL-BenCo </a>was published in NeurIPS 2024 Benchmark Tracks and received a Spotlight award.</p>
                </div>
                <div>
                <p> <strong>üéì 2024-09-01</strong> - Beginning my pursuit of a PhD degree in Hong Kong PolyU.</p>
                </div>
                

                <!-- Ââ©‰ΩôÁöÑÊñ∞ÈóªÈªòËÆ§ÈöêËóè -->

                <div class="more-news">
                <p> <strong>üåü 2025-01-15</strong> - <a href="https://ojs.aaai.org/index.php/AAAI/article/view/33198">Mesoscopic Insights: Orchestrating Multi-Scale & Hybrid Architecture for Image Manipulation Localization</a> was published in AAAI 2025.</p>
                
                <p> <strong>üéì 2024-06-26</strong> - Graduated from Sichuan University with a bachelor's degree.</p>

                <p> <strong>üõ†Ô∏è 2024-06-12</strong> - Completed the co-work project <a href="https://github.com/scu-zjz/IMDLBenCo"> IMDLBenCo</a> and finished a paper
                  <a href="https://arxiv.org/abs/2406.10580"><strong>IMDL-BenCo: A Comprehensive Benchmark and
                    Codebase for Image Manipulation Detection & Localization</strong></a>
                </p>

                <p> <strong>üîç 2024-05-24</strong> - Finished a paper <a
                  href="https://arxiv.org/abs/2406.12736"><strong>Beyond
                    Visual Appearances: Privacy-sensitive Objects Identification via Hybrid Graph
                    Reasoning</strong></a> </p>
                </div>

                    <div style="text-align: center; margin-top: 20px;">
                    <button id="expand-btn" onclick="showMoreNews()" class="hvr-grow" style="padding: 10px 20px; font-size: 16px; background-color: #007BFF; color: white; border: none; border-radius: 5px; cursor: pointer;">
                      More News
                    </button>
                    </div></div>
                </td>
              </tr>
            </tbody>
          </table>
          <hr style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Publications</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>A Survey of WebAgents: Towards Next-Generation AI Agents for Web Automation with Large Foundation Models</papertitle>
                  [<a href="https://arxiv.org/abs/2503.23350">arxiv</a>]
                  <br>
                  Liangbo Ning, Ziran Liang, <b>Zhuohang Jiang</b>, Haohao Qu, Yujuan Ding, Wenqi Fan, Xiao-yong Wei, Shanru Lin, Hui Liu, Philip S. Yu, Qing Li
                  <br>
                  <font color='gray'>
                    With the advancement of web techniques, they have significantly revolutionized various aspects of people's lives. Despite the importance of the web, many tasks performed on it are repetitive and time-consuming, negatively impacting overall quality of life. To efficiently handle these tedious daily tasks, one of the most promising approaches is to advance autonomous agents based on Artificial Intelligence (AI) techniques, referred to as AI Agents, as they can operate continuously without fatigue or performance degradation. In the context of the web, leveraging AI Agents -- termed WebAgents -- to automatically assist people in handling tedious daily tasks can dramatically enhance productivity and efficiency. Recently, Large Foundation Models (LFMs) containing billions of parameters have exhibited human-like language understanding and reasoning capabilities, showing proficiency in performing various complex tasks. This naturally raises the question: `Can LFMs be utilized to develop powerful AI Agents that automatically handle web tasks, providing significant convenience to users?' To fully explore the potential of LFMs, extensive research has emerged on WebAgents designed to complete daily web tasks according to user instructions, significantly enhancing the convenience of daily human life. In this survey, we comprehensively review existing research studies on WebAgents across three key aspects: architectures, training, and trustworthiness. Additionally, several promising directions for future research are explored to provide deeper insights. 
                  </font>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>HiBench: Benchmarking LLMs Capability on Hierarchical Structure Reasoning</papertitle>
                  [<a href="https://arxiv.org/abs/2503.00912">arxiv</a>] [<a href="https://github.com/jzzzzh/HiBench">GitHub</a>] [<a href="https://huggingface.co/datasets/zhuohang/HiBench">Hugging Face</a>]
                  <br>
                  <b>Zhuohang Jiang</b>, Pangjing Wu, Ziran Liang, Peter Q. Chen, Xu Yuan, Ye Jia, Jiancheng Tu, Chen Li, Peter H.F. Ng, Qing Li
                  <br>
                  <font color='gray'>
                    Structure reasoning is a fundamental capability of large language models (LLMs), enabling them to reason about structured commonsense and answer multi-hop questions. However, existing benchmarks for structure reasoning mainly focus on horizontal and coordinate structures (\emph{e.g.} graphs), overlooking the hierarchical relationships within them. Hierarchical structure reasoning is crucial for human cognition, particularly in memory organization and problem-solving. It also plays a key role in various real-world tasks, such as information extraction and decision-making. To address this gap, we propose HiBench, the first framework spanning from initial structure generation to final proficiency assessment, designed to benchmark the hierarchical reasoning capabilities of LLMs systematically. HiBench encompasses six representative scenarios, covering both fundamental and practical aspects, and consists of 30 tasks with varying hierarchical complexity, totaling 39,519 queries. To evaluate LLMs comprehensively, we develop five capability dimensions that depict different facets of hierarchical structure understanding. Through extensive evaluation of 20 LLMs from 10 model families, we reveal key insights into their capabilities and limitations: 1) existing LLMs show proficiency in basic hierarchical reasoning tasks; 2) they still struggle with more complex structures and implicit hierarchical representations, especially in structural modification and textual reasoning. Based on these findings, we create a small yet well-designed instruction dataset, which enhances LLMs' performance on HiBench by an average of 88.84% (Llama-3.1-8B) and 31.38% (Qwen2.5-7B) across all tasks. The HiBench dataset and toolkit are available here, this https URL, to encourage evaluation. 
                  </font>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <papertitle>Mesoscopic Insights: Orchestrating Multi-Scale & Hybrid Architecture for Image Manipulation Localization</papertitle>
                  [<a href="https://arxiv.org/abs/2412.13753">arxiv</a>]
                  <br>
                  Xuekang Zhu, Xiaochen Ma, Lei Su, <b>Zhuohang Jiang</b>, Bo Du, Xiwen Wang, Zeyu Lei, Wentao Feng, Chi-Man Pun, Jizhe Zhou
                  <br>
                  <font color='gray'>
                    The mesoscopic level serves as a bridge between the macroscopic and microscopic worlds, addressing gaps overlooked by both. Image manipulation localization (IML), a crucial technique to pursue truth from fake images, has long relied on low-level (microscopic-level) traces. However, in practice, most tampering aims to deceive the audience by altering image semantics. As a result, manipulation commonly occurs at the object level (macroscopic level), which is equally important as microscopic traces. Therefore, integrating these two levels into the mesoscopic level presents a new perspective for IML research. Inspired by this, our paper explores how to simultaneously construct mesoscopic representations of micro and macro information for IML and introduces the Mesorch architecture to orchestrate both. Specifically, this architecture i) combines Transformers and CNNs in parallel, with Transformers extracting macro information and CNNs capturing micro details, and ii) explores across different scales, assessing micro and macro information seamlessly. Additionally, based on the Mesorch architecture, the paper introduces two baseline models aimed at solving IML tasks through mesoscopic representation. Extensive experiments across four datasets have demonstrated that our models surpass the current state-of-the-art in terms of performance, computational complexity, and robustness. 
                  </font>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <!-- <a href="under-review.html" id="AutoCost"> -->

                  <papertitle>IMDL-BenCo: A Comprehensive Benchmark and Codebase for Image Manipulation Detection &
                    Localization</papertitle>
                  [<a href="https://arxiv.org/abs/2406.10580">arxiv</a>]
                  <br>
                  Xiaochen Ma, Xuekang Zhu, Lei Su, Bo Du, <b>Zhuohang Jiang</b>, Bingkui Tong, Zeyu Lei, Xinyu Yang,
                  Chi-Man Pun, Jiancheng Lv, Jizhe Zhou

                  <br>
                  <font color='gray'>
                    A comprehensive benchmark is yet to be established in the Image Manipulation Detection &
                    Localization (IMDL) field. The absence of such a benchmark leads to insufficient and misleading
                    model evaluations, severely undermining the development of this field. However, the scarcity of
                    open-sourced baseline models and inconsistent training and evaluation protocols make conducting
                    rigorous experiments and faithful comparisons among IMDL models challenging. To address these
                    challenges, we introduce IMDL-BenCo, the first comprehensive IMDL benchmark and modular codebase.
                    IMDL-BenCo:i) decomposes the IMDL framework into standardized, reusable components and
                    revises the model construction pipeline, improving coding efficiency and customization
                    flexibility;ii) fully implements or incorporates training code for state-of-the-art models
                    to establish a comprehensive IMDL benchmark; and iii) conducts deep analysis based on the
                    established benchmark and codebase, offering new insights into IMDL model architecture, dataset
                    characteristics, and evaluation standards. Specifically, IMDL-BenCo includes common processing
                    algorithms, 8 state-of-the-art IMDL models (1 of which are reproduced from scratch), 2 sets of
                    standard training and evaluation protocols, 15 GPU-accelerated evaluation metrics, and 3 kinds of
                    robustness evaluation. This benchmark and codebase represent a significant leap forward in
                    calibrating the current progress in the IMDL field and inspiring future breakthroughs.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <!-- <a href="under-review.html" id="AutoCost"> -->

                  <papertitle>Beyond Visual Appearances: Privacy-sensitive Objects Identification via Hybrid Graph
                    Reasoning</papertitle>
                  [<a href="https://arxiv.org/abs/2406.12736">arxiv</a>]
                  <br>
                  <b>Zhuohang Jiang</b>, Bingkui Tong, Xia Du, Ahmed Alhammadi, Jizhe Zhou

                  <br>
                  <font color='gray'>
                    To explicitly derive the objects' privacy class from the scene contexts, in this paper, we interpret
                    the POI task as a visual reasoning task aimed at the privacy of each object in the scene. Following
                    this interpretation, we propose the PrivacyGuard framework for POI. PrivacyGuard contains three
                    stages. i) Structuring: an unstructured image is first converted into a structured, heterogeneous
                    scene graph that embeds rich scene contexts. ii) Data Augmentation: a contextual perturbation
                    oversampling strategy is proposed to create slightly perturbed privacy-sensitive objects in a scene
                    graph, thereby balancing the skewed distribution of privacy classes. iii) Hybrid Graph Generation &
                    Reasoning: the balanced, heterogeneous scene graph is then transformed into a hybrid graph by
                    endowing it with extra "node-node" and "edge-edge" homogeneous paths. These homogeneous paths allow
                    direct message passing between nodes or edges, thereby accelerating reasoning and facilitating the
                    capturing of subtle context changes.
                  </font>
                </td>
              </tr>
            </tbody>
          </table>





            <div class="more-publications" style="display: none;">
            <!-- Remaining publications go here -->

          <table
          style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                <!-- <a href="under-review.html" id="AutoCost"> -->

                <papertitle>IML-ViT: Benchmarking Image Manipulation Localization by Vision Transformer</papertitle>
                [<a href="https://arxiv.org/abs/2307.14863">arxiv</a>]
                <br>
                Xiaochen Ma, Bo Du,<b>Zhuohang Jiang</b>, Ahmed Y. Al Hammadi, Jizhe Zhou

                <br>
                <font color='gray'>
                  Due to limited datasets, there is currently no pure ViT-based approach for IML to serve as a
                  benchmark, and CNNs dominate the entire task. Nevertheless, CNNs suffer from weak long-range and
                  non-semantic modeling. To bridge this gap, based on the fact that artifacts are sensitive to image
                  resolution, amplified under multi-scale features, and massive at the manipulation border, we
                  formulate the answer to the former question as building a ViT with high-resolution capacity,
                  multi-scale feature extraction capability, and manipulation edge supervision that could converge
                  with a small amount of data. We term this simple but effective ViT paradigm IML-ViT, which has
                  significant potential to become a new benchmark for IML. Extensive experiments on five benchmark
                  datasets verified our model outperforms the state-of-the-art manipulation localization methods.
                </font>
              </td>
            </tr>
          </tbody>
        </table>


        <table
          style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                <!-- <a href="under-review.html" id="AutoCost"> -->

                <papertitle>Perceptual MAE for Image Manipulation Localization: A High-level Vision Learner Focusing
                  on Low-level Features</papertitle> [<a href="https://arxiv.org/abs/2310.06525">arxiv</a>]
                <br>
                Xiaochen Ma, <b>Zhuohang Jiang</b>, Xiong Xu, Chi-Man Pun, Jizhe Zhou
                <br>
                <font color='gray'> This necessitates IML models to carry out a semantic understanding of the entire
                  image. In this paper, we
                  reformulate the IML task as a high‚Äëlevel
                  vision task that greatly benefits from low‚Äëlevel features. We propose a method to enhance the Masked
                  Autoencoder (MAE) by incorporating
                  high‚Äëresolution inputs and a perceptual loss supervision module, which we term Perceptual MAE
                  (PMAE). While
                  MAE has demonstrated an
                  impressive understanding of object semantics, PMAE can also comprehend low‚Äëlevel semantics with our
                  proposed
                  enhancements. This
                  paradigm effectively unites the low‚Äëlevel and high‚Äëlevel features of the IML task and outperforms
                  state‚Äëof‚Äëthe‚Äëart tampering localization methods
                  on five publicly available datasets, as evidenced by extensive experiments.</font>
              </td>
            </tr>
          </tbody>
        </table>
          <table
          style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
            <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
              <!-- <a href="under-review.html" id="AutoCost"> -->
              <papertitle>Contour‚ÄëAware Contrastive Learning for Image Manipulation Localization</papertitle>
              <br>
              Qin Li, Chunfang Yu, <b>Zhuohang Jiang</b>, AND Jizhe Zhou
              <br>
              <font color='gray'>We propose a novel Contour‚Äëaware Contrastive Learning Network (CaCL‚ÄëNet) based on
              the encoder‚Äëdecoder architecture. On the encoder side,
              since the contour is foremost concerned in IML, we consider the image patches sampled along the
              manipulation contour are the hard examples
              and set them as the anchor. The patches of pure tampered and authentic pixels are set as positives
              and negatives respectively to conduct
              contrastive learning. The decoder then manages to specify the manipulated regions and restores the
              explicit contours of the manipulations
              through the proposed Contour Binary Cross‚ÄëEntropy (CBCE) loss.
              </font>
            </td>
            </tr>
          </tbody>
          </table>
            </div>

            <div style="text-align: center; margin-top: 20px;">
              <button id="expand-publications-btn" onclick="togglePublications()" class="hvr-grow" style="padding: 10px 20px; font-size: 16px; background-color: #007BFF; color: white; border: none; border-radius: 5px; cursor: pointer;">
              More Publications
              </button>
            </div></div>

            <script>
            function togglePublications() {
                var morePublications = document.querySelector('.more-publications');
                var button = document.getElementById('expand-publications-btn');
                button.addEventListener('click', function() {
                if (morePublications.style.display === 'none') {
                  morePublications.style.display = 'block';
                  button.classList.remove('hvr-grow');
                  button.classList.add('hvr-shrink');
                  button.textContent = 'Less Publications';
                  morePublications.classList.add('magictime', 'puffIn');
                } else {
                  morePublications.style.display = 'none';
                  button.classList.remove('hvr-shrink');
                  button.classList.add('hvr-grow');
                  button.textContent = 'More Publications';
                }
                });
            }
            </script>

          <hr style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading class="">Research Projects</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <!-- <a href="under-review.html" id="AutoCost"> -->

                  <papertitle>Research on Scene Graph Structure Learning Method for Private Object Detection
                  </papertitle>

                  <br>
                  Advisor: Jizhe Zhou
                  <br>
                  <strong> Participate as an intern</strong>
                  <br>
                  <em>National Natural Science Foundation of China </em>,&nbsp;2024&nbsp;
                  <br>
                  <font color='gray'>The privacy-sensitive object detection problem reauires the model to locate private
                    objects in bounding boxes on images or videos. Research on privacy-sensitive object detection has
                    imnortant value for personal-privacyprotection. Privacy-sensitive ob ject detection is actually a
                    scene reasoning problem. However existing privacy-sensitive oh iect detection methods are all
                    basedon the object detection framework.Due to the lack of scene reasoning ability,existing methods
                    suffer from detection accuracy,generalizability,and interpretability.This project intends to build a
                    set of privacy-sensitive objectdetection methods with scene reasoning capability through scene
                    graphs. Unlikeother tasks, privacy-sensitive object detection requires a non-parametric scenegraph
                    structure to keep the graph sparseÔºådynamicÔºåand interpretable.Therefore,this project correspondingly
                    proposes the scene graph structure learning methods.By studying 1) the distillation method of the
                    graph structure to sparse the scenegraphÔºå2) the transferring method between the scene graphs of
                    different frames tomake the scene graph dynamicÔºå3) the privacy-rule reasoning method based on
                    thescene graph structureÔºåsolves the problem of scene graph generation with thenon-parametric graph
                    structureÔºå builds a new privacy-sensitive object detectionframework based on scene reasoningÔºå break
                    the bottlenecks of privacy-sensitiveobject detection methods in accuracyÔºågeneralizability,and
                    interpretability.Thisprivacy-sensitive object detection framework also enriched the theoretical
                    framework and application scenarios of neural networks.</font>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                  <!-- <a href="under-review.html" id="AutoCost"> -->

                  <papertitle>Intelligent control and full life feedforward deduction technology through pre planning
                    and post evaluation</papertitle>

                  <br>
                  Advisor: Jizhe Zhou
                  <br>
                  <strong> Participate as an intern</strong>
                  <br>
                  <em>National Key R&D
                    Program of China</em>,&nbsp;2023&nbsp;
                  <br>
                  <font color='gray'>This topic proposes to adopt the scheme of "first completing information, then path
                    reasoning". Specifically, this sub project intends to improve the initial network diagram based on
                    the data base established in previous projects and further consider the frequency of common
                    occurrence of impact factors. Then, based on the external knowledge causal information completion
                    method of remote supervision, the network graph is again completed and cleaned, and then a path
                    reasoning algorithm based on depth first traversal of the graph is used to achieve feedforward
                    reasoning. Finally, a human-computer interactive network information verification method based on
                    uncertainty reasoning is used to revise the causal relationship in the network diagram again based
                    on artificial feedback on the reasoning path, further improving the accuracy and recall rate of the
                    path reasoning algorithm results.</font>
                </td>
              </tr>
            </tbody>
          </table>
          
          <hr style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Education</heading>
                </td>
              </tr>
              <tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                  <b>Sichuan University</b>, Chengdu, Sichuan, China
                  <br>
                  B.E. in Computer Science and Technology ‚Ä¢ Sep. 2020 to Jun. 2024
                  </br>
                </td>
                <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img
                    src="images/SichuanUniversityLOGO.png" width="75" height="75"></td>
              </tr>
            </tbody>
          </table>
          <br>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                  <b>Hong Kong Polytechnic University</b>, Hongkong, China
                  <br>
                  PHD. in Computer Science and Technology ‚Ä¢ Sep. 2024 to Present
                  </br>
                </td>
                <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img src="images/polyu.jpg"
                    width="75" height="75"></td>
              </tr>
            </tbody>

          </table>
          
          <hr style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Experience</heading>
                </td>
              </tr>
              <tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                  <a href="http://dicalab.cn/"> DICALab</a>, <b>Sichuan University</b>
                  <br>
                  Research Assistant ‚Ä¢ Sep. 2022 to Jun. 2024
                  <br>
                  Advisor: <a href="https://scholar.google.com/citations?user=-cNWmJMAAAAJ&hl=zh-CN"> Prof. JiZhe
                    Zhou</a>
                  </br>
                </td>
                <td style="padding:0px 20px 0px 20px;width:8%;vertical-align:middle"><img src="images/DICA.png"
                    width="75" height="75"></td>
              </tr>
            </tbody>
          </table>
          <br>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/covscript/covscript"> Covariant association</a>, <b>Sichuan University</b>
                  <br>
                  the president of the covariant association ‚Ä¢ Sep. 2022 to Jun. 2023
                  <br>
                  the Covariant association obtain at least <font color="red"><em><strong>400+</strong></em></font>
                  members, in which communicate the technology of the
                  computer science.
                </td>
                <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img src="images/covariant.jpg"
                    width="75" height="75"></td>
              </tr>
            </tbody>
          </table>
          <hr style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Award</heading>
                </td>
              </tr>
              <tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <strong>Computer Design Competition</strong> China, 2023
                </td>
                <td style="width:10%;vertical-align:middle; width: 40%;">
                  <strong>Provincial First Prize</strong>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
            <tbody>
              <tr>
                <td>
                  <strong>Comprehensive First Class Scholarship</strong> Sichuan University, Sichuan, China, 2022
                </td>
                <td style="width:10%;vertical-align:middle; width: 40%;">
                  <strong>Top 1%</strong>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <strong>Outstanding students of Sichuan University</strong> Sichuan University, Sichuan, China, 2022
                </td>
                <td style="width:10%;vertical-align:middle; width: 40%;">
                  <strong>Top 5%</strong>
                </td>
              </tr>
            </tbody>
          </table>






          <hr style="border: 0; height: 1px; background: linear-gradient(to right, transparent, #ccc, transparent); width: 100%; margin: 20px 0;">





          <div style="text-align: center;">
            <a href="https://clustrmaps.com/site/1buac" title="Visit tracker">
              <img src="//www.clustrmaps.com/map_v2.png?d=arwquO5nouqZwMbhchX9awZ1bM3tO8fPWrcUr78F1E4&cl=ffffff" />
            </a>
          </div>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px">
                  <div style="float:left;">
                    Updated at Apr. 2025
                  </div>
                  <div style="float:right;">
                    Thanks <a href="https://jonbarron.info">Jon Barron</a> for this amazing template
                  </div>
                  <br>
                  <br>
                  <p></p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>


  <script>
    new WOW().init();


    function showMoreNews() {
      var moreNews = document.querySelector('.more-news');
      var button = document.getElementById('expand-btn');
      button.addEventListener('click', function() {
      if (moreNews.style.display === 'none') {
        moreNews.style.display = 'block';
        button.textContent = 'Less News';
        button.classList.remove('hvr-grow');
        button.classList.add('hvr-shrink');
        moreNews.classList.add('magictime', 'puffIn');
      } else {
        moreNews.style.display = 'none';
        button.textContent = 'More News';
        button.classList.remove('hvr-shrink');
        button.classList.add('hvr-grow');
      }
      });
    }



  </script>
</body>


</html>